---
title: "Equivariances & Graphical Neural Networks"
description: "Description of this Post"
date: "2023-11-27"
date-format: long
year: "2023"
categories: [All, Deep Learning, TAGS]
toc: false
jupyter: git-pages
code-fold: true
number-sections: true
---

## Title
<center>![Slide 1](imgs/page_1.png){.w575}</center><pre></pre>


## Organisation
<center>![Slide 2](imgs/page_2.png){.w575}</center><pre></pre>


## Lecture overview
<center>![Slide 3](imgs/page_3.png){.w575}</center><pre></pre>



## Graphs! They’re everywhere
<center>![Slide 4](imgs/page_4.png){.w575}</center><pre></pre>



## What are graphs?
<center>![Slide 5](imgs/page_5.png){.w575}</center><pre></pre>



## What are graphs?
<center>![Slide 6](imgs/page_6.png){.w575}</center><pre></pre>



## Graphs as geometry.
<center>![Slide 7](imgs/page_7.png){.w575}</center><pre></pre>



## 1) Classifying graphs
<center>![Slide 8](imgs/page_8.png){.w575}</center><pre></pre>



## 2) Classifying nodes
<center>![Slide 9](imgs/page_9.png){.w575}</center><pre></pre>



## 3) Graph generation
<center>![Slide 10](imgs/page_10.png){.w575}</center><pre></pre>



## 4) Link/Edge prediction
<center>![Slide 11](imgs/page_11.png){.w575}</center><pre></pre>



## Three tasks visualized: here with nodes that carry features
<center>![Slide 12](imgs/page_12.png){.w575}</center><pre></pre>



## Graphs can be static, varying, or even evolving with time
<center>![Slide 13](imgs/page_13.png){.w575}</center><pre></pre>



## Regular structures vs graphs
<center>![Slide 14](imgs/page_14.png){.w575}</center><pre></pre>



## Title
<center>![Slide 15](imgs/page_15.png){.w575}</center><pre></pre>



## Directed graphs
<center>![Slide 16](imgs/page_16.png){.w575}</center><pre></pre>



## Undirected graphs
<center>![Slide 17](imgs/page_17.png){.w575}</center><pre></pre>



## Graph neighborhood
<center>![Slide 18](imgs/page_18.png){.w575}</center><pre></pre>



## Attributes
<center>![Slide 19](imgs/page_19.png){.w575}</center><pre></pre>

The attention score is measured by these softmax 

The dot product here it ends up being 2x3 again

<center>![](imgs/2023-11-28-16-14-09.png){.w450}</center><pre></pre>

## Adjacency matrix
<center>![Slide 20](imgs/page_20.png){.w575}</center><pre></pre>



## Adjacency matrix for undirected graphs
<center>![Slide 21](imgs/page_21.png){.w575}</center><pre></pre>



## Weighted adjacency matrix
<center>![Slide 22](imgs/page_22.png){.w575}</center><pre></pre>



## Graph representation for us
<center>![Slide 23](imgs/page_23.png){.w575}</center><pre></pre>



## Quiz:
<center>![Slide 24](imgs/page_24.png){.w575}</center><pre></pre>



## Graph Laplacian
<center>![Slide 25](imgs/page_25.png){.w575}</center><pre></pre>


## Graph Laplacian: meaning
<center>![Slide 26](imgs/page_26.png){.w575}</center><pre></pre>



## Applications of the Graph Laplacian
<center>![Slide 27](imgs/page_27.png){.w575}</center><pre></pre>



## Applied Laplacian written out:
<center>![Slide 28](imgs/page_28.png){.w575}</center><pre></pre>



## Title
<center>![Slide 29](imgs/page_29.png){.w575}</center><pre></pre>



## The shift operator, a special circulant matrix
<center>![Slide 30](imgs/page_30.png){.w575}</center><pre></pre>



## Now we want to know:
<center>![Slide 31](imgs/page_31.png){.w575}</center><pre></pre>



## As it turns out: circulant matrices commute
<center>![Slide 32](imgs/page_32.png){.w575}</center><pre></pre>



## What this means: Translation equivariance > circulant matrices/convolutions
<center>![Slide 33](imgs/page_33.png){.w575}</center><pre></pre>



## Where we are
<center>![Slide 34](imgs/page_34.png){.w575}</center><pre></pre>



## Maths: All circulant matrices have the same eigenvectors!
<center>![Slide 35](imgs/page_35.png){.w575}</center><pre></pre>



## All circulant matrices have the same eigenvectors!
<center>![Slide 36](imgs/page_36.png){.w575}</center><pre></pre>



## Circulant eigenvectors © Shift eigenvectors
<center>![Slide 37](imgs/page_37.png){.w575}</center><pre></pre>



## But first: What are the eigenvectors of the shift operator
<center>![Slide 38](imgs/page_38.png){.w575}</center><pre></pre>



## Computing a convolution in the frequency domain
<center>![Slide 39](imgs/page_39.png){.w575}</center><pre></pre>



## Convolution Theorem
<center>![Slide 40](imgs/page_40.png){.w575}</center><pre></pre>



## Convolution theorem
<center>![Slide 41](imgs/page_41.png){.w575}</center><pre></pre>



## Frequency representation:
<center>![Slide 42](imgs/page_42.png){.w575}</center><pre></pre>



## ; Quiz: Remember the Fourier transform for images: |
<center>![Slide 43](imgs/page_43.png){.w575}</center><pre></pre>



## Convolution theorem: x * w = ®- (A(w) - (®* - x))
<center>![Slide 44](imgs/page_44.png){.w575}</center><pre></pre>



## Implications
<center>![Slide 45](imgs/page_45.png){.w575}</center><pre></pre>



## If translation equivariance leads to CNNs, what else is there?
<center>![Slide 46](imgs/page_46.png){.w575}</center><pre></pre>



## A large field: Group Equivariant Deep Learning
<center>![Slide 47](imgs/page_47.png){.w575}</center><pre></pre>



## Circulant matrices
<center>![Slide 48](imgs/page_48.png){.w575}</center><pre></pre>



## “I was lucky...

How research gets done part 6
<center>![Slide 49](imgs/page_49.png){.w575}</center><pre></pre>



## Title
<center>![Slide 50](imgs/page_50.png){.w575}</center><pre></pre>



## From convolutions to spectral graph convolutions
<center>![Slide 51](imgs/page_51.png){.w575}</center><pre></pre>



## Approach: Use Eigenvectors of Graph Laplacian to replace Fourier
<center>![Slide 52](imgs/page_52.png){.w575}</center><pre></pre>



## Actually:
<center>![Slide 53](imgs/page_53.png){.w575}</center><pre></pre>



## Further details
<center>![Slide 54](imgs/page_54.png){.w575}</center><pre></pre>



## In analogy to convolutions in frequency domain:

We now define spectral graph convolutions
<center>![Slide 55](imgs/page_55.png){.w575}</center><pre></pre>



## Where we are, part 2
<center>![Slide 56](imgs/page_56.png){.w575}</center><pre></pre>



## Why the graph Laplacian*?
<center>![Slide 57](imgs/page_57.png){.w575}</center><pre></pre>



## Spectral graph convolution
<center>![Slide 58](imgs/page_58.png){.w575}</center><pre></pre>



## Some drawbacks of this variant
<center>![Slide 59](imgs/page_59.png){.w575}</center><pre></pre>



## Easy to increase the field of view with powers of the Laplacian
<center>![Slide 60](imgs/page_60.png){.w575}</center><pre></pre>



## Putting it together: stacking graph convolutions
<center>![Slide 61](imgs/page_61.png){.w575}</center><pre></pre>



## . Quiz: What nronerties does this nolvnomial variant have? |
<center>![Slide 62](imgs/page_62.png){.w575}</center><pre></pre>



## Some drawbacks of this variant now
<center>![Slide 63](imgs/page_63.png){.w575}</center><pre></pre>



## Title
<center>![Slide 64](imgs/page_64.png){.w575}</center><pre></pre>



## A FF fF F Fg
<center>![Slide 65](imgs/page_65.png){.w575}</center><pre></pre>



## Graph convolutions
<center>![Slide 66](imgs/page_66.png){.w575}</center><pre></pre>



## What can we use from the spectral approach?
<center>![Slide 67](imgs/page_67.png){.w575}</center><pre></pre>



## Graph Convolutional Networks (GCN)
<center>![Slide 68](imgs/page_68.png){.w575}</center><pre></pre>



## Graph Convolutional Networks (GCN)
<center>![Slide 69](imgs/page_69.png){.w575}</center><pre></pre>



## Putting it together:
<center>![Slide 70](imgs/page_70.png){.w575}</center><pre></pre>



## Other kind of aggregation: Graph Attention Networks (GAT)
<center>![Slide 71](imgs/page_71.png){.w575}</center><pre></pre>



## Self-attention for graph convolutions
<center>![Slide 72](imgs/page_72.png){.w575}</center><pre></pre>



## Connection to transformers
<center>![Slide 73](imgs/page_73.png){.w575}</center><pre></pre>



## Message Passing Neural Network (MPNN)
<center>![Slide 74](imgs/page_74.png){.w575}</center><pre></pre>



## PyTorch Geometric baseclass
<center>![Slide 75](imgs/page_75.png){.w575}</center><pre></pre>



## Overview
<center>![Slide 76](imgs/page_76.png){.w575}</center><pre></pre>



## Finally, a note about coarsening graphs
<center>![Slide 77](imgs/page_77.png){.w575}</center><pre></pre>



## Where we are, part 3
<center>![Slide 78](imgs/page_78.png){.w575}</center><pre></pre>



## The last few lectures
<center>![Slide 79](imgs/page_79.png){.w575}</center><pre></pre>



## Title
<center>![Slide 80](imgs/page_80.png){.w575}</center><pre></pre>



## Title
<center>![Slide 81](imgs/page_81.png){.w575}</center><pre></pre>














