---
title: "DL Part 3"
description: "Description of this Post"
date: "2023-12-08"
date-format: long
year: "2023"
categories: [All, TAGS]
toc: false
code-fold: true
number-sections: true
draft: true
css: ../css/custom_page_width/800px.css
# include-after-body: reset_eqns.html
---


<!-- \mathbfcal{I} -->
## Q1.1
1. We sample $\textbf{z}_n$ from the the normal distribution $\mathcal(N)(0, \textbf{\textit{I}}_D)$ 
2. We feed the sampled latent vector into the decoder $f_{\theta}(\textbf{z}_n)$ 
3. For every m pixel we sample $x_n$ from the categorical distribution as expressed in fig (5). Ultimately we get $p(x_n | z_n)$ which we can use is to generate an image


## Q1.2
<center>![](2023-12-08-20-31-16.png){.w350}</center><pre></pre>

1. If we use Montecarlo to approximate $\log p(\textbf{x}_n)$ we will be sampling $\textbf{z}_{n}^{(l)}$ from the prior distribution $p(\textbf{z}_n)$. However, we can see that for most of these samples $p(\textbf{x}_n|\textbf{z}_n^{(l)})$ will be close to zero. This is a problem because we see that in order to sample $\textbf{z}_n$ which is then used to compute $p(\textbf{x}_n|\textbf{z}_n^{(l)})$ we want to fall in the small region represented by the posterior (depicted with blue line contours). Thus we would be sampling many times but only a small portion of those samples would be beneficial to approximate $\log p(\textbf{x}_n)$ thereby not an efficient method.

2. As we can see in the graph this is a 2-dimensional latent space showing the prior and the posterior distributions. If we would increase the dimensionality of this space (the dimensionality of $\textbf{z}_n$ increases as well) then effectively we are increasing the region from which we can sample $\textbf{z}_n$. Thus, to get samples that contribute to compute $\log p(\textbf{x}_n)$ we would have to sample even more which is inefficient. 


## Q1.3
Because the term $KL(q(\textbf{z}_n | \textbf{x}_n) || p(\textbf{z}_n | \textbf{x}_n) ))≥ 0$ is always positive. It follows that: 

$$
\begin{align*}
\log p(\textbf{x}_n) - KL(q(\textbf{z}_n | \textbf{x}_n) || p(\textbf{z}_n | \textbf{x}_n) )) &= \mathbb{E}_{q (\textbf{z}_n | \textbf{x}_n)} [\log p(\textbf{x}_n | \textbf{z}_n)] - KL(q(\textbf{z}_n | \textbf{x}_n) || p(\textbf{z}_n))\\
\log p(\textbf{x}_n) &≥ \mathbb{E}_{q (\textbf{z}_n | \textbf{x}_n)} [\log p(\textbf{x}_n | \textbf{z}_n)] - KL(q(\textbf{z}_n | \textbf{x}_n) || p(\textbf{z}_n)) \tag{2}
\end{align*}
$$
Therefore the RHS of the equation (2) is a lower bound for $\log p(x)$.

## Q1.4

When the lower bound is pushed up effectively the $\log p(\textbf{x}_n)$ is being maximized. Conversely this makes $KL(q(\textbf{z}_n | \textbf{x}_n) || p(\textbf{z}_n | \textbf{x}_n) ))$ to be minimized. This implies:

1. The gap to the log-liklelihood $\log p(\textbf{x}_n)$ tightens. This in turn means we have a better generative model. 
2. If we minimize  $KL(q(\textbf{z}_n | \textbf{x}_n) || p(\textbf{z}_n | \textbf{x}_n) ))$ that means $q (\textbf{z}_n | \textbf{x}_n)$ will approximate to the true posterior $p(\textbf{z}_n | \textbf{x}_n))$. This in turn means that we have a better latent representation. This also means that the problem of $p(\textbf{z}_n | \textbf{x}_n) )$ not being tractable is solved.

## Q1.4

<center>![](imgs/2023-12-09-02-06-15.png){.w475}</center><pre></pre>
Img Source at: lilianweng.github.io


- In the first case the term reconstruction is appropriate because $\mathbb{E}_{q_{\phi} (\textbf{z}_n | \textbf{x}_n)} [\log p(\textbf{x}_n | \textbf{z}_n)]$ represents the expected value of $x$ if we were to sample $z_n$ from the approximated posterior $q_{\phi}(\textbf{z}_n | \textbf{x}_n)$ and then reconstruct it using $p_{\theta}(\textbf{x}_n | \textbf{z}_n) )$. In other words how well the model predicts/reconstructs an observation from a sample from the variational posterior.

- For the second case the term regularization is appropriate because the KL-divergence pushes the variational posterior  $q_{\phi}(\textbf{z}_n | \textbf{x}_n)$ towards the prior $p_{\theta}(\textbf{z}_n)$. In other words the KL-divergence term tries to approximate/regulate that the distribution $q_{\phi}(\textbf{z}_n | \textbf{x}_n)$ (blue circle above) be as close as possible to the distribution of the latent variable $p_{\theta}(\textbf{z}_n)$ (red circle).


<!-- 46,59
The term is appropriate because  

The reconstruction loss measures the difference between the original input and the ouput generated by the decoder. This is what this equation is doing. IT first samples a z from our approximated posterior q(z|x), and then using p(x|z) reconstructs x.   

We do this by first sampling z from our approximated posterior q(z|x), and then using p(x|z) to reconstruct x. 
Thus this loss measures how well the sample (due to the hint) has been reconstructed. A h


Ideally we want this loss to be as close to zero means means the reconstructed data  -->


## Q1.5

Passing the derivative through samples can be done using the reparameterization trick. In a few sentences, explain why the act of sampling usually prevents us from computing ∇φL, and how the reparameterization trick solves this problem.
 
<center>![](https://www.deeplearningbook.com.br/wp-content/uploads/2019/12/form11.png){}</center><pre></pre>
Img Source at: deeplearningbook.com 

**Why the act of sampling usually prevents us from computing ∇φL?**

This is because sampling $\textbf{z}$ from  $q_{\phi}(\textbf{z}_n | \textbf{x}_n)$ directly is not differentiable. To minimize the loss and learn the variational parameters we need this to be differentiable so that we can do backpropagation and thus learn the parameters. This problem can be visualized in the left side of the above picture.

A way to circumvent this problem is by using the reparametrization trick. Essentially we want to be able to rewrite the expectation so that the distribution w.r.t. which we take the gradient is independent of parameter $\phi$. To achieve this, we can reparametrize a sample from  $q_{\phi}(\textbf{z}_n | \textbf{x}_n)$ by expressing it as a function of a sample $\epsilon$ from some distribution $p(\epsilon)$

$$
z = g(\epsilon, \phi)
$$

Here $g(\epsilon, \phi)$ is the function that maps $\epsilon$ to $z$. Importantly, this function needs to be differentiable w.r.t $\phi$, i.e a Gaussian distribution so that $z$ becomes:


$$
z = \mu + \sigma \, \odot \, \epsilon \quad \text{with}\quad \epsilon \sim  N(0, I) \tag{1}
$$

What we accomplish with eq (1) is that now the sampling is done over $\epsilon$. This means that now we can compute backpropagatin over $\mu, \sigma$ which are deterministic parameters that we can learn to optimize the loss. Recall the loss was composed of two terms the $L_{reg}$  and $L_{recon}$ (which this later is the one we use the reparametrization trick). To wrap up, this idea of sampling with reparametrization can be visualized in the right side of the picture above.




 <!-- so that our expectation can be rewriten as follows:

$$
\nabla_{\phi} E_{q_{\phi}(z)}[f(z)] = E_{p(\epsilon)}[\nabla_{z}f(z)\nabla_{\phi}g(\epsilon, \phi)]
$$


The above equation this allow us is to compute now the gradient of z to do backprogatation. This is illustrated in the picture above in the right side. There we can see that if i.e  $g(\epsilon, \phi)$ is a Gaussian distribution then we would be able to take its gradient. -->
<!-- 
Summing out, we have use this trick so the Montecarlo estimate of the expectation is differentiable w.r.t. $\phi$. This then allow us to compute $L_{recon}$ which is one of the terms that we need to compute the total loss as defined as follows: -->






<!-- 

 need to make the stochastic element in  $q_{\phi}(\textbf{z}_n | \textbf{x}_n)$ be independent of $\phi$. Hence, we write $z$ as:

$$
z = \mu + \sigma \, \odot \, \epsilon \quad \text{with}\quad \epsilon \sim  N(0, I)
$$

 -->

<center>![](imgs/2023-12-09-20-06-58.png){.w550}</center><pre></pre>
Img Source at: regorygundersen.com 



## Q2.1



1. When q(z|x) is a dirac delta distribution the encoder always produce the same latent code z given input x. That means to obtain q(z) we can sample the encoders ouput for a given number of x inputs to obtain this distribution. We can accomplish this because the encoder will map x to a given z (deterministically) so the more you sample from the encoder given a particular x the more we get close to the true distribution q(z). Analytically, this can be expressed as follows:

$$
q(z) = \int_{x} q(z|x) p_d(x) dx
$$  
Where $p_d(x)$ represents the distribution of our data $x$.

2. When the posterior is a Gaussian distribution then the encoder will predict a mean $\mu$ and std $\sigma$ for each latent variable. To obtain q(z), similarly as the previous case, we can sample from q(z|x) and the resulting distribution will be gaussian with the parameters  $\mu$ and $\sigma$ dictated by the encoder's ouput. 

3. When an arbitraty complex posterioir distribution is used, we can think of the encoder as a function that takes input x, and random noise $\eta$ to produce a sample from $q(z|x, \eta)$ [Makhzani et al., 2015]. To obtain q(z) similarly as with the previous two methods we will sample as many times possible as to get close to the real distribution. Mathematically, we can express this distribution as follows:

$$
q(z) = \int_{x} \int_{\eta} q(z|x,\eta) p_d(x) p_{\eta}(\eta) d_{\eta}dx
$$  
Where $p_{\eta}(\eta)$ represents the distribution to account for the random noise.







 



:::{.callout-note collapse="false"}

# Research

The distribution $q(z)$ in the context of autoencoders is the approximation of the posterior distribution $p(z|x)$, which is the distribution of the latent variables $z$ given an observation $x$. The method for computing $q(z)$ depends on the assumptions made about the encoder:

1. **Deterministic function (Dirac delta distribution)**:
    - When the encoder is deterministic, $q(z|x)$ is modeled as a Dirac delta distribution centered around a single point, which means it always outputs the same $z$ for the same $x$.
    - Mathematically, for a deterministic encoder, $q(z|x) = \delta(z - f(x))$, where $f(x)$ is the deterministic function mapping $x$ to $z$. There is no uncertainty in $z$ given $x$; the output is a single point in the latent space.
    - To compute $q(z)$, you would essentially just collect the points $z$ for all $x$ in your dataset, which would give you an empirical distribution of the latent codes.

2. **Gaussian posterior**:
    - In this case, the encoder outputs parameters of a Gaussian distribution, typically the mean $\mu$ and standard deviation $\sigma$ (or variance $\sigma^2$) for each latent variable.
    - $q(z|x)$ is then a Gaussian distribution $\mathcal{N}(z; \mu(x), \sigma^2(x))$, where $\mu(x)$ and $\sigma^2(x)$ are the outputs of the encoder.
    - To compute $q(z)$, you sample from the Gaussian distribution parameterized by the mean and variance predicted by the encoder for each input $x$. During training, the reparameterization trick is often used to allow for gradient backpropagation through the random sampling process.

3. **Universal approximator posterior**:
    - Here, the encoder is capable of modeling arbitrarily complex distributions, not limited to Gaussian. This can be achieved using normalizing flows, autoregressive models, or other flexible density estimation techniques.
    - $q(z|x)$ in this case could be any parameterizable distribution that the encoder is designed to approximate.
    - Computing $q(z)$ involves using the encoder to output the parameters that define the distribution for each $x$, and then sampling from this distribution. Depending on the complexity, different techniques such as Monte Carlo sampling may be employed to approximate samples from $q(z|x)$.

In each case, the encoder learns to map inputs $x$ to the latent space in a way that captures the underlying distribution of the data. For deterministic encoders, this is straightforward, but for stochastic encoders (like the latter two cases), the process involves learning the parameters of the distributions that can generate the observed data when sampled.

### 2nd Source

**Question 2.1:**

**How to compute q(z)** based on each of the following encoders:

* A deterministic function, i.e. q(z|x) being a dirac delta distribution

* Gaussian posterior, i.e. the encoder predicts a mean and std per latent

* A universal approximator posterior, i.e. q(z|x) being arbitrary complex distributions

**Answer:**

**Deterministic function:**

If q(z|x) is a dirac delta distribution, then q(z) is simply the distribution of the latent codes z. This distribution can be computed by sampling from the encoder's output and then taking the average.

**Gaussian posterior:**

If the encoder predicts a mean and std per latent, then q(z) is a Gaussian distribution with the predicted mean and std.

**Universal approximator posterior:**

If q(z|x) is an arbitrary complex distribution, then q(z) can be computed by using a sampling-based approach. For example, we can sample from the encoder's output and then take the average.
:::



## Q2.2


In vanilla GANs, mode collapse problem occur because the generator is unable to produce diverse data samples. This could be as we saw in the lectures with the flowers example that the generator may have picked one single data point that believes it will fool the discriminator. If this strategy is sacksful then the generator has no reason to explore other possible data samples. Thus the generator gets stuck in producing images that look repetitive. AAE mitigate this problem by encouraging/guiding the generator to produce latent codes that are distributed according to p(z). We can do this because the adversarial network provides feedback to the encoder, which is also the generator in this adversarial network, about how well its latent codes match the prior distribution p(z). Thus, this procedure allows to the discriminator to give a signal on how the encoder is doing. Before with vanilla GANs there was not feedback loop that will guide the encoder to be diverse with this second network we regularize the autoencoder from overfitting to the training data.


:::{.callout-note collapse="false"}

# Research

Adversarial autoencoders (AAEs) are a type of generative model that can help to reduce the mode collapse problem compared to the vanilla GAN. AAE has the following advantages over GANs:

* **Encourages the generator to learn the latent space of the data:** The autoencoder in AAE learns the latent space of the data by compressing and then reconstructing the data. This helps to ensure that the generator learns a representation of the data that is consistent with the latent space.

* **Regularizes the generator:** The autoencoder in AAE also acts as a regularizer for the generator. This helps to prevent the generator from overfitting to the data and to instead focus on learning a general representation of the data.

* **Constrains the generator to generate realistic data:** The autoencoder in AAE helps to constrain the generator to generate realistic data. This is because the autoencoder needs to be able to reconstruct the data that the generator generates. As a result, the generator is encouraged to generate data that is similar to the data that was used to train the autoencoder.

Overall, AAE is a more stable and robust method for training GANs than the vanilla GAN. This is because AAE helps to prevent mode collapse and to encourage the generator to learn a good representation of the data. As a result, AAE can be used to train more realistic and diverse generative models.

Here is a table that summarizes the differences between AAE and GANs:

| Feature | Adversarial Autoencoder (AAE) | Vanilla GAN |
|---|---|---|
| Use of autoencoder | Yes | No |
| Regularization | Autoencoder acts as regularizer | No autoencoder |
| Consistency with latent space | Encouraged by autoencoder | Not explicitly encouraged |
| Likelihood of mode collapse | Lower | Higher |
| Realism of generated samples | Higher | Lower |
| Diversity of generated samples | Higher | Lower |

2.2 mine

The distribution q(z) helps the generator guide the generator in generating realistic data samples. The generator of the adversarial network is also the encoder of the autoencoder q(z|x). The goal of the adversarial autoencoder is to train the encoder to produce latent codes that are distributed according to p(z). The adversarial network helps to achieve this goal by providing feedback to the encoder about how well its latent codes match the prior distribution p(z). The discriminator in the adversarial network is trained to distinguish between real data and data that is generated by the autoencoder. The encoder is trained to produce latent codes that are so realistic that the discriminator cannot tell that they are not real.

By using an adversarial network, the adversarial autoencoder can be trained to generate data that is both realistic and diverse.

Once the training procedure is done, the decoder of the autoencoder will define a generative model
that maps the imposed prior of p(z) to the data distribution


### 2nd Source:

**Question 2.2:**

**How adversarial auto-encoders can reduce the mode collapse problem compared to the vanilla GAN.**

**Answer:**

Adversarial auto-encoders (AAEs) can reduce the mode collapse problem compared to vanilla GANs in a few ways. First, the autoencoder component of the AAE helps to regularize the generator and prevent it from overfitting to the training data. Second, the adversarial component of the AAE helps to encourage the generator to produce diverse samples.

Here is a more detailed explanation of how each of these components contributes to reducing mode collapse:

* **Autoencoder regularization:** The autoencoder component of the AAE is trained to reconstruct the input data. This helps to regularize the generator and prevent it from overfitting to the training data. Overfitting is a major cause of mode collapse in GANs, so by preventing overfitting, the AAE can help to reduce mode collapse.

* **Adversarial diversity:** The adversarial component of the AAE is trained to distinguish between real data and fake data generated by the generator. This encourages the generator to produce diverse samples, so that it can fool the discriminator. Mode collapse occurs when the generator learns to produce a small number of samples that can fool the discriminator. By encouraging the generator to produce diverse samples, the adversarial component of the AAE can help to reduce mode collapse.

Overall, AAEs are a more robust and stable method for training GANs than vanilla GANs. This is because AAEs help to prevent overfitting and encourage diversity, which are two common causes of mode collapse in GANs.



AAE can reduce this problem by imposing a prior distribution p(z) on the latent codes as this helps to:

1. Regularize the autoencoder from overfitting to the training data
2. Guide the generated data to follow a specific distribution p(z) so as to produce diverse samples. 

This helps to mitigate the mode collapse problem as how we saw the example in the lectures with the flowers. All generated flowers would be similar. There was no diversity. Thus if we can encourage the model to generate diverse samples then we can solve this problem. By guide here we mean that the adversarial network in this architecture will try to make


The distribution q(z) helps the generator guide the generator in generating realistic data samples. The generator of the adversarial network is also the encoder of the autoencoder q(z|x). The goal of the adversarial autoencoder is to train the encoder to produce latent codes that are distributed according to p(z). The adversarial network helps to achieve this goal by providing feedback to the encoder about how well its latent codes match the prior distribution p(z). The discriminator in the adversarial network is trained to distinguish between real data and data that is generated by the autoencoder. The encoder is trained to produce latent codes that are so realistic that the discriminator cannot tell that they are not real.


:::









