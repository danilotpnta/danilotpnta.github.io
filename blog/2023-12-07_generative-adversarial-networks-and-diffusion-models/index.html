<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Danilo Toapanta">
<meta name="dcterms.date" content="2023-12-07">
<meta name="description" content="Description of this Post">

<title>Generative Adversarial Networks and Diffusion models – Danilo Toapanta</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/danilo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<!-- <link href="../../site_libs/quarto-contrib/material-icons-0.14.2/mi.css" rel="stylesheet"> -->
<script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../css/index-posts.css">
</head>

<body class="floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <div id="quarto-announcement" data-announcement-id="af66dd50c39dd2ed9de488e10a192054" class="alert alert-primary hidden"><i class="bi bi-info-circle quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p><strong>Let op</strong> - This website is undergoing scheduled maintenance</p>
</div></div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title"><span id="danilo_topanta_brand"> Danilo Toapanta</span> <a id="mysite" class="mysite" href="../../site-ver-hist/">v1.3</a></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text"><span id="home-welcome-msg">Home</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/danilotpnta?tab=repositories" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full blog-page" style="display: none !important;">
    <div class="quarto-title-banner page-columns page-full">
        <div class="quarto-title column-body">
            <h1 class="title">Generative Adversarial Networks and Diffusion models</h1>
                
            <!-- Description Block -->
                        <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
                        
            <!-- Categories Block -->
                                            <div class="quarto-categories">

                    <!-- Display Categories -->
                                            <div class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=Deep Learning">
                                Deep Learning
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div> 
                    
                    <!-- Display Tags if any -->
                                    </div>
                            
        </div>
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">December 7, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    
</header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">
<script>
    var currentUrl = window.location.href;
    var index_init_post = currentUrl.lastIndexOf("/20");
    var string_init_post= currentUrl.slice(index_init_post, index_init_post+3 );

    // console.log("currentUrl: " + currentUrl);
    // console.log("index: " + index_init_post);
    // console.log("string: " + string_init_post);

    // If is equal to /blog/20... then make navbar title READING MODE
    if (string_init_post === "/20"){
        let mysite = document.getElementById("mysite");
        mysite.classList.add("mysite-change");

        let navbar = document.getElementById("danilo_topanta_brand");
        navbar.classList.add("navbar-brand-change");

        // This will render a new title saying READING DANILOS BLOG
        // navbar.innerHTML = 'You are Reading Danilo\'s Blog<span style="font-size:35px; vertical-align: middle; opacity: 0.65; padding-bottom: 6px; padding-left: 14px;" class="material-icons-round"> auto_awesome </span>';
        
        const smallDevice = window.matchMedia("(min-width: 570px)");
        smallDevice.addListener(handleDeviceChange);

        function handleDeviceChange(mediaQuery) {
            if (mediaQuery.matches) {
                navbar.innerHTML = "";
                // navbar.innerHTML = "<-- You are Reading Danilo's Blog -->";
            } else  {
                navbar.innerHTML = "Danilo Toapanta";
            }
        }

        // Run it initially
        handleDeviceChange(smallDevice);

        let link = document.getElementsByClassName("navbar-brand")[0];
        link.classList.add("disablePointerEvents");

        let brand_container = document.getElementsByClassName("navbar-brand-container")[0];
        brand_container.classList.add("navbar-brand-container-new-padding");

    }
</script>


<!-- <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Running my first Marathon</h1>
                  <div>
        <div class="description">
          I will be running at the 42km TCS Amsterdam 2023, 15th October
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">News</div>
              </div>
                  </div>
  </div> -->

  <!-- ---
  coming-soon: true
  tags: [Spanish]
  --- -->








<main id="title-block-header" class="quarto-title-block default page-columns page-full" style="padding-bottom: 40px;">

    <div class="quarto-title column-body" style="margin-bottom: 1em;">
        <h1 class="title" style="padding-bottom:8px" ;="">Generative Adversarial Networks and Diffusion models</h1>
        
        <!-- Description Block -->
                    <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
        
        <!-- Categories Block -->
                    
                <!-- Display Categories -->
                <div class="quarto-categories">
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title">
                        <i class="fa-solid fa-hashtag" ></i> Categories:
                    </div> -->

                                            <div id="All" class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div>
                                            <div id="Deep Learning" class="quarto-category">
                            <a href="../../blog/#category=Deep Learning">
                                Deep Learning
                            </a>
                        </div>
                                            <div id="TAGS" class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div>
                                    </div>
                


                <div class="quarto-categories tag-categories">
                    
                    <!-- Tags Icon  -->
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title"> -->
                        <!-- <i class="fa-solid fa-tag" ></i> Tags: -->
                        <!-- <i class="fa-solid fa-hashtag" ></i> Tags: -->
                        <!-- <span class="material-icons-outlined" >local_offer</span> Tags: -->
                        <!-- / -->
                    <!-- </div> -->

                    <!-- Display Tags -->
                                            <div id="All-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=All">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                All
                            </a>
                        </div>
                                            <div id="Deep Learning-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=Deep Learning">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                Deep Learning
                            </a>
                        </div>
                                            <div id="TAGS-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=TAGS">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                TAGS
                            </a>
                        </div>
                    
                    
                </div>

                    
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">December 7, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    <!-- Current link: Font-awesome, Google icons, Bootstrap icons -->
    
</main>


<!-- ## Title
<center>![Slide 1](imgs/page_1.jpeg){.w575}</center><pre></pre>


## Organisation
<center>![Slide 2](imgs/page_2.jpeg){.w575}</center><pre></pre> -->
<section id="lecture-overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="lecture-overview"><span class="header-section-number">1</span> Lecture overview</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_3.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 3</figcaption>
</figure>
</div>
</center>
<pre></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What are implicit density models?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>These models are “implicit” because they do not explicitly define the probability distribution, but rather define it indirectly. Instead of specifying a probability density function, implicit density models rely on generating samples from a distribution and use these samples to implicitly model the underlying probability distribution.</p>
<p>One popular approach to implicit density modeling is through the use of generative models</p>
</div>
</div>
</div>
</section>
<section id="a-map-of-generative-models" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="a-map-of-generative-models"><span class="header-section-number">2</span> A map of generative models</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_4.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 4</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here in the implicity density you can sample from the probability distribution, but it cannot give you an estimate of how likely a particular image is</p>
</section>
<section id="last-time" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="last-time"><span class="header-section-number">3</span> Last time</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_5.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="today" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="today"><span class="header-section-number">4</span> Today</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_6.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here we say Generated as refering to fake, because it was generated. The <strong>Discriminator</strong> would be in such a manner that it will learn how to tell between real and fake. And the <strong>Generator</strong> will also be a NN trained in a manner that tries to fool the Discriminator</p>
</section>
<section id="explicit-density-vs-implicit-density" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="explicit-density-vs-implicit-density"><span class="header-section-number">5</span> Explicit density vs implicit density</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_7.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li>Purple: if x was taken from the true distribution then P_theta is high
<ul>
<li>Explicit and VAE kinda do both, it includes both things. Explicitly density is more difficult because we can do both escentially.</li>
</ul></li>
<li>Green: samples taken from p_theta they should behave similar to the real sample
<ul>
<li>Here we can get a sample but we cannot give get the distribution p_theta of x so the probability values</li>
</ul></li>
</ul>
</section>
<section id="learning-an-implicit-density-function" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="learning-an-implicit-density-function"><span class="header-section-number">6</span> Learning an implicit density function</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_8.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 8</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>If the generation is not possible, then lets train some gradients to train the generator.</p>
</section>
<section id="generations-of-high-quality-various-potential-applications" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="generations-of-high-quality-various-potential-applications"><span class="header-section-number">7</span> Generations of high quality, various potential applications</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_9.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="title" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="title"><span class="header-section-number">8</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_10.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>LEFT:</p>
<ol type="1">
<li>Sample on image from the dataset</li>
<li>Put it through the differentiable function <span class="math inline">\(D\)</span>, <code>Discriminator</code>. Now the descriminator for real iamges will try to retunr a value near 1, if is from the real image.</li>
</ol>
<p>RIGHT: Generated image</p>
<ol type="1">
<li>Sample noise</li>
<li>Put it into the generator <span class="math inline">\(G\)</span> network and we will end up with some image.</li>
</ol>
<p>Then we do it all over again. Sample x from data, but now the Discriminator tries to detect if its wrong, a fake image. The Generator then tries to fool it</p>
</section>
<section id="what-is-a-gan" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="what-is-a-gan"><span class="header-section-number">9</span> What is a GAN?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_11.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="gan-intuition-arms-race" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="gan-intuition-arms-race"><span class="header-section-number">10</span> GAN: Intuition: arms race</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_12.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>At the start the discriminator will discriminate well. The analogy is of detecting money. At the begining, we want to detect whether the money is true. Both do not have clue. At the beginning the police is the baby and the money looks rather fake. You start trianing these and they become better.</p>
<p>When they reach Nash equilibrium it means they cannot improve their solution in its own. It may not be a globally optimal solution but that is how the Nash equilibrium is defined.</p>
</section>
<section id="title-1" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="title-1"><span class="header-section-number">11</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_13.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="gan-architecture" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="gan-architecture"><span class="header-section-number">12</span> GAN architecture</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_14.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="gan-has-no-encoder-its-a-discriminator" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="gan-has-no-encoder-its-a-discriminator"><span class="header-section-number">13</span> GAN has no ”encoder” — it’s a discriminator</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_15.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>GAN has no encoder but instead it has a discriminator. The features that the discriminator learn are not necessarily as noise as for example the higuer level features may be car types or not. Because for detecting real vs fake all what we need is for example if the image is blurry. GANs do not end up with blurry features if trained correctly vs VAE tend to end up with blurry features</p>
<p>We cannot test the likelihood but we can only sample new data points</p>
</section>
<section id="generator-network-x-gz-theta_g" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="generator-network-x-gz-theta_g"><span class="header-section-number">14</span> 1) Generator network x = G(z; <span class="math inline">\(\theta\)</span>_G)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_16.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We start with a lower dimensionality z, and generate x, except we are not giveing z any constraints. We are simply sampling it from some distribution. Since i.e you are sampling from a Gaussian distribution 0, 1, whithou any correlations between the dimensions of z, it means you can sample new samples easility because you just generate a new noise sample and is very likely that if you sample a new one, this has not been seen yet. So then this new image that is not seen it goes trhough the network and you get a completely new sample.</p>
<p>In the image on the right you see de-convolutions happening to create a new image. With transpose convolutions you also have the bias that neighboring pixels are similar and that operations are shared across the spatial dimensions</p>
</section>
<section id="discriminator-network-y-dx-theta_d9" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="discriminator-network-y-dx-theta_d9"><span class="header-section-number">15</span> 2) Discriminator network y = D(x; <span class="math inline">\(\theta\)</span>_D9)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_17.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The loss that we will learn is the binary cross entropy loss, because we are simply trying to detect between one and another class so the loss is a binary Cross entropy loss. We want to ouput 1 if real, 0 if fake,</p>
</section>
<section id="generator-discriminator-implementation" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="generator-discriminator-implementation"><span class="header-section-number">16</span> Generator &amp; Discriminator: Implementation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_18.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="quiz-the-starting-point-of-the-gan-is-the-random-noise-z." class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="quiz-the-starting-point-of-the-gan-is-the-random-noise-z."><span class="header-section-number">17</span> Quiz: The starting point of the GAN is the random noise z.</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_19.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ol type="1">
<li>Is always true because anything can be called regularizer. It simply means help training</li>
<li>Not true because you are really using augmentations. You dont want a network that i.e to ouput heavily color jiter ouputs or you dont want vertically flipped images. Noisy isn’t really meant for that</li>
<li>It is not true because it is trained in a manner that it should do because as long as you sampling z from the Gaussian distribution so you could end up with a point that is very far away but is unlikely</li>
<li>If your dataset is small enough, for i.e it is only 5k points then, one millions points is close to be random at that point. And should work because even if you grid is a fixed thins and your model needs to map this to a dog and this to a cat. It will do it by itself</li>
</ol>
<p>Z needs to vary. If z is much smaller than the dataset size you will have trouble becuase then you can only generate say 3 different kind of classes but the dataset is 4 classes. Then the discriminator will have an easy time at telling what is real and what is fake</p>
</section>
<section id="how-do-we-train-the-generator" class="level2" data-number="18">
<h2 data-number="18" class="anchored" data-anchor-id="how-do-we-train-the-generator"><span class="header-section-number">18</span> How do we train the generator?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_20.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We generate somthing that looks like a deer and we can compare it to an image of a deer. We cannot do this. They are completely independent</p>
<p>The question is how can we get meaningfull gradients if we are not comaparing the same things?</p>
<p>We have:</p>
<ol type="1">
<li>Minimax Loss</li>
<li>Heuristic non-saturating loss</li>
<li>Modifying GANs for max-likelihood</li>
</ol>
</section>
<section id="minimax-loss" class="level2" data-number="19">
<h2 data-number="19" class="anchored" data-anchor-id="minimax-loss"><span class="header-section-number">19</span> 1) Minimax Loss</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_21.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>So basically the loss of the generator is the negative version of the discriminator. his make it symmetric, the higher one the lower the other</p>
<p>It is called minimax because the discriminator tries to maximize its loss so <span class="math inline">\(J_D\)</span> and then the generator tries to minimize that loss. What you end up with this min max loss is topically a saddle point. See next slide</p>
</section>
<section id="minimax-loss-1" class="level2" data-number="20">
<h2 data-number="20" class="anchored" data-anchor-id="minimax-loss-1"><span class="header-section-number">20</span> 1) Minimax Loss</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_22.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We get a saddle point as the learning stops at some point. At that point we have have gradients being zero in both directions</p>
<p>The generator first will not recieve any gradient anymore and at that point the discriminator will also not get any gradients any more. This behaviour is not great because it allows for easy analysis but it will cause training to get stuck because the job of the discriminator is very easy at that point.</p>
</section>
<section id="heuristic-non-saturating-loss" class="level2" data-number="21">
<h2 data-number="21" class="anchored" data-anchor-id="heuristic-non-saturating-loss"><span class="header-section-number">21</span> 2) Heuristic non-saturating loss</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_23.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>This is the adapeted version of minmax which is a non saturated loss.</p>
<p>Changes pretty easily in that, changes to the discriminator loss as it is. And instead you use the generator loss.</p>
<p>Now, you cannot describe the equilibrium by a single loss anymore as we did earlier with the minus loss. That means you dont end up at a saddle point. The discriminator job is now to maximize the log-likelihood of discovering the real samples and the fake samples. And the generator maximizes the log likelihood of the discriminator being wrong</p>
<p>What changes now is that the generator learns still when the discriminator is too good on real images. This is because previously the generator loss is also this mixture of how good the discriminator is on real images and how good it is in fake iamges and now the generator loss only depend on the generator ouputs</p>
<p>What you generate with GANs can look like augmentations but depending on the augmentation, z can model that.</p>
<p>A generator is not a different augmentation, a generator has a whole different purpose that you actually can model and generate whole new images. It is not like these faces are just slightly blurrier or slightly brighter of existing faces. It is literally new samples that do not exist in the real word. This is different from augmentations</p>
<p>In the above formulas we can sample three batches from the generator and one batch from the real distribution and still calculate the loss. You can leave out the 1/2, you can simply multiply by the learning rate by 1/2 and then you end with the same result. These batches are computed in average so you could compute this loss with ten times as many from the real one and only one from the real distribution</p>
</section>
<section id="modifying-gans-for-max-likelihood" class="level2" data-number="22">
<h2 data-number="22" class="anchored" data-anchor-id="modifying-gans-for-max-likelihood"><span class="header-section-number">22</span> 3) Modifying GANs for max-likelihood</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_24.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>By computing this loss we have the advantage that when the discriminator is optimal, the gradient for the generator gradient matches that of maximum likelihood.</p>
<!-- A value of 0 on the x-axis means that the discriminator is unable to distinguish between real and fake samples, while a value of 1 means that the discriminator is able to perfectly distinguish between real and fake samples.

In other words, the x-axis represents the difficulty of the task for the generator. When the discriminator is too good (i.e., D(G(z)) is close to 1), the generator has a harder time fooling the discriminator. This is why the generator's loss is higher when the discriminator is too good. -->
<!-- The plot shows that the non-saturating heuristic loss is more effective at training GANs when the discriminator is too good. This is because the non-saturating heuristic loss has a non-flat loss curve when D(G(z)) is close to 1. This means that the generator has gradients to follow and can continue to improve its performance. -->
</section>
<section id="comparison-of-generator-losses" class="level2" data-number="23">
<h2 data-number="23" class="anchored" data-anchor-id="comparison-of-generator-losses"><span class="header-section-number">23</span> Comparison of Generator Losses</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_25.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>0 means we can detect the fake images and thus we set them with a lower value. When is 1 it means that the discriminator believes that the generated image is a true image.</p>
<p>1: Not a good discriminator because he thinks the fake image is true</p>
<p>0: Good discriminator, can distinguish between fake an real images</p>
</section>
<section id="optimal-discriminator" class="level2" data-number="24">
<h2 data-number="24" class="anchored" data-anchor-id="optimal-discriminator"><span class="header-section-number">24</span> Optimal discriminator</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_26.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The real distribution given the data is given in black. The model distribution is in green, that is the thing that the generator would be adapting. The discrimnator after a while stops learning. That is the picture from the right. The discriminator tries to see where the overlap is. At the last point the discriminator measure this ratio of overlaps</p>
</section>
<section id="why-is-this-the-optimal-discriminator" class="level2" data-number="25">
<h2 data-number="25" class="anchored" data-anchor-id="why-is-this-the-optimal-discriminator"><span class="header-section-number">25</span> Why is this the optimal discriminator?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_27.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>X: is data</p>
<p>z: is the generated data</p>
<p>We sum the losses, in the second row we rewrite z into another x</p>
</section>
<section id="gans-and-jensen-shannon-divergence" class="level2" data-number="26">
<h2 data-number="26" class="anchored" data-anchor-id="gans-and-jensen-shannon-divergence"><span class="header-section-number">26</span> GANs and Jensen-Shannon divergence</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_28.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The Jenses-Shannon divergence (in grey) is a metric to compare two distributions, it compares them by taking the weighted average of the KL.</p>
<p>Now solving for the optimal discriminator and you get the two term. There you can see these two integrals. Rewriting we have that the loss is two times the Jensen-Shannon between the two probabilities distributions. For the optimal loss -2log2. Then that means the Jensen divergence is minimized, meaning it becomes zero. What we are optimizing is actually the Jensen divergence between these two probabilities distributions.</p>
</section>
<section id="is-the-divergence-important" class="level2" data-number="27">
<h2 data-number="27" class="anchored" data-anchor-id="is-the-divergence-important"><span class="header-section-number">27</span> Is the divergence important?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_29.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The question is why is the divergence important, it could be any kind of divergence like the KL and another ones.</p>
</section>
<section id="kl-vs-js" class="level2" data-number="28">
<h2 data-number="28" class="anchored" data-anchor-id="kl-vs-js"><span class="header-section-number">28</span> KL vs JS</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_30.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here we see that the JS is symmetric</p>
</section>
<section id="is-the-divergence-important-1" class="level2" data-number="29">
<h2 data-number="29" class="anchored" data-anchor-id="is-the-divergence-important-1"><span class="header-section-number">29</span> Is the divergence important?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_31.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>It matters which divergence metric we take depending on what we do. You will either capture something that is in between two modes of the data which means the samples you will get will not be realistic or only rarely. Or the other option is that you capture one mode i.e with the reversed KL, except that in our case is the other way around which means the KL wil give you the mode capturing so depending on how you write you will have this risk of adversion behaviour this mode capturing behaviour.</p>
<p>For the backward KL is also called ‘zero forcing’ which make it more conservative, it will try to avoid putting probability mass where there is none. So it will avoid this zero areas whereas on the other way around is completely fine</p>
</section>
<section id="general-observations" class="level2" data-number="30">
<h2 data-number="30" class="anchored" data-anchor-id="general-observations"><span class="header-section-number">30</span> General observations</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_32.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="training-procedure" class="level2" data-number="31">
<h2 data-number="31" class="anchored" data-anchor-id="training-procedure"><span class="header-section-number">31</span> Training procedure</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_33.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="how-research-gets-done-part-8" class="level2" data-number="32">
<h2 data-number="32" class="anchored" data-anchor-id="how-research-gets-done-part-8"><span class="header-section-number">32</span> How research gets done part 8</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_34.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="challenges-of-training-gans" class="level2" data-number="33">
<h2 data-number="33" class="anchored" data-anchor-id="challenges-of-training-gans"><span class="header-section-number">33</span> Challenges of Training GANs</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_35.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>There is three versions Loss of GANS and the original one (minimax) does not work in practice</p>
</section>
<section id="challenge-1-vanishing-gradients" class="level2" data-number="34">
<h2 data-number="34" class="anchored" data-anchor-id="challenge-1-vanishing-gradients"><span class="header-section-number">34</span> Challenge 1: Vanishing Gradients</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_36.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here in the right you see a trained GAN model. You keep the generator fix and you only train the discriminator for a few iterations. Then what you can observe is that in the y-axis is the loss that you would get to the generator so you keep the generator fix but you simply look at the loss that the generator would get for this given discriminator. What you can see is that the loss is given in absolute value and you can see that the blue line goes from 10^0 to 10^-3. So it crosses three levels of magnitude the red one even crosses 5 values of magnitud which means the discriminator can learn to distinguish this real-true extremely quickly and at that point your gradients are 10^-6 so then is like zero. At that point you do not get gradients anymore if the discriminator is really good, so the gradients will go to zero.</p>
<p>If the discriminator is bad then the generator will not get any good gradients as well which means it will not get good training. Specially earlier in the training where everything is bad. The discriminator will have an easier time because it can still recognize these weird patches and rare edges, so some solutions is that you give to the generator more iterations. Another solution is that you add an encoder that encodes a real image and then you discriminate on this encoding of the image</p>
</section>
<section id="challenge-2-low-dimensional-supports" class="level2" data-number="35">
<h2 data-number="35" class="anchored" data-anchor-id="challenge-2-low-dimensional-supports"><span class="header-section-number">35</span> Challenge 2: Low dimensional supports</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_37.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Another challenge is the with low-dimensional manifolds. They idally capture a theme or an object and once you have them all the pixels can be generated from it very easily. That is why this low dimensional embedding space. However if this manifolds (which are surfaces or lines) they do not match at all so it is very easy for the discriminator to simple put a line trough this and understand they are different.</p>
<p>In training the manifold is not completely perfect so it is extremelly easy for these models to discriminate this because the Jensen divergence gives you extremelly low value if they do not have an overlap. So here you have a chicken-egg problem that the loss will start being meningful once your generationed data is good.</p>
<p>For this we have the solution wGAN which basically uses other metric to compare the real vs false. So even if you have 2D surfaces, such as here, you can see that the overlap will be fairly low that means the discriminator has a easy time</p>
</section>
<section id="challenge-3-batch-normalization" class="level2" data-number="36">
<h2 data-number="36" class="anchored" data-anchor-id="challenge-3-batch-normalization"><span class="header-section-number">36</span> Challenge 3: Batch Normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_38.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Batch norm is a problem here because you are mixing for example batches of real images and batches from not real images. Then you compute the bach statistics from this combination. That leads to a smooth but awkward faces. So instead of this you can do the following. See next slide</p>
</section>
<section id="reference-batch-normalization" class="level2" data-number="37">
<h2 data-number="37" class="anchored" data-anchor-id="reference-batch-normalization"><span class="header-section-number">37</span> Reference batch normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_39.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here we can keeo a refence batch. Say you train with two minibatches, you have a reference batch where you compute the mean and standard deviation. Then you use the other second minibatch for training. However this lack of variation wrt to the reference batch will give you troubles because one of the main benefits from batch norm was that it sort of gives you this variability and regularization effect. A solution for that is</p>
</section>
<section id="virtual-batch-normalization" class="level2" data-number="38">
<h2 data-number="38" class="anchored" data-anchor-id="virtual-batch-normalization"><span class="header-section-number">38</span> Virtual batch normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_40.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="challenge-4-convergence" class="level2" data-number="39">
<h2 data-number="39" class="anchored" data-anchor-id="challenge-4-convergence"><span class="header-section-number">39</span> Challenge 4: Convergence</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_41.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The other challenge is convergence. In the context of Generative Adversarial Networks (GANs), convergence to a saddle point is the desired outcome of the training process. This is because saddle points represent a stable equilibrium point in the game between the generator and the discriminator, where neither agent can unilaterally improve its performance. In other words, both the generator and the discriminator are “optimal” in their respective roles, but they cannot push each other further towards their respective optima without sacrificing their own performance.</p>
<p><strong>The Challenges of Saddle Points</strong></p>
<p>While saddle points are the desired outcome of GAN training, they can be challenging to achieve due to the complex and non-convex nature of the optimization landscape. This means that there are many different paths that the training process can take, and it is possible to get stuck in local optima that are not truly optimal.</p>
</section>
<section id="challenge-mode-collapse" class="level2" data-number="40">
<h2 data-number="40" class="anchored" data-anchor-id="challenge-mode-collapse"><span class="header-section-number">40</span> Challenge: mode collapse</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_42.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 42</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p><strong>Mode-collapse</strong> is a phenomenon that occurs when the generator becomes unable to generate a diverse range of samples. Instead, the generator becomes fixated on producing a small subset of samples, effectively “collapsing” into a single mode. This can result in blurry, repetitive, or unrealistic generated images.</p>
<p>For example if you have this simple target distribution of these 8 Gaussians and you want to train a generative model, it sometimes will fit towards one of these modes and it will have a difficult time fitting to one of these modes of input data. Similarly for image based models you can see that all these images of flowers they literally look the same, so there is no variation capture at all. It is simply capturing one of the modes. This happens because if the discriminator wants to see that it look similar to the real data. Then the Generator can simply learn only one target and not care about divergence at all.</p>
<p>This is not desired and so a solution would be to add diversity as a regularization loss</p>
</section>
<section id="potential-solution-regularize-for-diversity" class="level2" data-number="41">
<h2 data-number="41" class="anchored" data-anchor-id="potential-solution-regularize-for-diversity"><span class="header-section-number">41</span> Potential solution regularize for diversity</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_43.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The solution for falling into mode-collapse is that you compare each sample by the other examples in the mini-batch. Yuo check whether you can discriminate between these two. You look what is the entropy whithin these samples. And you can add this as a loss. If you simply inforce a huge amount of entropy whithin the mini-batch then this will give more more varied exmaples but this will not look realistic pictures anymore</p>
</section>
<section id="mode-collapse-vs-over-generalisation" class="level2" data-number="42">
<h2 data-number="42" class="anchored" data-anchor-id="mode-collapse-vs-over-generalisation"><span class="header-section-number">42</span> Mode-collapse vs over-generalisation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_44.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here in over-generalization is not like you train with cat and dogs and then you can generate pictures of planes. Here in generalization images go over the training set which generally means they are looking realistic</p>
<p>In the case of unconditional generation, we generate dogs and catgs. In the case con conditinal generation we generate dogs from different type of dogs</p>
</section>
<section id="challenge-how-to-evaluate" class="level2" data-number="43">
<h2 data-number="43" class="anchored" data-anchor-id="challenge-how-to-evaluate"><span class="header-section-number">43</span> Challenge: how to evaluate?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_45.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>FReshee inception distance, You push the real and the generated images of same size through a pre-trained inception network. You then use the dense features and compare these dense features by contruction a Gaussian distribution out of them. So you then comparing them by simply taking the difference between the dataset wise features. This tells me how good a set of generated images matches another images.</p>
<ul>
<li><p>Low FID: the distribution of the real images is similar to the fake ones.<br>
For instance if you do mode dropping the FID will be low because the distance to all these dog images would be suddenly pretty high</p></li>
<li><p>High FID: the distribution of the real images is <em>different</em> to the fake ones.<br>
If these things do not look at all like the real class then the FDI would be high</p></li>
</ul>
<p>The Fréchet Inception Distance (FID) is a metric used to measure the similarity between the distribution of real and fake images generated by a GAN. It is calculated by comparing the activations of a pre-trained InceptionNet model on both the real and fake images.</p>
<p>The FID is a powerful metric for evaluating the performance of GANs, as it can provide insights into both the realism and diversity of the generated images. A <strong>lower</strong> FID indicates that the distribution of the fake images is more similar to the distribution of the real images, suggesting that the GAN is generating more realistic and diverse images.</p>
</section>
<section id="challenge-beyond-images" class="level2" data-number="44">
<h2 data-number="44" class="anchored" data-anchor-id="challenge-beyond-images"><span class="header-section-number">44</span> Challenge: beyond images</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_46.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li>Text are discrete chunks, so you cannot model as continous variables where you can simply push the gradietns trough, which is what we are doing for RGB images. For these iamges we are pretending that they are continuous.</li>
</ul>
</section>
<section id="some-open-challenges-for-gans" class="level2" data-number="45">
<h2 data-number="45" class="anchored" data-anchor-id="some-open-challenges-for-gans"><span class="header-section-number">45</span> Some open challenges for GANs</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_47.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 47</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p><strong>What sorts of distributions can GANs model?</strong></p>
<p>GANs can model a wide variety of distributions, including both continuous and discrete distributions. For example, GANs have been used to generate images, text, music, and code.</p>
<p><strong>What can we say about the global convergence of the training dynamics?</strong></p>
<p>The global convergence of GAN training dynamics is a challenging problem that is not fully understood. However, there has been some progress in developing theoretical guarantees for the convergence of GANs. For example, it has been shown that GANs can converge to a Nash equilibrium under certain conditions.</p>
<p><strong>How should we evaluate GANs and when should we use them?</strong></p>
<p>There is no single best way to evaluate GANs. However, some common evaluation metrics include the Fréchet Inception Distance (FID) for image generation and the BLEU score for text generation. GANs should be used when the goal is to generate realistic and diverse samples from a given distribution.</p>
<p><strong>GAN scaling: dataset size and model size</strong></p>
<p>GANs can be scaled to large datasets and model sizes. However, scaling GANs can be challenging, as it can lead to training instability and mode collapse. There are a number of techniques that have been developed to help scale GANs, such as gradient clipping and spectral normalization.</p>
<p><strong>GANs and adversarial examples?</strong></p>
<p>GANs have been used to generate adversarial examples, which are inputs that are designed to fool machine learning models. Adversarial examples can be a security risk, as they can be used to attack machine learning models that are used in critical applications, such as facial recognition and self-driving cars.</p>
</section>
<section id="one-sided-label-smoothing" class="level2" data-number="46">
<h2 data-number="46" class="anchored" data-anchor-id="one-sided-label-smoothing"><span class="header-section-number">46</span> One-sided label smoothing</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_48.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 48</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="benefits-of-label-smoothing" class="level2" data-number="47">
<h2 data-number="47" class="anchored" data-anchor-id="benefits-of-label-smoothing"><span class="header-section-number">47</span> Benefits of label smoothing</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_49.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 49</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="ganss-sometimes-explode" class="level2" data-number="48">
<h2 data-number="48" class="anchored" data-anchor-id="ganss-sometimes-explode"><span class="header-section-number">48</span> GANSs sometimes explode</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_50.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 50</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="title-2" class="level2" data-number="49">
<h2 data-number="49" class="anchored" data-anchor-id="title-2"><span class="header-section-number">49</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_51.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 51</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dcgan" class="level2" data-number="50">
<h2 data-number="50" class="anchored" data-anchor-id="dcgan"><span class="header-section-number">50</span> DCGAN</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_52.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 52</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="examples" class="level2" data-number="51">
<h2 data-number="51" class="anchored" data-anchor-id="examples"><span class="header-section-number">51</span> Examples</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_53.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 53</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="even-vector-space-arithmetics" class="level2" data-number="52">
<h2 data-number="52" class="anchored" data-anchor-id="even-vector-space-arithmetics"><span class="header-section-number">52</span> Even vector space arithmetics …</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_54.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 54</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="can-generate-new-views" class="level2" data-number="53">
<h2 data-number="53" class="anchored" data-anchor-id="can-generate-new-views"><span class="header-section-number">53</span> Can generate new views</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_55.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 55</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="wasserstein-gan" class="level2" data-number="54">
<h2 data-number="54" class="anchored" data-anchor-id="wasserstein-gan"><span class="header-section-number">54</span> Wasserstein GAN</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_56.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 56</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>You can get gradients even if this two distributions do no match. Here we would have a 1 W value indicating the distributions do not match at all. So even if the discriminator is having an easy time, you still are getting gradients</p>
</section>
<section id="differences-in-gans" class="level2" data-number="55">
<h2 data-number="55" class="anchored" data-anchor-id="differences-in-gans"><span class="header-section-number">55</span> Differences in GANs</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_57.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 57</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bigbigan" class="level2" data-number="56">
<h2 data-number="56" class="anchored" data-anchor-id="bigbigan"><span class="header-section-number">56</span> BigBiGAN</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_58.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 58</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="so-what-changed-more-data-no" class="level2" data-number="57">
<h2 data-number="57" class="anchored" data-anchor-id="so-what-changed-more-data-no"><span class="header-section-number">57</span> So what changed? More data? – No</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_59.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 59</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="so-what-changed-architectures-and-compute-yes" class="level2" data-number="58">
<h2 data-number="58" class="anchored" data-anchor-id="so-what-changed-architectures-and-compute-yes"><span class="header-section-number">58</span> So what changed? Architectures and compute: yes</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_60.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 60</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="conditional-gan" class="level2" data-number="59">
<h2 data-number="59" class="anchored" data-anchor-id="conditional-gan"><span class="header-section-number">59</span> Conditional GAN</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_61.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 61</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="image-to-image-translation" class="level2" data-number="60">
<h2 data-number="60" class="anchored" data-anchor-id="image-to-image-translation"><span class="header-section-number">60</span> Image to image translation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_62.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 62</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="adversarial-autoencoders-and-adversarial-network-in-latent-space" class="level2" data-number="61">
<h2 data-number="61" class="anchored" data-anchor-id="adversarial-autoencoders-and-adversarial-network-in-latent-space"><span class="header-section-number">61</span> Adversarial AutoEncoders: and adversarial network in latent space</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_63.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 63</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In the autoencoder we want to minimize the reconstructed error. Here we simply add an adversarial loss in the latent space <span class="math inline">\(z\)</span>. That means we will now be containing the latent space in a manner that it will be easy to sample from.</p>
<p><strong>Encoder:</strong> This part takes an input image x and encodes it into a latent representation z through a deterministic function q(z|x), which is typically a neural network. The deterministic aspect means that for the same input x, the encoder will always produce the same latent code z.</p>
<center>
<img src="imgs/2023-12-14-17-09-46.png" class="w300 img-fluid">
</center>
<pre></pre>
<p>Here the q(z|x) is called the variational posterior. It can be Gaussian and then we will make it Gaussian</p>
<p>Before we have that we wanted to approximate the real posterior p(z|x) with this q(z|x). So here we are saying that before we use the KL in VAE to give it structure but not the AAE uses an adversarial network for this.</p>
<p>Remember that: p(z∣x) is the true posterior distribution of the latent variables given the input data, which is generally intractable to compute directly. Remember the picture below we were unable to commpute this posterior because it related to compute this integral</p>
<center>
<img src="imgs/2023-12-14-17-17-06.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>The AAE does not need to operate in the image domain it can just work in the latent domain. It also does not need to be image model but instead a bunch of MLPs</p>
</section>
<section id="cyclegan-img2img-models" class="level2" data-number="62">
<h2 data-number="62" class="anchored" data-anchor-id="cyclegan-img2img-models"><span class="header-section-number">62</span> CycleGAN: “img2img” models</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_64.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 64</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>It is called cycle because you go from one domain to another, if you look at for example image B. You go from domian x to y and then you go back to domain x. And then you know that you should end up in the same place. If you do then that means that the mapping worked perfectly</p>
</section>
<section id="stylegan-and-styleganv2" class="level2" data-number="63">
<h2 data-number="63" class="anchored" data-anchor-id="stylegan-and-styleganv2"><span class="header-section-number">63</span> StyleGAN and StyleGANv2</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_65.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 65</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="title-3" class="level2" data-number="64">
<h2 data-number="64" class="anchored" data-anchor-id="title-3"><span class="header-section-number">64</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_66.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 66</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="overview-of-methods" class="level2" data-number="65">
<h2 data-number="65" class="anchored" data-anchor-id="overview-of-methods"><span class="header-section-number">65</span> Overview of methods</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_67.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 67</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="basic-idea-of-diffusion-models-learning-how-to-denoise" class="level2" data-number="66">
<h2 data-number="66" class="anchored" data-anchor-id="basic-idea-of-diffusion-models-learning-how-to-denoise"><span class="header-section-number">66</span> Basic idea of diffusion models: learning how to denoise</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_68.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 68</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>You start with a real image and then you start increasingly adding noise. So you are trying to learn something that is noisy to something that is less noisy.</p>
<p>This is a completely new paradigm</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This is how difussion models work
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Forward Process (Adding Noise):</strong> Starting with an original image <span class="math inline">\(X_0\)</span>, noise is gradually added over a series of steps until the image becomes a noisy version <span class="math inline">\(X_T\)</span> that is typically assumed to follow a Gaussian distribution. This process is termed the forward diffusion process and is denoted by the rightward arrows in the image. Each step adds a controlled amount of noise, progressively making the image less recognizable and more like random noise.</p></li>
<li><p><strong>Reverse Process (Removing Noise):</strong> The key idea behind diffusion models is to learn how to reverse this noising process. This is where the model learns to ‘denoise’ the image. Starting from a noisy image <span class="math inline">\(X_T\)</span>, the model attempts to recover the clean image <span class="math inline">\(X_0\)</span> through a series of reverse steps. This is the reverse diffusion process and is indicated by the leftward arrows in the image. Each step in the reverse process is denoted by <span class="math inline">\(p_\theta(X_{t-1}|X_t)\)</span>, which is the learned distribution to predict the cleaner image <span class="math inline">\(X_{t-1}\)</span> from the noisier image <span class="math inline">\(X_t\)</span>.</p></li>
<li><p><strong>Variational Lower Bound:</strong> The training of diffusion models involves optimizing the variational lower bound, which is a way to ensure that the learned distribution over the reverse process closely matches the true distribution of the data. It’s a technique derived from variational inference used to approximate complex distributions.</p></li>
<li><p><strong>Unknown Transition Distributions:</strong> In the diagram, it is noted that <span class="math inline">\(q(X_{t-1}|X_t)\)</span> is unknown. This represents the true reverse transition probabilities from a noisier to a less noisy image, which we do not have explicitly. The model has to learn an approximation <span class="math inline">\(p_\theta(X_{t-1}|X_t)\)</span> without knowing <span class="math inline">\(q(X_{t-1}|X_t)\)</span>.</p></li>
<li><p><strong>Learning Process:</strong> During training, the model learns the parameters <span class="math inline">\(\theta\)</span> that define how to reverse the noise added during the forward process effectively. The learned distribution <span class="math inline">\(p_\theta(X_{t-1}|X_t)\)</span> is used to iteratively generate less noisy images until a clear image is formed.</p></li>
<li><p><strong>Image Generation:</strong> Once trained, diffusion models can generate new images by sampling from the noise distribution <span class="math inline">\(X_T\)</span> and applying the reverse process. This allows the model to create images that were not in the training set but share the same statistical properties.</p></li>
</ol>
</div>
</div>
<p>Escentially what you are trying to learn is the green function which is how to reverse the noise added during the forward process</p>
</section>
<section id="diffusion-models-turn-generative-learning-into-a-sequence-of-supervised-problems" class="level2" data-number="67">
<h2 data-number="67" class="anchored" data-anchor-id="diffusion-models-turn-generative-learning-into-a-sequence-of-supervised-problems"><span class="header-section-number">67</span> Diffusion models turn generative learning into a sequence of supervised problems</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_69.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 69</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>These difussion models basically turn generative modelling into a sequence of supervised models because you have your image and you have noise. So you know how it looks before so you simply are trying to turn this whole dinoising procedure into a different steps. These steps are form <span class="math inline">\(t\)</span> to <span class="math inline">\(t-1\)</span>. You can use this as a supervised learning method</p>
</section>
<section id="the-architecture-a-modified-u-net-that-uses-diffusion-time-t" class="level2" data-number="68">
<h2 data-number="68" class="anchored" data-anchor-id="the-architecture-a-modified-u-net-that-uses-diffusion-time-t"><span class="header-section-number">68</span> The architecture: a modified U-Net that uses diffusion time t</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_70.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 70</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The architecture here is a U-Net because a U-Net is an architecture that takes an image in and ouputs an image out. Like one of those segmentations. The only difference is that there is a time <span class="math inline">\(t\)</span> which is used for going from going noisy to less noisy. In adittion the NN also knows when this time frame is</p>
</section>
<section id="combining-this-with-text-as-cond.-inputs-dall-e-v2-unclip" class="level2" data-number="69">
<h2 data-number="69" class="anchored" data-anchor-id="combining-this-with-text-as-cond.-inputs-dall-e-v2-unclip"><span class="header-section-number">69</span> Combining this with text as cond. inputs: DALL-E v2 / “unCLIP”</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_71.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 71</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>So you first train a CLIP which gives you an encoding and a text encoding and then image encoding. Afterwards you use this text encondings to convert</p>
</section>
<section id="final-note-about-deep-fakes-and-ethics" class="level2" data-number="70">
<h2 data-number="70" class="anchored" data-anchor-id="final-note-about-deep-fakes-and-ethics"><span class="header-section-number">70</span> Final note about deep fakes and ethics</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_72.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 72</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="recommended-watch-just-34min" class="level2" data-number="71">
<h2 data-number="71" class="anchored" data-anchor-id="recommended-watch-just-34min"><span class="header-section-number">71</span> Recommended watch (just 34min)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_73.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 73</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="quiz-what-dimensions-need-to-be-considered-when-thinking-about" class="level2" data-number="72">
<h2 data-number="72" class="anchored" data-anchor-id="quiz-what-dimensions-need-to-be-considered-when-thinking-about"><span class="header-section-number">72</span> Quiz: What dimensions need to be considered when thinking about</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_74.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 74</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="more-on-that-previous-one.." class="level2" data-number="73">
<h2 data-number="73" class="anchored" data-anchor-id="more-on-that-previous-one.."><span class="header-section-number">73</span> More on that previous one..</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_75.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 75</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="sy-generative-adversarial-networks-gans-are-a-type-of-deep-learning-model" class="level2" data-number="74">
<h2 data-number="74" class="anchored" data-anchor-id="sy-generative-adversarial-networks-gans-are-a-type-of-deep-learning-model"><span class="header-section-number">74</span> Sy Generative Adversarial Networks (GANs) are a type of deep learning model</h2>
<p>Si | I } } I } } ar of GANs that is used for unsupervised learning. GANs consist of two components: a</p>
generative model, which is trained to generate samples that are similar to a
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_76.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 76</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="title-4" class="level2" data-number="75">
<h2 data-number="75" class="anchored" data-anchor-id="title-4"><span class="header-section-number">75</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_77.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 77</figcaption>
</figure>
</div>
</center>
<pre></pre>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.danilotpnta\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about/index.html">
<p><span class="footerDaniloToapanta">© 2024 Danilo Toapanta</span></p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../privacy/index.html">
<p>Privacy</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../contact/index.html">
<p>Contact</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license/index.html">
<p>License</p>
</a>
  </li>  
</ul>
    <div class="toc-actions"><ul><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/blob/main/blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/edit/main/blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../docs/sitemap.xml">
      <i class="bi bi-rss-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>

    let navbar = document.getElementsByClassName("navbar-nav")[0]    

    let li2 = document.createElement("li");
    li2.className = "nav-item compact";

    let a2 = document.createElement("a");
    a2.className = "nav-link quarto-color-scheme-toggle";
    a2.style.cursor = "pointer"
    li2.appendChild(a2)

    let i2 = document.createElement("i");
    i2.className = "bi bi-moon"
    a2.append(i2)

    navbar.appendChild(li2);

    i2.onclick = function() {
        window.quartoToggleColorScheme(); return false;
    }
    // <a href="http://localhost:4200/about/" class="quarto-color-scheme-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>

    let li = document.createElement("li");
    li.className = "nav-item compact";

    let a = document.createElement("a");
    a.className = "nav-link";
    a.style.cursor = "pointer"
    li.appendChild(a)

    let i = document.createElement("i");
    i.className = "bi bi-search"
    a.append(i)

    // let span = document.createElement("span");
    // span.className = "menu-text"
    // a.append(span)

    navbar.appendChild(li);

    a.onclick = function() {
        window.quartoOpenSearch()
    }


</script>






</body></html>