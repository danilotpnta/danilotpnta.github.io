<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Danilo Toapanta">
<meta name="dcterms.date" content="2023-09-25">
<meta name="description" content="Notes for the course at: Machine Learning 1">

<title>Classification and Decision Theory – Danilo Toapanta</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/danilo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<!-- <link href="../../site_libs/quarto-contrib/material-icons-0.14.2/mi.css" rel="stylesheet"> -->
<script>
document.addEventListener('DOMContentLoaded', function () {
  function openSearchIfParamExists() {
    const urlParams = new URLSearchParams(window.location.search);
    if (urlParams.has('search')) {
      // Programmatically click the search icon to open the search overlay
      const searchIcon = document.querySelector('.bi-search');
      if (searchIcon) {
        searchIcon.click(); // Trigger the search icon click to open the search overlay
        setTimeout(() => {
          const searchBox = document.querySelector('.aa-Input');
          if (searchBox) {
            searchBox.focus(); // Focus on the actual input box
          }
        }, 500); // Adjust the timeout as needed
      }
    }
  }

  // Call the function to check the query parameter and open the search if needed
  openSearchIfParamExists();
});

</script>
<script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>
<link rel="shortcut icon" href="../../../../../../../../../../../assets/danilo.ico">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../css/index-posts.css">
</head>

<body class="floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <div id="quarto-announcement" data-announcement-id="af66dd50c39dd2ed9de488e10a192054" class="alert alert-primary hidden"><i class="bi bi-info-circle quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p><strong>Let op</strong> - This website is undergoing scheduled maintenance</p>
</div></div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title"><span id="danilo_topanta_brand"> Danilo Toapanta</span> <a id="mysite" class="mysite" href="../../../../../sites/">MySites</a></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text"><span id="home-welcome-msg">Home</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/danilotpnta?tab=repositories" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full blog-page" style="display: none !important;">
    <div class="quarto-title-banner page-columns page-full">
        <div class="quarto-title column-body">
            <h1 class="title">Classification and Decision Theory</h1>
                
            <!-- Description Block -->
                        <div>
                <div class="description">
                    Notes for the course at: Machine Learning 1
                </div>
            </div>
                        
            <!-- Categories Block -->
                                            <div class="quarto-categories">

                    <!-- Display Categories -->
                                            <div class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=Machine Learning">
                                Machine Learning
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=Classification">
                                Classification
                            </a>
                        </div> 
                    
                    <!-- Display Tags if any -->
                                    </div>
                            
        </div>
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">September 25, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    
</header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">
<script>
    var currentUrl = window.location.href;
    var index_init_post = currentUrl.lastIndexOf("/20");
    var string_init_post= currentUrl.slice(index_init_post, index_init_post+3 );

    // console.log("currentUrl: " + currentUrl);
    // console.log("index: " + index_init_post);
    // console.log("string: " + string_init_post);

    // If is equal to /blog/20... then make navbar title READING MODE
    if (string_init_post === "/20"){
        let mysite = document.getElementById("mysite");
        mysite.classList.add("mysite-change");

        let navbar = document.getElementById("danilo_topanta_brand");
        navbar.classList.add("navbar-brand-change");

        // This will render a new title saying READING DANILOS BLOG
        // navbar.innerHTML = 'You are Reading Danilo\'s Blog<span style="font-size:35px; vertical-align: middle; opacity: 0.65; padding-bottom: 6px; padding-left: 14px;" class="material-icons-round"> auto_awesome </span>';
        
        const smallDevice = window.matchMedia("(min-width: 570px)");
        smallDevice.addListener(handleDeviceChange);

        function handleDeviceChange(mediaQuery) {
            if (mediaQuery.matches) {
                navbar.innerHTML = "";
                // navbar.innerHTML = "<-- You are Reading Danilo's Blog -->";
            } else  {
                navbar.innerHTML = "Danilo Toapanta";
            }
        }

        // Run it initially
        handleDeviceChange(smallDevice);

        let link = document.getElementsByClassName("navbar-brand")[0];
        link.classList.add("disablePointerEvents");

        let brand_container = document.getElementsByClassName("navbar-brand-container")[0];
        brand_container.classList.add("navbar-brand-container-new-padding");

    }
</script>


<!-- <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Running my first Marathon</h1>
                  <div>
        <div class="description">
          I will be running at the 42km TCS Amsterdam 2023, 15th October
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">News</div>
              </div>
                  </div>
  </div> -->

  <!-- ---
  coming-soon: true
  tags: [Spanish]
  --- -->








<main id="title-block-header" class="quarto-title-block default page-columns page-full" style="padding-bottom: 40px;">

    <div class="quarto-title column-body" style="margin-bottom: 1em;">
        <h1 class="title" style="padding-bottom:8px" ;="">Classification and Decision Theory</h1>
        
        <!-- Description Block -->
                    <div>
                <div class="description">
                    Notes for the course at: Machine Learning 1
                </div>
            </div>
        
        <!-- Categories Block -->
                    
                <!-- Display Categories -->
                <div class="quarto-categories">
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title">
                        <i class="fa-solid fa-hashtag" ></i> Categories:
                    </div> -->

                                            <div id="All" class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div>
                                            <div id="Machine Learning" class="quarto-category">
                            <a href="../../blog/#category=Machine Learning">
                                Machine Learning
                            </a>
                        </div>
                                            <div id="TAGS" class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div>
                                            <div id="Classification" class="quarto-category">
                            <a href="../../blog/#category=Classification">
                                Classification
                            </a>
                        </div>
                                    </div>
                


                <div class="quarto-categories tag-categories">
                    
                    <!-- Tags Icon  -->
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title"> -->
                        <!-- <i class="fa-solid fa-tag" ></i> Tags: -->
                        <!-- <i class="fa-solid fa-hashtag" ></i> Tags: -->
                        <!-- <span class="material-icons-outlined" >local_offer</span> Tags: -->
                        <!-- / -->
                    <!-- </div> -->

                    <!-- Display Tags -->
                                            <div id="All-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=All">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                All
                            </a>
                        </div>
                                            <div id="Machine Learning-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=Machine Learning">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                Machine Learning
                            </a>
                        </div>
                                            <div id="TAGS-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=TAGS">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                TAGS
                            </a>
                        </div>
                                            <div id="Classification-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=Classification">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                Classification
                            </a>
                        </div>
                    
                    
                </div>

                    
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">September 25, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    <!-- Current link: Font-awesome, Google icons, Bootstrap icons -->
    
</main>


<h1>
Classification
</h1>
<p>This section focus on third week of the course. For Regression continue to this <a href="../2023-09-05_equation-for-ml1/">post</a>.</p>
<section id="classification-through-decision-regions" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="classification-through-decision-regions"><span class="header-section-number">1</span> Classification through decision regions</h2>
<ul>
<li>Here the targets can take only discrete values. In Linear Regression the targets were continuous</li>
</ul>
<p><span class="math display">\[
\begin{align}
\underline{x} &amp;\in \mathbb{R}^{Dx1}, \text{ $D$ Dimensional Space}\\
&amp;= [x_1, .., x_D]^T \nonumber
\end{align}
\]</span></p>
<ul>
<li><p>For instance when <span class="math inline">\(D=2\)</span> we can think of <span class="math inline">\(x_1\)</span> as the amount of black pixels in the image, and <span class="math inline">\(x_2\)</span> as the white pixels. Then I can clasify one image into this 2-D dimensional space. So in the xy-plane one image has <span class="math inline">\((x1,x2)\)</span> coordinates</p></li>
<li><p>We dive <span class="math inline">\(\underline{x}\)</span> into <span class="math inline">\(K\)</span> Decision Regions <span class="math inline">\(R_k\)</span>.</p></li>
<li><p>For each Decision Region <span class="math inline">\(R_k\)</span> we assign it to a class <span class="math inline">\(C_k\)</span>.</p></li>
<li><p>The target <span class="math inline">\(\underline{t} \in \{C_1,...,C_k\}\)</span> meaning the target can be classified either <span class="math inline">\(C_1\)</span> or the target can be classified as <span class="math inline">\(C_2\)</span></p></li>
</ul>
</section>
<section id="linear-classification" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="linear-classification"><span class="header-section-number">2</span> Linear Classification</h2>
<blockquote class="blockquote">
<p>Note: do not confuse <span class="math inline">\(D\)</span> the amount of data points with this new <span class="math inline">\(D\)</span> where we talk about the dimensionality of how each data point is represented (the coordinates)</p>
</blockquote>
<ul>
<li>The classification is done by only linear decision boundaries</li>
</ul>
<p>For <span class="math inline">\(D\)</span>-dimensional input space: <span class="math inline">\(\underline{x}\in\mathbb{R}^{D}\)</span>. The decision surface is a <span class="math inline">\(D-1\)</span> dimensional hyperplane. For instance:</p>
<ul>
<li>The decision boundaries can take a form of a line, i.e when <span class="math inline">\(\underline{x}\in\mathbb{R}^{D=2x1}\)</span> meaning the dots are drawn in the <span class="math inline">\(x,y\)</span> coordinates</li>
<li>The decision boundaries can take a form of a plane, i.e when <span class="math inline">\(\underline{x}\in\mathbb{R}^{D=3x1}\)</span> meaning the dots are drawn in the <span class="math inline">\(x,y,z\)</span> coordinates</li>
</ul>
<p>Linear Classifiers have however some constraints: one-versus-one, or one-vs-rest cannot classified in one specific region when majority vote its applied. There is class of decisions</p>
</section>
<section id="decision-theory" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="decision-theory"><span class="header-section-number">3</span> Decision Theory</h2>
<p>Here we talked about when we consider a classifier (a model) a good classifier.</p>
<ul>
<li>We talk about the Bayes Error Rate</li>
<li>How to minimize the misclassification rate</li>
</ul>
<p>Model that you start with:</p>
<ul>
<li>Class-conditional densities: <span class="math inline">\(p(\underline{x}|C_k)\)</span> <em>aka</em> (<strong>Likelihood</strong>)</li>
<li><strong>Prior</strong> class probabilities: <span class="math inline">\(p(C_k)\)</span></li>
</ul>
<p>From these two you can derive:</p>
<ol type="1">
<li>The Joint distribution <span class="math inline">\(p(\underline{x}, C_k)\)</span></li>
</ol>
<p><span class="math display">\[
\begin{align}
p(\underline{x}, C_k)=p(\underline{x}|C_k)p(C_k)
\end{align}
\]</span></p>
<ol start="2" type="1">
<li>The <strong>Posterior</strong> <span class="math inline">\(p(C_k|\underline{x})\)</span>.</li>
</ol>
<p><span class="math display">\[
\begin{align}
p(C_k|\underline{x}) = \frac{p(\underline{x}| C_k)p(C_k)}{p(\underline{x})}
\end{align}
\]</span></p>
<ul>
<li>Decision Theory tell us that the best prediction for input <span class="math inline">\(\underline{x}\)</span> is to choose the class with highest joint <span class="math inline">\(p(\underline{x}, C_k)\)</span></li>
<li>Or equivalently: choose class with the highest posterior <span class="math inline">\(p(C_k|\underline{x})\)</span></li>
<li>Decision boundary between <span class="math inline">\(C_k\)</span> and <span class="math inline">\(C_j\)</span> are at <span class="math inline">\(p(C_k | \underline{x})=p(C_j | \underline{x})\)</span></li>
</ul>
</section>
<section id="from-bayes-rule-to-bayes-classifiers" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="from-bayes-rule-to-bayes-classifiers"><span class="header-section-number">4</span> From Bayes Rule to Bayes Classifiers</h2>
<p><span class="math display">\[
\begin{align}
p(C|X) = \frac{P(X|C)P(C)}{P(X)} \\
\end{align}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P(C|X)\)</span> is the <strong>Posterior</strong> (Given the data what is the prob of being class C_k)</li>
<li><span class="math inline">\(P(X|C)\)</span> is the <strong>Likelihood</strong> (How the data is distributed given C)</li>
<li><span class="math inline">\(P(C)\)</span> is the <strong>Prior</strong></li>
<li><span class="math inline">\(P(X)\)</span> is the <strong>Evidence</strong>/ Marginal likelihood</li>
</ul>
<p>The evidence <span class="math inline">\(P(X)\)</span> can also be decomposed in:</p>
<p><span class="math display">\[
\begin{align}
P(X) &amp;= \sum_{j}{}P(X,C_j)\\
     &amp;= \sum_{j}{}P(X|C_j)P(C_j) \\
\end{align}
\]</span></p>
<p>Unlike regression, I will have one likelihood per each class, mainly:</p>
<p><span class="math display">\[
\begin{align}
p(X|C_0) \quad \text{and} \quad p(X|C_1) \\
\end{align}
\]</span></p>
<p>For instance for <span class="math inline">\(K=2\)</span> two classes. This means per each class we are going to have our own model</p>
<p>The decicion boundary then becomes equal when both likelihoods are equal:</p>
<p><span class="math display">\[
\begin{align}
p(C_0|X) &amp;= p(C_1|X) \quad \text{Decision Boundary}\\
% P(X, C_0) &amp;= p(X, C_1) \nonumber
\end{align}
\]</span></p>
<p>Here:</p>
<ul>
<li><span class="math inline">\(P(C_0|X)\)</span> is the <strong>Posterior</strong> probability</li>
</ul>
</section>
<section id="types-of-classification" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="types-of-classification"><span class="header-section-number">5</span> Types of Classification</h2>
<ul>
<li>Decision Trees</li>
<li>Logistic Regressions</li>
<li>Bayes Classifiers (Generative): first fit P(x|C) and then use Bayes Rule to flip it and tell which type of class correspond <span class="math inline">\(x\)</span>
<ul>
<li>Naive Bayes: you put an x and it gives you to which class <span class="math inline">\(K\)</span> correponds</li>
<li>Gaussian Likelihood: here <span class="math inline">\(p(x|C_k) = N(\mu_k, \Sigma_k)\)</span><br>
QDA: Quadratic discriminant analysis
<ul>
<li>Shared Parameters<br>
LDA: Linear Discriminant Analysis</li>
</ul></li>
</ul></li>
</ul>
<section id="gaussian-generative-classifiers" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="gaussian-generative-classifiers"><span class="header-section-number">5.1</span> Gaussian Generative Classifiers</h3>
<p>Here we assume arbitrary covariance matrices for each class</p>
<p><span class="math display">\[
\begin{align}
p(x|C_k) = N(\mu_k, \Sigma_k)
\end{align}
\]</span></p>
<ul>
<li>Each class <span class="math inline">\(K\)</span> is going to have its normal distribution. And each of these distributions would be completely different from the others.</li>
</ul>
<section id="quadratic-discriminant-analysis" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h4>
<p>Here the decision boundary would be quadratic.</p>
<p>Here the covariances would different for each class, like so:</p>
<p><span class="math display">\[
\begin{align}
1&amp;=\frac{p(x, C_2)}{p(x, C_1)} \\
&amp;=\frac{p(x| C_2)p(C_2)}{p(x| C_1)p(C_1)} \nonumber
\end{align}
\]</span></p>
<p>Taking the log at both sides</p>
<p><span class="math display">\[
\begin{align}
0 &amp;= log N(\mu_2, \Sigma_2) - log N(\mu_1, \Sigma_1) + log \frac{p(C_2)}{p(C_1)}
\end{align}
\]</span></p>
<p>If you solve the equation above then you end up with quadratic terms.</p>
<p>This tell us that now we can handle non-linear separate cases now the data can needs to be quadratic separable.</p>
</section>
<section id="shared-parameters" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="shared-parameters"><span class="header-section-number">5.1.1</span> Shared Parameters</h4>
</section>
<section id="linear-discriminant-analysis" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="linear-discriminant-analysis">Linear Discriminant Analysis</h4>
<p>Here the decision boundary would be linear.</p>
<p>Here the covariances would be the same: 1 covariance matrix like so:</p>
<p><span class="math display">\[
\begin{align}
0 &amp;= log N(\mu_2, \Sigma) - log N(\mu_1, \Sigma) + log \frac{p(C_2)}{p(C_1)}
\end{align}
\]</span></p>
<p>Here we do not impose <span class="math inline">\(diag\{\Sigma_k\}\)</span>, we can have Naive Bayes approach here. The latter meaning we can indeeed if we want have the covariance matrices to be <span class="math inline">\(diag\{\Sigma_k\}\)</span>. In this called we called. <em>Naive Bayes applied to LDA</em>.</p>
<ul>
<li>Naive Bayes applied to LDA: same/shared parameters for <span class="math inline">\(\Sigma_k\)</span> and <span class="math inline">\(diag\{\Sigma_k\}\)</span></li>
</ul>
</section>
</section>
<section id="understanding-covariance-and-variance" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="understanding-covariance-and-variance">Understanding Covariance and Variance</h3>
<div id="2478ca99" class="cell" data-freeze="true" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.colorbar <span class="im">as</span> cbar</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the mean and covariance matrix for the original case</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>covariance_matrix <span class="op">=</span> [[<span class="dv">2</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">2</span>]]  <span class="co"># Example covariance matrix</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of points</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>), np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> np.dstack((x, y))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a multivariate Gaussian distribution for the original case</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> multivariate_normal(mean, covariance_matrix)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the probability density at each point in the grid for the original case</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> rv.pdf(pos)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate eigenvalues and eigenvectors for the original case</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(covariance_matrix)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the scale factor for the arrows</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>scale_factor <span class="op">=</span> <span class="fl">2.0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the first subplot for the original case</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.contourf(x, y, pdf, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    plt.arrow(</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        mean[<span class="dv">0</span>],</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        mean[<span class="dv">1</span>],</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        eigenvectors[i, <span class="dv">0</span>] <span class="op">*</span> np.sqrt(eigenvalues[i]) <span class="op">*</span> scale_factor,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        eigenvectors[i, <span class="dv">1</span>] <span class="op">*</span> np.sqrt(eigenvalues[i]) <span class="op">*</span> scale_factor,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        head_width<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        head_length<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        fc<span class="op">=</span><span class="st">'r'</span>,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        ec<span class="op">=</span><span class="st">'r'</span>,</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Var(X) = </span><span class="sc">{</span>eigenvalues[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Var(Y) = </span><span class="sc">{</span>eigenvalues[<span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>), color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Cov(X, Y) = </span><span class="sc">{</span>covariance_matrix[<span class="dv">0</span>][<span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">1</span>), color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Original Case (Covariance ≠ 0)'</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the mean and covariance matrix for the equal variance case</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>equal_variance_cov_matrix <span class="op">=</span> np.diag([<span class="dv">2</span>, <span class="dv">2</span>])  <span class="co"># Equal variance along both dimensions</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a multivariate Gaussian distribution for the equal variance case</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>rv_equal_variance <span class="op">=</span> multivariate_normal(mean, equal_variance_cov_matrix)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the probability density at each point in the grid for the equal variance case</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>pdf_equal_variance <span class="op">=</span> rv_equal_variance.pdf(pos)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the second subplot for the equal variance case</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>plt.contourf(x, y, pdf_equal_variance, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Var(X) = </span><span class="sc">{</span>equal_variance_cov_matrix[<span class="dv">0</span>, <span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>plt.annotate(<span class="ss">f'Var(Y) = </span><span class="sc">{</span>equal_variance_cov_matrix[<span class="dv">1</span>, <span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">'</span>, xy<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">2</span>), color<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Equal Variance Case (Covariance = 0)'</span>)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Set common labels</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> plt.gcf().axes:</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">'X'</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">'Y'</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure equal aspect ratio for both subplots</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> plt.gcf().axes:</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    ax.set_aspect(<span class="st">'equal'</span>, adjustable<span class="op">=</span><span class="st">'box'</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new axes for the legend with adjusted width</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>cax <span class="op">=</span> plt.gcf().add_axes([<span class="fl">0.96</span>, <span class="fl">0.3</span>, <span class="fl">0.02</span>, <span class="fl">0.4</span>])  <span class="co"># Adjust the width and position as needed</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot vertical colorbar for the legend</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>cbar.ColorbarBase(cax, cmap<span class="op">=</span><span class="st">'viridis'</span>, orientation<span class="op">=</span><span class="st">'vertical'</span>, label<span class="op">=</span><span class="st">'Probability Density'</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the overall layout</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>plt.subplots_adjust(wspace<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="690" height="306" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This plot visually illustrates how variance represents the spread along each dimension, and the arrows depict how the covariance matrix encodes the relationships between the dimensions in a Gaussian distribution.</p>
<ol type="1">
<li><strong>Original Case (Covariance ≠ 0):</strong></li>
</ol>
<ul>
<li>In the first plot (Original Case), the color yellow represents regions where the probability density is higher. In a Gaussian distribution, the probability density is highest at the mean (center) of the distribution and decreases as you move away from the mean. The color yellow typically corresponds to higher probability values in this context.</li>
</ul>
<ol start="2" type="1">
<li><strong>Equal Variance Case (Covariance = 0):</strong></li>
</ol>
<ul>
<li>In the second plot (Equal Variance Case), the color yellow also represents regions of higher probability density. Even though the covariance is zero, meaning there is no linear relationship between the X and Y dimensions, the multivariate Gaussian distribution still has a peak at the mean (center) in each dimension. The color yellow again corresponds to higher probability values in this context.</li>
</ul>
<center>
<img class="img-fluid" width="450px" src="imgs/img_m.jpeg">
</center>
<ol type="1">
<li><p><strong>QDA:</strong> they have separate Covariances</p></li>
<li><p><strong>LDA:</strong> they share a non-zero covariance</p></li>
<li><p>Here the shared variances mean for example 3 in the y-direction and 1 in the x-direction, they however contain a non-zero covariance</p></li>
<li><p>Here the shared variances mean for example 1 in the y-direction and 1 in the x-direction, they however contain a zero covariance.</p></li>
</ol>
</section>
</section>
<section id="probabilistic-generative-models-k2" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="probabilistic-generative-models-k2"><span class="header-section-number">6</span> Probabilistic Generative Models <span class="math inline">\(K=2\)</span></h2>
<p><span class="math display">\[
\begin{align}
P(C_1|x) &amp;= \frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)p(x|C_2)p(C_2)}\\
&amp;= \frac{1}{1+e^-a}\nonumber\\
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(a\)</span> is the log odds:</p>
<p><span class="math display">\[
\begin{align}
a &amp;= \ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}\label{log_odds}\\
\end{align}
\]</span></p>
<p>This can be expressed as the Sigmoid Function:</p>
<p><span class="math display">\[
\begin{align}
\sigma &amp;= \frac{1}{1+e^{(-a)}}\\
\end{align}
\]</span></p>
<ul>
<li>When the log odds its possitive in the sigmud function it will converge to 1. This means I am certain it will be class <span class="math inline">\(C_1\)</span></li>
<li>If the log odds is equal meaning not clue which class to assign. The probability of classifing the target to either class is 0.5.
<ul>
<li>For <span class="math inline">\(a\)</span> to be equal to zero. I need <span class="math inline">\(a=\ln(1)\)</span>. This 1 means <span class="math inline">\(p(x|C_1)p(C_1)=p(x|C_2)p(C_2)\)</span></li>
</ul></li>
<li>When the log odds its negative, the sigmoid function will go to zero. This means I am certain it will be class <span class="math inline">\(C_2\)</span></li>
</ul>
</section>
<section id="probabilistic-generative-models-generalk" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="probabilistic-generative-models-generalk"><span class="header-section-number">7</span> Probabilistic Generative Models, general:<span class="math inline">\(K\)</span></h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remember
</div>
</div>
<div class="callout-body-container callout-body">
<p>You are given:</p>
<ul>
<li>Class-conditional densities: <span class="math inline">\(p(x|C_k)\)</span> <em>aka</em> <strong>Likelihood</strong></li>
<li><strong>Prior</strong> class probabilities: <span class="math inline">\(p(Ck)\)</span></li>
</ul>
<p>With these two guys you can get:</p>
<ul>
<li>Joint Distribution: <span class="math inline">\(p(x,C_k) = p(x|C_k)p(C_k)\)</span> the <em>numerator</em></li>
<li>The <strong>Posterior</strong> <span class="math inline">\(p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}\)</span></li>
</ul>
<p>Goal:</p>
<ul>
<li>Find <span class="math inline">\(p(C_k|x)\)</span> so that you can determine the class of <span class="math inline">\(x\)</span> by knowing the Decision boundaries<br>
</li>
</ul>
</div>
</div>
<p><span class="math display">\[
\begin{align}
P(C_k|x) &amp;= \frac{p(x|C_k)p(C_k)}{\sum_{j=1}^{K}p(x|C_j)p(C_j)}\\
&amp;= \frac{e^{a_k}}{\sum_{j=1}^{K}e^{a_j}}\label{softmax}\\
a_k &amp;= \ln(p(x|C_k)p(C_k)) \nonumber
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(\ref{softmax}\)</span> is called the <strong>Softmax</strong>:</p>
<ul>
<li>if <span class="math inline">\(a_k&gt;&gt;a_j\)</span> for all <span class="math inline">\(j \neq k\)</span> then <span class="math inline">\(p(C_k|x)=1\)</span> and <span class="math inline">\(p(C_j|x)=0\)</span></li>
<li>The <strong>Softmax</strong> reduces to the <strong>Simoid</strong> function when <span class="math inline">\(K=2\)</span></li>
</ul>
<section id="how-to-parametrize-class-conditional-densities-aka-the-likelihood" class="level4" data-number="7.0.1">
<h4 data-number="7.0.1" class="anchored" data-anchor-id="how-to-parametrize-class-conditional-densities-aka-the-likelihood"><span class="header-section-number">7.0.1</span> How to parametrize Class Conditional Densities (<em>aka</em> The Likelihood)?</h4>
<p>With Gaussians!</p>
<p><span class="math display">\[
\begin{align}
p(x|C_k)&amp;=\mathcal{N}(x|\mu_k , \Sigma_k ) \\
        &amp;= \frac{1}{(2 \pi)^{D/2}} \frac{1}{|\Sigma_k|^{1/2}} \exp^{\{-\frac{1}{2}(x-\mu_k)^T \Sigma_k^-1 (x-\mu_k)\}} \label{gaussian_lda}\\
\end{align}
\]</span></p>
<ul>
<li>Where the Gaussian is going to be multivariate and it will be <span class="math inline">\(D\)</span>-dimensional due to the input being <span class="math inline">\(x \in \mathbb{R}^{D}\)</span>
<ul>
<li>This means for each <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\Sigma_k\)</span> I would have a different Gaussian distribution</li>
</ul></li>
<li><span class="math inline">\(\Sigma_k\)</span> determines the shape of my distribution. Like in the python plot above in the left graph</li>
</ul>
<p>When we assume each of these Gaussian share the same the Covariance matrix: <span class="math inline">\(\Sigma_k\)</span> then we are talking about LDA</p>
<p>To determine the decision boundary we have that:</p>
<p><span class="math display">\[
\begin{align}
p(C_1|X) &amp;= \frac{1}{1+e^(-a)} = \sigma(a)\\
\end{align}
\]</span></p>
<p>Where <span class="math inline">\(a\)</span> is defined was defined as the log odds. So replacing <span class="math inline">\(p(x|C_k)\)</span> so replacing <span class="math inline">\(\ref{gaussian_lda}\)</span> (the Gaussians) in the log odds <span class="math inline">\(\ref{log_odds}\)</span> give us The Generalized Linear Model:</p>
<p><span class="math display">\[
\begin{align}
p(C_1|x) &amp;= \sigma(\underline{w}^T\underline{x}+w_o)\\
\end{align}
\]</span></p>
<p>And now recall that the decision boundary happens when <span class="math inline">\(p(C_1|x) = p(C_2|x)\)</span>.</p>
<ul>
<li>This all means if we want to make decisions based on the posterior distributions, then <span class="math inline">\(a=0\)</span> meaning the prob/prob is 1 or the <span class="math inline">\(\sigma(a) = \frac{1}{2}\)</span></li>
</ul>
</section>
</section>
<section id="lda-maximum-likelihood-for-k2" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="lda-maximum-likelihood-for-k2"><span class="header-section-number">8</span> LDA: Maximum Likelihood for K=2</h2>
<blockquote class="blockquote">
<p><strong>Goal:</strong> recover the Gaussian distributions (the join distribution = p(X, C_k)) that have generated the data. To accomplished that we need to take the MLE over the the Gaussian conditional densities aka the likelihood and solve for <span class="math inline">\(u_k\)</span>, <span class="math inline">\(\Sigma\)</span> and priors <span class="math inline">\(p(C_k)\)</span></p>
</blockquote>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Isometric Covariance definition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>It is a special case of a covariance matrix where all off-diagonal elements are zero, and the diagonal elements are equal, representing a constant variance or dispersion in all dimensions.</p>
<p>Mathematically, an isometric covariance matrix Σ can be represented as:</p>
<ul>
<li>Σ = σ² * I</li>
</ul>
<p>Where:</p>
<ul>
<li>Σ is the covariance matrix.</li>
<li>σ² is the common variance or dispersion parameter.</li>
<li>I is the identity matrix, which has ones on the diagonal and zeros elsewhere.</li>
</ul>
<p>In this form, each element along the diagonal of the covariance matrix Σ is equal to σ², and all off-diagonal elements are zero. This implies that the variables in a multivariate distribution with an isometric covariance matrix have equal variances and are uncorrelated with each other.</p>
</div>
</div>
</div>
<p>Given:</p>
<ol type="1">
<li>Gaussian conditional densities aka <strong>Likelihoods</strong>:</li>
</ol>
<p><span class="math display">\[
\begin{align}
p(x|C_k) = \frac{1}{(2 \pi)^{D/2}} \frac{1}{|\Sigma_k|^{1/2}} \exp^{\{-\frac{1}{2}(x-\mu_k)^T \Sigma_k^-1 (x-\mu_k)\}} \nonumber\\
\end{align}
\]</span></p>
<ol start="2" type="1">
<li>Prior <span class="math inline">\(p(C_k)\)</span><br>
Because we have <span class="math inline">\(k=2\)</span> then we can assign <span class="math inline">\(p(C_1) = q\)</span> and <span class="math inline">\(p(C_2) = 1-q\)</span></li>
</ol>
<p>With 1. and 2. we can solve for the Joint distributions:</p>
<p>For <span class="math inline">\(x_n\)</span> with <span class="math inline">\(t_n =1\)</span>: <span class="math display">\[
\begin{align}
p(x_n, C_1) &amp;= p(x_n|C_1)p(C_1) = q \, N(x_n|\mu_1,\Sigma) \\
\end{align}
\]</span></p>
<p>For <span class="math inline">\(x_n\)</span> with <span class="math inline">\(t_n =0\)</span>: <span class="math display">\[
\begin{align}
p(x_n, C_2) &amp;= p(x_n|C_2)p(C_2) = (1-q) \, N(x_n|\mu_2, \Sigma)\\
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(t_n\)</span> is binary if I do <span class="math inline">\(\sum t_n\)</span> here I am counting the number of times <span class="math inline">\(t_n\)</span> is equals to 1.</li>
</ul>
<section id="deriving-q_ml" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="deriving-q_ml"><span class="header-section-number">8.1</span> Deriving <span class="math inline">\(q_{ML}\)</span></h3>
<p><span class="math display">\[
\begin{align}
q_{ML} &amp;= \frac{1}{N} \sum_{n=1}^{N} t_n = \frac{N_1}{N}\\
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(q\)</span> is the prior probability of observing class <span class="math inline">\(K=1\)</span>. The result above means the total number of observations that I have observed <span class="math inline">\(t_n=1\)</span></li>
</ul>
</section>
<section id="deriving-muml" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="deriving-muml"><span class="header-section-number">8.2</span> Deriving <span class="math inline">\(\mu{ML}\)</span></h3>
<p><span class="math display">\[
\begin{align}
\mu_{1,ML} &amp;= \frac{1}{N_1} \sum_{n=1}^{N} t_n \, x_n\\
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(\mu_{1,Ml}\)</span> is the sample mean of all my observations where <span class="math inline">\(x_n\)</span> belongs to class <span class="math inline">\(K=1\)</span>. Here $t_n =1 $ when the class is 1.</li>
</ul>
<p><span class="math display">\[
\begin{align}
\mu_{2,ML} &amp;= \frac{1}{N_2} \sum_{n=1}^{N} (1-t_n) \, x_n\\
\end{align}
\]</span></p>
<ul>
<li><span class="math inline">\(\mu_{2,Ml}\)</span> is the sample mean of all my observations where <span class="math inline">\(x_n\)</span> belongs to class <span class="math inline">\(K=2\)</span></li>
</ul>
</section>
<section id="covariance-for-discrete-random-variables" class="level3" data-number="8.3">
<h3 data-number="8.3" class="anchored" data-anchor-id="covariance-for-discrete-random-variables"><span class="header-section-number">8.3</span> Covariance for discrete Random Variables</h3>
<p>For class <span class="math inline">\(i\)</span> the covariance matrix can be calculated as: <span class="math display">\[
\begin{align}
\Sigma_i &amp;= \frac{1}{N_i}\sum_{n=1}^{N_i}(x_n-\mu_i)(x_n-\mu_i)^T \\
\end{align}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N_i\)</span> ​is the number of data points in class <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(x_n\)</span> is a data point in class <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\mu_i\)</span> is the mean vector of class <span class="math inline">\(i\)</span></li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How to classify a new data point?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<ol type="1">
<li><p><strong>Gaussian Classifier:</strong> Once you have the covariance matrices for each class, you can use them to build a Gaussian classifier. The Gaussian classifier estimates the likelihood of a given data point belonging to each class based on the probability density function of a multivariate Gaussian distribution with the class’s mean and covariance matrix.</p></li>
<li><p><strong>Classification:</strong> When you receive a new data point, you calculate the likelihood of it belonging to each class using the Gaussian distribution parameters (mean and covariance matrix) for each class. You can then assign the data point to the class with the highest likelihood.</p></li>
</ol>
</div>
</div>
</div>
<p>The whole point of LDA was explained at: <a href="https://youtu.be/R6yNZ4diIQo?si=53KJbtvIB71V8Yad&amp;t=1436">here</a></p>
</section>
<section id="disadvantages-of-lda" class="level3" data-number="8.4">
<h3 data-number="8.4" class="anchored" data-anchor-id="disadvantages-of-lda"><span class="header-section-number">8.4</span> Disadvantages of LDA</h3>
<ul>
<li>Sensitive to outliers. Meaning if I have a point really far from <span class="math inline">\(\mu_1\)</span> then it induces a large shift to the actual <span class="math inline">\(\mu_1\)</span></li>
<li>Relies in handcrafted features, if I go to high dimensional spaces I need to make choices and complicates things</li>
<li>The same as regression, here in clarification with LDA the MLE MAximum Likelihood are prone to overfilling. The latter because any regularization has been applied</li>
</ul>
<p>So far:</p>
</section>
</section>
<section id="probabilistic-generative-models-discrete" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="probabilistic-generative-models-discrete"><span class="header-section-number">9</span> Probabilistic Generative Models: Discrete</h2>
<p>In this section we parametrize the class-conditional density with other distributions</p>
<ul>
<li><p>So far we parametrize the Class conditional distributions with Gaussians. We can use however different ones. This is necessary when ie. the data it is not continuous and for instance its discrete.</p></li>
<li><p>In the Continuous space the number of parameters does not scales as much as in the discrete where then we have for i.e a binary classification to <span class="math inline">\(2^D\)</span> parameters. The reasons is that we cannot fit anymore a Gaussains distribution to the discrete variables</p></li>
</ul>
<p>To contrast the huge num. of parameters then we are going to make a model assumption that is:</p>
<ul>
<li>Naive Bayes assumption: feature value are treated as independent when conditioned on class <span class="math inline">\(C_k\)</span>.</li>
</ul>
<p>The above means that we are going to model each feature value with its own distribution. This in turn means that to model <span class="math inline">\(p(x|C_k, \lambda_1,...,\lambda_D)\)</span> we will have <span class="math inline">\(D\)</span> number of parameters. The following equation accounts for that using the product symbol.</p>
<p>Given:</p>
<ul>
<li>Class-conditional probabilities: <span class="math display">\[
\begin{align}
p(x|C_k) &amp;= \prod_{i=1}^{D} \, p(x_i|C_k) \\
&amp;= \prod_{i=1}^{D} \,  \pi_{k_i}^{x_i}(1- \pi_{k_i}^{x_i})^{1-x_i} \nonumber
\end{align}
\]</span></li>
</ul>
<p>Here:</p>
<ul>
<li><span class="math inline">\(x_i\)</span> takes the value <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> given my class <span class="math inline">\(C_k\)</span></li>
</ul>
<p>The above equation was modeled using the bernoulli equation</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Effect of changing parameters in Bernoulli distribution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes, often referred to as “success” and “failure.” It is named after Swiss mathematician Jacob Bernoulli. The Bernoulli distribution is used to represent situations where an event can result in one of two possible outcomes, typically denoted as 1 (success) or 0 (failure).</p>
<p>Here are the key characteristics and properties of the Bernoulli distribution:</p>
<ol type="1">
<li><p><strong>Parameters</strong>: The Bernoulli distribution has a single parameter, denoted as “p,” which represents the probability of success (or the probability of the outcome being 1). The parameter “q” represents the probability of failure (q = 1 - p).</p></li>
<li><p><strong>Probability Mass Function (PMF)</strong>: The probability mass function of the Bernoulli distribution is defined as follows:</p>
<p>P(X = x) = p^x * q^(1-x)</p>
<p>Where:</p>
<ul>
<li>P(X = x) is the probability of the random variable X taking the value x (either 0 or 1).</li>
<li>p is the probability of success (X = 1).</li>
<li>q is the probability of failure (X = 0).</li>
</ul></li>
<li><p><strong>Mean and Variance</strong>:</p>
<ul>
<li>The mean (expected value) of a Bernoulli random variable is E(X) = p.</li>
<li>The variance of a Bernoulli random variable is Var(X) = p(1-p).</li>
</ul></li>
<li><p><strong>Support</strong>: The Bernoulli distribution is defined over the set of values {0, 1}.</p></li>
</ol>
<p>Applications of the Bernoulli distribution: - Modeling coin flips (heads or tails). - Modeling success/failure outcomes, such as whether a product is defective (failure) or non-defective (success). - Modeling binary decisions, such as whether a customer makes a purchase (success) or does not make a purchase (failure).</p>
<p><strong>Example Scenario</strong>: Suppose you have a coin that is not fair; it is biased. When you flip this biased coin, it does not have an equal chance of landing on heads (H) or tails (T). Instead, it has a higher probability of landing on heads.</p>
<p><strong>Probability of Success (Heads)</strong>: In this example, we have a probability of success (getting heads) denoted as “p.” Let’s say that “p” is equal to 0.6. This means that when you flip the coin, there is a 60% chance of getting heads (H) and a 40% chance of getting tails (T).</p>
<p><strong>Using the Bernoulli Distribution</strong>: To model this coin-flipping scenario, you can use a Bernoulli distribution. In this context:</p>
<ul>
<li>The outcome “1” can represent success (getting heads).</li>
<li>The outcome “0” can represent failure (getting tails).</li>
</ul>
<p>The Bernoulli distribution allows you to calculate the probability of success (1) or failure (0) for each coin flip.</p>
<p><strong>Bernoulli Distribution Parameters</strong>: - Parameter “p” is the probability of success (1), which is 0.6 in this case. - Parameter “q” is the probability of failure (0), which is 1 - p, or 0.4 in this case.</p>
<p><strong>Using the Bernoulli PMF</strong>: The Bernoulli probability mass function (PMF) allows you to calculate the probability of each outcome:</p>
<ul>
<li>P(X = 1) represents the probability of getting heads (success), which is equal to “p” or 0.6.</li>
<li>P(X = 0) represents the probability of getting tails (failure), which is equal to “q” or 0.4.</li>
</ul>
<p><strong>Interpreting the Results</strong>: When you flip this biased coin multiple times, the Bernoulli distribution helps you understand the likelihood of getting heads (success) or tails (failure) for each individual flip. For example:</p>
<ul>
<li>If you flip the coin 10 times, you would expect to get heads approximately 6 times (0.6 * 10) on average.</li>
<li>However, the actual outcomes in any given sequence of 10 flips may vary around this expected value due to randomness.</li>
</ul>
<p>The Bernoulli distribution provides a mathematical framework to model and analyze such binary outcomes in scenarios like coin flipping, where there are two possible results with different probabilities of occurrence.</p>
<div id="29a7a02c" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> bernoulli</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the probability of success (getting heads)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of coin flips</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>num_flips <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the outcomes of flipping the biased coin 10 times</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> bernoulli.rvs(p, size<span class="op">=</span>num_flips)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the PMF of the Bernoulli distribution</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> bernoulli.pmf(x, p)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a bar plot to visualize the PMF</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>plt.bar(x, pmf, align<span class="op">=</span><span class="st">'center'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>plt.xticks(x)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Outcome'</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Bernoulli Distribution PMF (p=</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>plt.bar(<span class="bu">range</span>(num_flips), outcomes, align<span class="op">=</span><span class="st">'center'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(num_flips))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Coin Flip'</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Outcome (0 or 1)'</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Outcomes of </span><span class="sc">{</span>num_flips<span class="sc">}</span><span class="ss"> Coin Flips (p=</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="667" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
<ul>
<li><span class="math inline">\(\pi_{k_i}=p(x_i=1|C_k)\)</span><br>
</li>
<li>Now the number of parameters per class is <span class="math inline">\(D\)</span></li>
</ul>
<p>With this parametrization we can calculate the posterior probability: <span class="math inline">\(p(C_k|x)\)</span> where we can compute it with our recently parametrized-bernouli like class density aka the likelihood. Here modeling the prior follows the same as how we did it with the class-density akak Likelihood, With these two we can get the joint aka the evidence evidence.</p>
<p>Remember the evidence is the marginalization (sumation) over the joint (<span class="math inline">\(p(x,C_k)\)</span>).</p>
<p><span class="math display">\[
\begin{align}
p(x) = \sum_{k=1}^{K}p(x,C_k) = \sum_{k=1}^{K}p(x|C_k)p(C_k)
\end{align}
\]</span></p>
</section>
<section id="discriminative-linear-models" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="discriminative-linear-models"><span class="header-section-number">10</span> Discriminative Linear Models</h2>
<p>We go away momentarily from the prob view and try model the discriminant function without describing first a prob model.</p>
<p>We will do the following. Given input <span class="math inline">\(x \in \mathbb{R}^{Dx1}\)</span> and targets <span class="math inline">\(t \in {C1, C2}={-1,1}\)</span></p>
<p><span class="math display">\[
\begin{align}
y(x,\overline{w})&amp;=f(\overline{w}^t \boldsymbol{\phi}) \\
\boldsymbol{\phi} &amp;= (\phi_{0}(x),\phi_{1}(x),...,\phi_{M-1}(x))^T
\end{align}
\]</span></p>
<p>This model is linear with respect to <span class="math inline">\(w\)</span></p>
</section>
<section id="least-squares-for-classification" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="least-squares-for-classification"><span class="header-section-number">11</span> Least Squares for Classification</h2>
<p>We are gonna have for each <span class="math inline">\(K\)</span> a linear model (discriminant function)</p>
<p><span class="math display">\[
\begin{align}
y_k = (\textbf{x}) = \textbf{w}_{k}^{T}\textbf{x} + w_{k0}
\end{align}
\]</span></p>
<p>Where:</p>
<ul>
<li><p>Inputs: <span class="math inline">\(\textbf{x} \in \mathbb{R}^{Dx1}\)</span>, <span class="math inline">\(D\)</span>-Dimensional feature vector (data points that describe each vector)<br>
i.e for <span class="math inline">\(D=10\)</span> we have <span class="math inline">\(\textbf{x}_n = (x1, x2, ...,x_D)^T = (0,1,0,...0)\)</span><br>
For instance in the document classifier form the practicals, we have that each word from the document will add up to form a <span class="math inline">\(D\)</span> vector. Each element in this vector correspond <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> indicating whether a word belongs or not to the document.</p></li>
<li><p><span class="math inline">\(\textbf{w}_k^T \in \mathbb{R}^{1xD}\)</span> is the weight vector for class <span class="math inline">\(k\)</span><br>
<span class="math inline">\(w_{k0}\)</span> is the bias term.</p></li>
</ul>
<p>We can generalize this not only for one <span class="math inline">\(y_k\)</span> but for all <span class="math inline">\(K\text{s}\)</span> like so:</p>
<p><span class="math display">\[
\begin{align}
\textbf{y} =  \mathbf{\widetilde W}^T \mathbf{\tilde x}
\end{align}
\]</span></p>
<p>Where:</p>
<ul>
<li>Matrix <span class="math inline">\(\mathbf{\widetilde W} \in \mathbb{R}^{MxK}\)</span>: has in the kth column <span class="math inline">\(\mathbf{\tilde w}_k = (w_{k0}, \textbf{w})^T \in \mathbb{R}^{(D+1)x1} \in \mathbb{R}^{Mx1}\)</span><br>
Matrix <span class="math inline">\(\mathbf{\widetilde W}^T \in \mathbb{R}^{KxM}\)</span></li>
<li><span class="math inline">\(\mathbf{\tilde{x}} = (1, \textbf{w})^T \in \mathbb{R}^{(D+1)x1}\in \mathbb{R}^{Mx1}\)</span></li>
<li>Vector: <span class="math inline">\(\textbf{y(x)} \in \mathbb{R}^{Kx1}\)</span></li>
</ul>
<p>Remember at the end <span class="math inline">\(y_k\)</span> is just a number (you can think of a number that tells you how far <span class="math inline">\(x\)</span> is from the decision surface)so we are going to assign <span class="math inline">\(\textbf{x}\)</span> to class <span class="math inline">\(C_k\)</span> if: <span class="math display">\[
\DeclareMathOperator*{\argmax}{argmax}
\begin{align}
k = \argmax_{j} \, y_j(\textbf{x})\\
\end{align}
\]</span></p>
<p>Now, for this to work we need target values so that we can minimize our error function:</p>
<p><span class="math display">\[
\begin{align}
E_D(\mathbf{\widetilde W})=\frac{1}{2}Tr[(\mathbf{\widetilde X}\mathbf{\widetilde W} - \mathbf{\widetilde T})^T(\mathbf{\widetilde X}\mathbf{\widetilde W} - \mathbf{\widetilde T})]
\end{align}
\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remember: What was Linear Regresion &amp; Do we get the target values?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In simple linear regression, there is one target value, whereas in multiple linear regression, there can be multiple target values.</p>
<p>In a typical linear regression problem, you are given the input features (independent variables) and the corresponding target values (dependent variables) as training data. The goal of linear regression is to learn a linear relationship between the input features and the target values</p>
<p><strong>Input Features (Independent Variables):</strong> These are the variables that you use to make predictions. Each data point in the dataset has a set of input features.</p>
<p><strong>Target Values (Dependent Variables):</strong> These are the values you are trying to predict or estimate based on the input features. Each data point in the dataset has a corresponding target value.</p>
<p>The linear regression model is trained using this dataset to find the coefficients (weights) for the input features that minimize the mean squared error between the predicted values and the actual target values. Once the model is trained, you can use it to make predictions on new or unseen data.</p>
</div>
</div>
</div>
<p>In the above equation <span class="math inline">\(E_D\)</span>:</p>
<ul>
<li><span class="math inline">\(X \in \mathbb{R}^{Nx(D+1)} \in \mathbb{R}^{NxM}\)</span>, where each row is a different observation (a different number of document like in the practicals) represented by the vector <span class="math inline">\(\mathbf{\tilde{x}}_n\)</span></li>
<li>Target: <span class="math inline">\(T \in \mathbb{R}^{NxK}\)</span>, where each row is a different one-hot encoded vector that is trying to predict what is the class that <span class="math inline">\(\mathbf{\tilde{x}}_n\)</span> belongs to. The one-hot enconding means that for that particular kth value is 1 meaning it belongs to the kth class and for the rest is zero.</li>
<li><span class="math inline">\(t \in \mathbb{R}^{Kx1}\)</span> where <span class="math inline">\(t \in \{C_1, C_2, ..., C_k\}\)</span> and <span class="math inline">\(K\)</span> classes<br>
i.e for <span class="math inline">\(K=4\)</span> for binary classification we have we have <span class="math inline">\(t = (0, 0, 0, 1)^T\)</span>. The latter would be the one-hot encoding for <span class="math inline">\(K=4\)</span> documents<br>
i.e if <span class="math inline">\(k=5\)</span> my one-hot encoding when it is predicting for class k=3 would be. <span class="math inline">\(t_n=(0,0,1,0,0)^T\)</span><br>
</li>
<li><span class="math inline">\(Tr\)</span>: Sum of the diagonal matrix</li>
</ul>
<blockquote class="blockquote">
<p><strong>Goal</strong>: Now that we have defined our error we want to minimize it as a function of <span class="math inline">\(W\)</span> so that when we new values for <span class="math inline">\(x\)</span> come, then we multiply with our computed <span class="math inline">\(W\)</span> and finally get our predicted <span class="math inline">\(y(x)\)</span>.</p>
</blockquote>
<p>Solution:</p>
<p><span class="math display">\[
\begin{align}
\mathbf{\widetilde W}_{LS} = (\mathbf{\widetilde X}^T\mathbf{\widetilde X})^{-1}\mathbf{\widetilde X}^T\textbf{T} = \mathbf{\widetilde X}^\dagger\textbf{T}
\end{align}
\]</span></p>
<p>Finally to predict the label for <span class="math inline">\(\mathbf{\tilde{x}}\)</span> we use our discriminant function: <span class="math display">\[
\begin{align}
\textbf{y}_{LS}(\textbf{x})&amp;=\mathbf{\widetilde W}_{LS}^T\mathbf{\tilde{x}}\\
&amp; \in \mathbb{R}^{KxM} \, \in \mathbb{R}^{Mx1} \nonumber\\
&amp;\in \mathbb{R}^{Kx1}\nonumber
\end{align}
\]</span></p>
<p>So one number per each class. We get a vector with dimensions <span class="math inline">\(K\)</span> because this vector was one-hot encoded so that means we have to look at the value that contains <span class="math inline">\(1\)</span> and that <span class="math inline">\(k_{th}\)</span> element would be our class <span class="math inline">\(C_k\)</span></p>
<ul>
<li><strong>Discriminant functions</strong> are used to classify data points into different classes based on the values of the discriminant function</li>
</ul>
<section id="why-linear-regresion-for-classification-is-not-a-good-idea" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="why-linear-regresion-for-classification-is-not-a-good-idea"><span class="header-section-number">11.1</span> Why Linear Regresion for Classification is not a good idea?</h3>
<ul>
<li>The decision boundaries are sensitive to outliers. Our Linear Regresion wants our distance to be as close to <span class="math inline">\(y(x)=1\)</span> if the target value is also <span class="math inline">\(1\)</span>, but if there is outliers then these points will influence the decision boundary skewing it.</li>
<li>For <span class="math inline">\(k&gt;2\)</span> some decision regions can become very small or are even completely ignored</li>
<li>The components of the <span class="math inline">\(\textbf{y}_{LS}(\textbf{x})\)</span> are not real probabilities</li>
</ul>
</section>
</section>
<section id="multi-class-logistic-regression" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="multi-class-logistic-regression"><span class="header-section-number">12</span> Multi-class Logistic Regression</h2>
<p>Consider logistic regresion for <span class="math inline">\(K\)</span> classes with <span class="math inline">\(N\)</span> training vectors <span class="math inline">\(\{x_n\}_{n=1}^{N}\)</span>. Each of these vectors is mapped to a different feature vector.</p>
<p><span class="math display">\[
\begin{align}
\phi_n = \phi(x_n) = (\phi_0(x_n), \phi_1(x_n), ..., \phi_{M-1}(x_n))^T
\end{align}
\]</span></p>
<ul>
<li>Each vector <span class="math inline">\(x_n\)</span> has a target vector <span class="math inline">\(t_n\)</span> of size <span class="math inline">\(K\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
t_n = (t_{n1}, t_{n2}, ..., t_{nK})^T
\end{align}
\]</span></p>
<p>Where, <span class="math inline">\(t_{nk} = 1\)</span> if <span class="math inline">\(x_n \in C_k\)</span>, <span class="math inline">\(0\)</span> otherwise.</p>
<ul>
<li>The input data can be collected in a matrix <span class="math inline">\(X\)</span> such that the n-th row is given by <span class="math inline">\(\textbf{x}_{n}^{T}\)</span>. The targets can also be collected in a matrix <span class="math inline">\(T\)</span> where each n-th row is given by <span class="math inline">\(\textbf{t}_{n}^{T}\)</span></li>
</ul>
<p>We have:</p>
<p><span class="math display">\[
\begin{align}
y_k(\phi) = p(C_k|\phi(x), \textbf{w}_1,...,\textbf{w}_K) = \frac{exp(a_k)}{\sum_{j=1}^{K}exp(a_j)}
\end{align}
\]</span></p>
<p>Where we have <span class="math inline">\(a_k = \textbf{w}_{k}^{T}\phi\)</span></p>
<p>Dimensions:</p>
<p><span class="math display">\[
\begin{align}
\textbf{y(x)} &amp;= (y_1(x),...,y_k(x) )^T &amp;\in \mathbb{R}^{Kx1} \nonumber\\
\phi &amp;= \phi(x) = (\phi_0(x), ..., \phi_{M-1}(x))^T &amp;\in \mathbb{R}^{Mx1}\nonumber\\
\mathbf{\phi}_n &amp;= \phi(x_n) = (\phi_0(x_n), ..., \phi_{M-1}(x_n))^T\nonumber\\
\Phi &amp;= (\phi_1, \phi_2, ..., \phi_{N})^T &amp;\in \mathbb{R}^{NxM}\nonumber\\
\mathbf{w}_k &amp;= (w_{k0},...,w_{k(M-1)})^T \in \mathbb{R}^{(D+1)x1} &amp;\in \mathbb{R}^{Mx1}\nonumber\\
X &amp;= (\textbf{x}_1,...,\textbf{x}_N)^T \in \mathbb{R}^{Nx(D+1)} &amp;\in \mathbb{R}^{NxM}\nonumber\\
\textbf{t}_n &amp;= (t_{n1}, ...,t_{nK})^T  &amp;\in \mathbb{R}^{1xK}\nonumber\\
T &amp;= (\textbf{t}_1,...,\textbf{t}_N)^T &amp;\in \mathbb{R}^{NxK}\nonumber\\
\end{align}
\]</span></p>
<section id="multivariate-gaussian-distribution" class="level3" data-number="12.1">
<h3 data-number="12.1" class="anchored" data-anchor-id="multivariate-gaussian-distribution"><span class="header-section-number">12.1</span> Multivariate Gaussian Distribution</h3>
<p>The PDF: <span class="math display">\[
\begin{align}
f(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{(2\pi)^{\frac{n}{2}} |\boldsymbol{\Sigma}|^{\frac{1}{2}}} \exp\left(-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right)\\
\end{align}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x}\)</span> represents the vector of random variables (n-dimensional).</li>
<li><span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean vector, which is also an n-dimensional vector.</li>
<li><span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the covariance matrix, which is an n x n symmetric positive-definite matrix.</li>
<li><span class="math inline">\(|\boldsymbol{\Sigma}|\)</span> represents the determinant of the covariance matrix.</li>
<li><span class="math inline">\((\mathbf{x} - \boldsymbol{\mu})^T\)</span> represents the transpose of the vector <span class="math inline">\((\mathbf{x} - \boldsymbol{\mu}\)</span>).</li>
<li><span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> is the inverse of the covariance matrix.</li>
</ul>
</section>
</section>
<section id="fixed-bssis-functions" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="fixed-bssis-functions"><span class="header-section-number">13</span> Fixed Bssis Functions</h2>
<p>In the context of basis functions, the statement “Easy way to define nonlinear models (wrt the original features) via linear models (in the parameters)” means that you can represent complex, nonlinear relationships between your input features and the output variable by using a linear model in terms of transformed or “basis” functions. Let’s break down what this statement implies:</p>
<ol type="1">
<li><p><strong>Nonlinear Models</strong>: Nonlinear models are those that cannot be expressed as simple linear relationships between the input features and the output. In many real-world problems, the relationships between variables are not linear, which makes modeling them directly with linear models (like linear regression) challenging.</p></li>
<li><p><strong>Original Features</strong>: These are the raw input features of your data. For example, if you’re working with a dataset of house prices, the original features might include the number of bedrooms, square footage, and location.</p></li>
<li><p><strong>Basis Functions</strong>: Basis functions are mathematical functions that transform the original features into a new set of features. These new features are designed to capture the underlying nonlinear relationships in the data. Common basis functions include polynomial functions (e.g., squaring a feature to capture quadratic relationships) and radial basis functions (used in radial basis function networks).</p></li>
<li><p><strong>Linear Models (in the Parameters)</strong>: Despite the use of basis functions to transform the input features, the model’s structure is still linear in terms of its parameters. This means that you can express the output variable as a linear combination of the transformed features, where the coefficients of this linear combination are the parameters of the model. For example, you might have a model like:</p>
<pre><code>y = w0 + w1 * basis_function1(x) + w2 * basis_function2(x) + ... + wn * basis_functionn(x)</code></pre>
<p>Here, <code>w0, w1, w2, ..., wn</code> are the model parameters, and <code>basis_function1(x), basis_function2(x), ...</code> are the transformed features created by the basis functions.</p></li>
</ol>
<p>So, the statement is highlighting that by using basis functions to transform the original features, you can still use a linear model in terms of its parameters to capture complex, nonlinear relationships in the data. This approach makes it easier to model nonlinear data patterns while benefiting from the simplicity and interpretability of linear models when it comes to parameter estimation and inference.</p>
</section>
<section id="faq" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="faq"><span class="header-section-number">14</span> FAQ</h2>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
1. Why if correlated features are treated independently, the evidence for a class will be overcounted?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>I apologize for any confusion. Let me break down that fragment for better clarity:</p>
<ol type="1">
<li><p><strong>Evidence for a Class</strong>: When we talk about “evidence for a class” in a classification problem, we’re referring to the information or characteristics of a data point’s features that suggest or indicate which class that data point should belong to. This evidence is essentially the input data that the classification algorithm uses to make predictions.</p></li>
<li><p><strong>Correlated Features</strong>: Correlated features are features that have some degree of statistical relationship or similarity between them. In the context of a classification problem, correlated features might provide similar or redundant information about the data.</p></li>
<li><p><strong>Redundant or Overlapping Information</strong>: When features are correlated, it means that some of the information they provide is redundant or overlapping. In other words, these features may convey the same or very similar details about the data point. For example, if you have two highly correlated features, knowing the value of one feature might give you a good idea about the value of the other.</p></li>
<li><p><strong>Treating Features Independently</strong>: In some classification algorithms, especially simple ones like Naive Bayes, each feature is treated as if it is completely independent of the others. This assumption simplifies the modeling process but can be problematic when features are correlated.</p></li>
<li><p><strong>Counting Shared Information Multiple Times</strong>: When you treat correlated features independently, you essentially consider the shared or overlapping information multiple times. For example, if two features are highly correlated and you treat them independently, you might effectively count the same information twice, once for each correlated feature. This can lead to an overestimation of the importance of that shared information.</p></li>
</ol>
<p>To illustrate this concept, consider a classification task where you’re trying to predict whether an email is spam (class 1) or not spam (class 0) based on two features: the number of times the word “money” appears in the email and the number of times the word “cash” appears. If these two features are highly correlated (i.e., they tend to occur together), treating them independently might lead to double-counting the evidence related to financial terms, which could skew the classification result. Therefore, it’s important to handle correlated features appropriately in order to make accurate predictions.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
2. What is the Köppen climate classification system?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>It consists of five primary climate groups, which are further subdivided into various climate types. Here’s a breakdown of the primary climate groups and their associated climate types:</p>
<ol type="1">
<li><strong>Group A: Tropical Climates</strong>:
<ul>
<li>Af: Tropical rainforest climate</li>
<li>Am: Tropical monsoon climate</li>
<li>Aw: Tropical wet and dry or savanna climate</li>
</ul></li>
<li><strong>Group B: Dry Climates</strong>:
<ul>
<li>BWh: Hot desert climate</li>
<li>BWk: Cold desert climate</li>
<li>BSh: Hot semi-arid climate</li>
<li>BSk: Cold semi-arid climate</li>
</ul></li>
<li><strong>Group C: Temperate Climates</strong>:
<ul>
<li>Cfa: Humid subtropical climate</li>
<li>Cwa: Monsoon-influenced humid subtropical climate</li>
<li>Cfb: Oceanic or maritime climate</li>
<li>Cfc: Subpolar oceanic climate</li>
<li>Csa: Mediterranean climate</li>
<li>Csb: Mediterranean climate with dry summer</li>
<li>Cwa: Monsoon-influenced humid subtropical climate</li>
<li>Cwc: Cold subtropical highland climate</li>
</ul></li>
<li><strong>Group D: Continental Climates</strong>:
<ul>
<li>Dfa: Hot-summer humid continental climate</li>
<li>Dfb: Warm-summer humid continental climate</li>
<li>Dfc: Subarctic or boreal climate</li>
<li>Dwa: Hot-summer subarctic climate</li>
<li>Dwb: Warm-summer subarctic climate</li>
</ul></li>
<li><strong>Group E: Polar Climates</strong>:
<ul>
<li>ET: Tundra climate</li>
<li>EF: Ice cap climate</li>
</ul></li>
</ol>
<p>Additionally, there is a <strong>Group H: Highland Climates</strong> category for high-altitude regions with their own unique climate characteristics.</p>
<p>So, there are a total of 12 primary climate types within the Köppen climate classification system, and each of these primary types can be further refined with additional letters and numbers to provide more specific details about temperature and precipitation patterns.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
3. What are Generative Models?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>In machine learning, generative models are a class of models used to learn the underlying probability distribution of the data. These models are called “generative” because they can generate new data samples that resemble the training data. In other words, they capture the structure and patterns within the data and can be used to create synthetic data points that are statistically similar to the real data.</p>
<ul>
<li><strong>How NB is a generative model?</strong></li>
</ul>
<p>In generative models we model two things: 1. the class-conditional densities p(x|C_k) 2. the class priors p(C_k). With these two we can compute the join distribution.</p>
<p>Furthermore, one can use Bayes Theorem to compute the posterior p(C_k|x). Looking at the equation above we see that we have defined the numerator as the joint distribution obtained from our generative model. For the denominator part, we have computed the evidence by marginalising over the k classes.</p>
<p>Having done this, (we have used Naive Bayes to model the distribution of our features values as independent to decrease the number of parameters) we have obtain the posterior probability which can be used to determine class membership for each new input x.</p>
<p>As referred in Bishop, approaches that explicit or implicitly model the distribution of input as well as the outputs are known as generative models. They called generative models because by sampling from them it is possible to generate synthetic data points in the input space.</p>
<ul>
<li><strong>What does it mean by Generative models?</strong></li>
</ul>
<p>In Bishop we have:</p>
<blockquote class="blockquote">
<p>Approaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.</p>
</blockquote>
<p>The statement you provided highlights a key characteristic of generative models and explains why they are called “generative.” Let’s break down the meaning of this statement:</p>
<ol type="1">
<li><p><strong>Approaches that explicitly or implicitly model the distribution of inputs as well as outputs</strong>: This part of the statement refers to machine learning models that are designed to not only capture the relationship between inputs and outputs but also to learn the underlying probability distribution of the inputs. In other words, these models aim to understand how the input data is generated probabilistically.</p></li>
<li><p><strong>Known as generative models</strong>: These models are commonly referred to as “generative models” because they have the capability to generate synthetic data points that resemble the real data. This is achieved by sampling from the learned distribution of inputs. In essence, generative models can create new data instances that are statistically similar to the training data.</p></li>
<li><p><strong>By sampling from them it is possible to generate synthetic data points in the input space</strong>: This part of the statement explains the practical significance of generative models. Once a generative model has learned the data distribution, you can use it to create new data points. These new data points are generated by sampling from the probability distribution of inputs that the model has learned during training.</p></li>
</ol>
<p>Here’s a simple example to illustrate this concept: Consider a generative model trained on a dataset of images of cats. The model learns not only to recognize cats but also the underlying distribution of features that define what a cat looks like. Once trained, you can sample from this model, and it will generate new images of cats that resemble those in the training data. These generated images are synthetic data points in the input space because they represent new, artificial cat images that the model has “generated” based on what it has learned about cats.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-none no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
4. Convex regions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Let’s consider a 2D space, and we have a convex region “C1.” We’ll choose two points within this region, x0 and x1. Then, we’ll create a line segment by varying λ between 0 and 1 and plot the points on the line segment. If all points on the line segment remain within C1, it indicates that C1 is convex.</p>
<div id="966e77ec" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the convex region C1 (a circle for this example)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>radius <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>center <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random points within C1</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span> np.pi, <span class="dv">100</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> center[<span class="dv">0</span>] <span class="op">+</span> radius <span class="op">*</span> np.cos(theta)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>y0 <span class="op">=</span> center[<span class="dv">1</span>] <span class="op">+</span> radius <span class="op">*</span> np.sin(theta)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Choose two points x0 and x1 within C1</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>x0_point <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>x1_point <span class="op">=</span> (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a figure and axis</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot C1 as a filled circle</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>ax.fill(x0, y0, <span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the two chosen points x0 and x1</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>ax.plot(x0_point[<span class="dv">0</span>], x0_point[<span class="dv">1</span>], <span class="st">'ro'</span>, label<span class="op">=</span><span class="st">'x0'</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>ax.plot(x1_point[<span class="dv">0</span>], x1_point[<span class="dv">1</span>], <span class="st">'go'</span>, label<span class="op">=</span><span class="st">'x1'</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate points along the line segment defined by x0 and x1</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>lambdas <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>line_points <span class="op">=</span> [(<span class="dv">1</span> <span class="op">-</span> l) <span class="op">*</span> np.array(x0_point) <span class="op">+</span> l <span class="op">*</span> np.array(x1_point) <span class="cf">for</span> l <span class="kw">in</span> lambdas]</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if all points on the line segment are within C1</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>all_inside_c1 <span class="op">=</span> <span class="bu">all</span>(np.linalg.norm(p <span class="op">-</span> center) <span class="op">&lt;=</span> radius <span class="cf">for</span> p <span class="kw">in</span> line_points)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight the line segment</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> all_inside_c1:</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    ax.plot([p[<span class="dv">0</span>] <span class="cf">for</span> p <span class="kw">in</span> line_points], [p[<span class="dv">1</span>] <span class="cf">for</span> p <span class="kw">in</span> line_points], <span class="st">'k-'</span>, label<span class="op">=</span><span class="st">'Line Segment (Convex)'</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    ax.plot([p[<span class="dv">0</span>] <span class="cf">for</span> p <span class="kw">in</span> line_points], [p[<span class="dv">1</span>] <span class="cf">for</span> p <span class="kw">in</span> line_points], <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'Line Segment (Not Convex)'</span>)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Set axis limits</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Add labels and legend</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'X-axis'</span>)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'Y-axis'</span>)</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Convex Region and Line Segment'</span>)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>ax.legend()</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="592" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In this example, we define a convex region “C1” as a circle. We choose two points, x0 and x1, within C1 and create a line segment by interpolating between them using λ values. If all points on the line segment remain within the circle (C1), it indicates that C1 is convex. The plot will show the circle, the two chosen points, and the line segment along with labels and a legend.</p>
<center>
<img class="img-fluid" width="450px" src="http://tinyurl.com/yuykwjc9">
</center>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.danilotpnta\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about/index.html">
<p><span class="footerDaniloToapanta">© 2024 Danilo Toapanta</span></p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../dev/index.html">
<p>Version 1.2</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html">
<p>Privacy - Code of Conduct</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../coming-soon.html">
<p>Newsletter</p>
</a>
  </li>  
</ul>
    <div class="toc-actions"><ul><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/blob/main/blog/2023-09-25_classification-and-decision-theory/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/edit/main/blog/2023-09-25_classification-and-decision-theory/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../docs/sitemap.xml">
      <i class="bi bi-rss-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>

    let navbar = document.getElementsByClassName("navbar-nav")[0]    

    let li2 = document.createElement("li");
    li2.className = "nav-item compact";

    let a2 = document.createElement("a");
    a2.className = "nav-link quarto-color-scheme-toggle";
    a2.style.cursor = "pointer"
    li2.appendChild(a2)

    let i2 = document.createElement("i");
    i2.className = "bi bi-moon"
    a2.append(i2)

    navbar.appendChild(li2);

    i2.onclick = function() {
        window.quartoToggleColorScheme(); return false;
    }
    // <a href="http://localhost:4200/about/" class="quarto-color-scheme-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>

    let li = document.createElement("li");
    li.className = "nav-item compact";

    let a = document.createElement("a");
    a.className = "nav-link";
    a.style.cursor = "pointer"
    li.appendChild(a)

    let i = document.createElement("i");
    i.className = "bi bi-search"
    a.append(i)

    // let span = document.createElement("span");
    // span.className = "menu-text"
    // a.append(span)

    navbar.appendChild(li);

    a.onclick = function() {
        window.quartoOpenSearch()
    }


</script>






</body></html>