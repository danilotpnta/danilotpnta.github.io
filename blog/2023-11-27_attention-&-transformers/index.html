<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Danilo Toapanta">
<meta name="dcterms.date" content="2023-11-27">
<meta name="description" content="Description of this Post">

<title>Danilo Toapanta - Attention &amp; Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/danilo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/material-icons-0.14.2/mi.css" rel="stylesheet">
<script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>
<link rel="shortcut icon" href="../../../../../../../../../../../assets/danilo.ico">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../css/index-posts.css">
</head>

<body class="floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title"><span id="danilo_topanta_brand"> Danilo Toapanta</span> <a id="mysite" class="mysite" href="../../../../../sites/">MySites</a></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text"><span id="home-welcome-msg">Home</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/index.html" rel="" target="">
 <span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/danilotpnta?tab=repositories" rel="" target="_blank"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full blog-page" style="display: none;">
    <div class="quarto-title-banner page-columns page-full">
        <div class="quarto-title column-body">
            <h1 class="title">Attention &amp; Transformers</h1>
                
            <!-- Description Block -->
                        <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
                        
            <!-- Categories Block -->
                                            <div class="quarto-categories">

                    <!-- Display Categories -->
                                            <div class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=Deep Learning">
                                Deep Learning
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div> 
                    
                    <!-- Display Tags if any -->
                                    </div>
                            
        </div>
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">November 27, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    
</header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">
<script>
    var currentUrl = window.location.href;
    var index_init_post = currentUrl.lastIndexOf("/20");
    var string_init_post= currentUrl.slice(index_init_post, index_init_post+3 );

    // console.log("currentUrl: " + currentUrl);
    // console.log("index: " + index_init_post);
    // console.log("string: " + string_init_post);

    // If is equal to /blog/20... then make navbar title READING MODE
    if (string_init_post === "/20"){
        let mysite = document.getElementById("mysite");
        mysite.classList.add("mysite-change");

        let navbar = document.getElementById("danilo_topanta_brand");
        navbar.classList.add("navbar-brand-change");

        // This will render a new title saying READING DANILOS BLOG
        // navbar.innerHTML = 'You are Reading Danilo\'s Blog<span style="font-size:35px; vertical-align: middle; opacity: 0.65; padding-bottom: 6px; padding-left: 14px;" class="material-icons-round"> auto_awesome </span>';
        
        const smallDevice = window.matchMedia("(min-width: 570px)");
        smallDevice.addListener(handleDeviceChange);

        function handleDeviceChange(mediaQuery) {
            if (mediaQuery.matches) {
                navbar.innerHTML = "";
                // navbar.innerHTML = "<-- You are Reading Danilo's Blog -->";
            } else  {
                navbar.innerHTML = "Danilo Toapanta";
            }
        }

        // Run it initially
        handleDeviceChange(smallDevice);

        let link = document.getElementsByClassName("navbar-brand")[0];
        link.classList.add("disablePointerEvents");

        let brand_container = document.getElementsByClassName("navbar-brand-container")[0];
        brand_container.classList.add("navbar-brand-container-new-padding");

    }
</script>


<!-- <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Running my first Marathon</h1>
                  <div>
        <div class="description">
          I will be running at the 42km TCS Amsterdam 2023, 15th October
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">News</div>
              </div>
                  </div>
  </div> -->

  <!-- ---
  coming-soon: true
  tags: [Spanish]
  --- -->








<main id="title-block-header" class="quarto-title-block default page-columns page-full" style="padding-bottom: 40px;">

    <div class="quarto-title column-body" style="margin-bottom: 1em;">
        <h1 class="title" style="padding-bottom:8px" ;="">Attention &amp; Transformers</h1>
        
        <!-- Description Block -->
                    <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
        
        <!-- Categories Block -->
                    
                <!-- Display Categories -->
                <div class="quarto-categories">
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title">
                        <i class="fa-solid fa-hashtag" ></i> Categories:
                    </div> -->

                                            <div id="All" class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div>
                                            <div id="Deep Learning" class="quarto-category">
                            <a href="../../blog/#category=Deep Learning">
                                Deep Learning
                            </a>
                        </div>
                                            <div id="TAGS" class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div>
                                    </div>
                


                <div class="quarto-categories tag-categories">
                    
                    <!-- Tags Icon  -->
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title"> -->
                        <!-- <i class="fa-solid fa-tag" ></i> Tags: -->
                        <!-- <i class="fa-solid fa-hashtag" ></i> Tags: -->
                        <!-- <span class="material-icons-outlined" >local_offer</span> Tags: -->
                        <!-- / -->
                    <!-- </div> -->

                    <!-- Display Tags -->
                                            <div id="All-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=All">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                All
                            </a>
                        </div>
                                            <div id="Deep Learning-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=Deep Learning">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                Deep Learning
                            </a>
                        </div>
                                            <div id="TAGS-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=TAGS">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                TAGS
                            </a>
                        </div>
                    
                    
                </div>

                    
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">November 27, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    <!-- Current link: Font-awesome, Google icons, Bootstrap icons -->
    
</main>

<section id="general-overview-transformer-architecture" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="general-overview-transformer-architecture"><span class="header-section-number">1</span> General Overview: Transformer Architecture</h2>
<center>
<img src="imgs/image.png" class="img-fluid">
</center>
<pre></pre>
</section>
<section id="background-knowledge" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="background-knowledge"><span class="header-section-number">2</span> Background knowledge</h2>
<center>
<iframe width="575" height="315" src="https://www.youtube.com/embed/4Bdc55j80l8?si=XFM7UfO0WeWhApIw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
</iframe>
</center>
<pre></pre>
<center>
<iframe width="575" height="315" src="https://www.youtube.com/embed/e9-0BxyKG10?si=QRjwGtaNgZYd__Gm&amp;start=971" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
</iframe>
</center>
<pre></pre>
<!-- ## Title
<center>![Slide 2](imgs/page_2.png){.w575}</center><pre></pre> -->
<!-- ## Title -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_3.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 3</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="seq2seq-models" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="seq2seq-models"><span class="header-section-number">3</span> Seq2seq models</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_4.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 4</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="neural-machine-translation-with-a-seq2seq-model" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="neural-machine-translation-with-a-seq2seq-model"><span class="header-section-number">4</span> Neural machine translation with a seq2seq model</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_5.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 5</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="defining-seq2seq-for-nmt-encoder" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="defining-seq2seq-for-nmt-encoder"><span class="header-section-number">5</span> Defining seq2seq for NMT”: Encoder</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_6.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 6</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>h_{t-1} is the hidden state, is this what makes a NN like <span class="math inline">\(f()\)</span> be recurrent</p>
</section>
<section id="defining-seq2seg-for-nmt-decoder" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="defining-seq2seg-for-nmt-decoder"><span class="header-section-number">6</span> Defining seq2seg for NMT: Decoder</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_7.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 7</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="issue-of-seq2seq-models" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="issue-of-seq2seq-models"><span class="header-section-number">7</span> Issue of seq2seq models</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_8.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 8</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The problem is that they were trying to compress all info into a context vector <span class="math inline">\(c\)</span></p>
</section>
<section id="attention" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="attention"><span class="header-section-number">8</span> Attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_9.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 9</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="here-found-what-you-were" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="here-found-what-you-were"><span class="header-section-number">9</span> Here, | found what you were</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_10.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 10</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="attention-1" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="attention-1"><span class="header-section-number">10</span> Attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_11.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 11</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="formal-definition-of-attention" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="formal-definition-of-attention"><span class="header-section-number">11</span> Formal definition of Attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_12.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 12</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="formal-definition-of-attention-1" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="formal-definition-of-attention-1"><span class="header-section-number">12</span> Formal definition of Attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_13.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 13</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>If the value e or the alignment model is high then it tells you how similar the inputs around position j and the outputs at position i match.</p>
</section>
<section id="formal-definition-of-attention-2" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="formal-definition-of-attention-2"><span class="header-section-number">13</span> Formal definition of Attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_14.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 14</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="why-attention" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="why-attention"><span class="header-section-number">14</span> Why attention?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_15.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 15</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="self-attention" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="self-attention"><span class="header-section-number">15</span> Self-attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_16.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 16</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="paying-attention-in-vision" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="paying-attention-in-vision"><span class="header-section-number">16</span> Paying attention in vision</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_17.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 17</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="attention-is-all-you-need" class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="attention-is-all-you-need"><span class="header-section-number">17</span> Attention is all you need</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_18.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 18</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="queries-keys-and-values" class="level2" data-number="18">
<h2 data-number="18" class="anchored" data-anchor-id="queries-keys-and-values"><span class="header-section-number">18</span> Queries, keys and values</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_19.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 19</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="scaled-dot-product-attention" class="level2" data-number="19">
<h2 data-number="19" class="anchored" data-anchor-id="scaled-dot-product-attention"><span class="header-section-number">19</span> Scaled Dot-Product Attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_20.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 20</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="multi-head-attention" class="level2" data-number="20">
<h2 data-number="20" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">20</span> Multi-head attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_21.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 21</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="multi-head-self-attention" class="level2" data-number="21">
<h2 data-number="21" class="anchored" data-anchor-id="multi-head-self-attention"><span class="header-section-number">21</span> Multi-head self-attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_22.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 22</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="multi-head-self-attention-1" class="level2" data-number="22">
<h2 data-number="22" class="anchored" data-anchor-id="multi-head-self-attention-1"><span class="header-section-number">22</span> Multi-head self-attention</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_23.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 23</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="transformer-encoder" class="level2" data-number="23">
<h2 data-number="23" class="anchored" data-anchor-id="transformer-encoder"><span class="header-section-number">23</span> Transformer encoder</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_24.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 24</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="transformer-decoder" class="level2" data-number="24">
<h2 data-number="24" class="anchored" data-anchor-id="transformer-decoder"><span class="header-section-number">24</span> Transformer decoder</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_25.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 25</figcaption>
</figure>
</div>
</center>
<pre></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why do we use a mask in the decoder of self attention?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<section id="autoregressive-property" class="level3" data-number="24.1">
<h3 data-number="24.1" class="anchored" data-anchor-id="autoregressive-property"><span class="header-section-number">24.1</span> Autoregressive Property:</h3>
<p>Autoregressive models generate outputs one step at a time in a sequential manner. In the context of natural language processing, this means predicting the next word in a sequence given the preceding words. Autoregressive models are trained to predict the next token in the sequence based on the tokens that have already been generated.</p>
</section>
<section id="decoder-in-a-transformer" class="level3" data-number="24.2">
<h3 data-number="24.2" class="anchored" data-anchor-id="decoder-in-a-transformer"><span class="header-section-number">24.2</span> Decoder in a Transformer:</h3>
<p>The decoder in a transformer is responsible for generating the output sequence. It consists of multiple layers, each containing a self-attention mechanism and feedforward neural networks. The self-attention mechanism allows the model to weigh different parts of the input sequence differently when generating each token.</p>
</section>
<section id="cheating-and-the-mask" class="level3" data-number="24.3">
<h3 data-number="24.3" class="anchored" data-anchor-id="cheating-and-the-mask"><span class="header-section-number">24.3</span> Cheating and the Mask:</h3>
<p>“Cheating” refers to the undesirable situation where the model uses information from future positions in the sequence during training. During training, the model is fed the true output sequence up to the current position to calculate loss and update its parameters. If the model were allowed to attend to future positions, it might artificially inflate its performance by relying on information that it wouldn’t have during actual generation.</p>
<p>The mask applied in the decoder’s self-attention mechanism prevents the model from accessing future information. The mask sets the attention scores for future positions to very small values, essentially blocking the model from attending to tokens that haven’t been generated yet. This ensures that the model learns to generate each token based only on the information available up to that point, aligning with the autoregressive nature of the decoding process.</p>
</section>
<section id="example" class="level3" data-number="24.4">
<h3 data-number="24.4" class="anchored" data-anchor-id="example"><span class="header-section-number">24.4</span> Example:</h3>
<p>Consider the task of language translation. When translating a sentence from English to French, the decoder generates the translation one word at a time. If it were allowed to attend to words in the future, it might incorrectly use information from the French translation that hasn’t been generated yet. This could lead to overfitting during training and poor generalization to unseen data.</p>
<p>In summary, preventing “cheating” by using a mask ensures that the decoder learns to generate outputs based on the information available up to the current step, improving the model’s ability to generalize to unseen data and maintain the autoregressive property essential for sequence generation tasks.</p>
</section>
</div>
</div>
</div>
</section>
<section id="the-full-transformer" class="level2" data-number="25">
<h2 data-number="25" class="anchored" data-anchor-id="the-full-transformer"><span class="header-section-number">25</span> The full Transformer</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_26.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 26</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="title" class="level2" data-number="26">
<h2 data-number="26" class="anchored" data-anchor-id="title"><span class="header-section-number">26</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_27.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 27</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="coding-a-transformer-pytorch-init" class="level2" data-number="27">
<h2 data-number="27" class="anchored" data-anchor-id="coding-a-transformer-pytorch-init"><span class="header-section-number">27</span> Coding a Transformer (PyTorch): init</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_28.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 28</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="coding-a-transformer-pytorch-forward-pass" class="level2" data-number="28">
<h2 data-number="28" class="anchored" data-anchor-id="coding-a-transformer-pytorch-forward-pass"><span class="header-section-number">28</span> Coding a Transformer (PyTorch): forward pass</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_29.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 29</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>tgt: in roder to predict the next word, you can look at the preceding word. So the target is the same as source except that is shofted one to the left. We do it so that it shofts and we are able to predict the last word</p>
</section>
<section id="transformer-positional-encodings" class="level2" data-number="29">
<h2 data-number="29" class="anchored" data-anchor-id="transformer-positional-encodings"><span class="header-section-number">29</span> Transformer: Positional encodings</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_30.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 30</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Attention is a permutation-invariant operation, but this is not ideal because we might have that sometimes the order is important like with ‘not’</p>
<p>positional enconding to locate where are you at the beginning or at the end</p>
</section>
<section id="transformer-positional-encodings-1" class="level2" data-number="30">
<h2 data-number="30" class="anchored" data-anchor-id="transformer-positional-encodings-1"><span class="header-section-number">30</span> Transformer: Positional encodings</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_31.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 31</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>if they are apart from each other their positional encoding should be different</p>
</section>
<section id="coding-the-positional-encodings-pytorch" class="level2" data-number="31">
<h2 data-number="31" class="anchored" data-anchor-id="coding-the-positional-encodings-pytorch"><span class="header-section-number">31</span> Coding the Positional Encodings (PyTorch)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_32.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 32</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="pros-cons" class="level2" data-number="32">
<h2 data-number="32" class="anchored" data-anchor-id="pros-cons"><span class="header-section-number">32</span> Pros &amp; Cons</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_33.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 33</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>It scales quadratically with the num inputs, the matrix is N * N let see an example:</p>
<p>The quadratic scaling of transformers with respect to the number of inputs primarily arises from the self-attention mechanism used in transformers. In self-attention, each element in the input sequence attends to all other elements, and the attention scores are computed pairwise. This leads to a quadratic dependency on the number of inputs.</p>
<p>Let’s consider a simple example with a sequence of length <span class="math inline">\(N\)</span>. For simplicity, let’s assume each input element has a dimension of 1 for illustration purposes.</p>
<p>What about other dimensions, wel that can be possible because remember we have our embeddings as the input to the NN, not the words itself</p>
<center>
<img src="imgs/2023-11-28-17-32-03.png" class="w350 img-fluid">
</center>
<pre></pre>
<p>Like in this picture our dimensions are clearly larger than 1 for the embeddings</p>
<ol type="1">
<li><p><strong>Original Sequence (1D):</strong> <span class="math display">\[
x_1, x_2, x_3, \ldots, x_N
\]</span></p></li>
<li><p><strong>Self-Attention Weights:</strong> For each element <span class="math inline">\(x_i\)</span>, self-attention computes a weight for all other elements <span class="math inline">\(x_j\)</span> based on their relationships. This results in a square matrix of attention weights:</p>
<p><span class="math display">\[
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; \ldots &amp; w_{1,N} \\
w_{2,1} &amp; w_{2,2} &amp; \ldots &amp; w_{2,N} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{N,1} &amp; w_{N,2} &amp; \ldots &amp; w_{N,N} \\
\end{bmatrix}
  \]</span></p>
<p>Each entry <span class="math inline">\(w_{i,j}\)</span> represents the attention weight between <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span>.</p></li>
<li><p><strong>Output for Each Element:</strong> The output for each element <span class="math inline">\(x_i\)</span> is computed as a weighted sum of all elements based on the attention weights:</p>
<p><span class="math display">\[
\text{output}_{i} = w_{i,1} \cdot x_1 + w_{i,2} \cdot x_2 + \ldots + w_{i,N} \cdot x_N
  \]</span></p>
<p>This involves <span class="math inline">\(N\)</span> multiplications for each element.</p></li>
<li><p><strong>Total Complexity:</strong> For <span class="math inline">\(N\)</span> elements, we need to compute <span class="math inline">\(N\)</span> attention weights for each element, resulting in a total of <span class="math inline">\(N^2\)</span> attention weights. Therefore, the overall complexity is quadratic, <span class="math inline">\(O(N^2)\)</span>, due to the pairwise comparisons.</p></li>
</ol>
<p>This quadratic scaling becomes computationally expensive as the sequence length increases, leading to challenges in handling long sequences efficiently. To address this, techniques like sparse attention patterns and approximations have been proposed in research to reduce the computational cost while maintaining the benefits of self-attention.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_34.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 34</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="recommended-papers" class="level2" data-number="33">
<h2 data-number="33" class="anchored" data-anchor-id="recommended-papers"><span class="header-section-number">33</span> Recommended papers</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_35.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 35</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_36.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 36</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_37.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 37</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bert" class="level2" data-number="34">
<h2 data-number="34" class="anchored" data-anchor-id="bert"><span class="header-section-number">34</span> BERT</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_38.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 38</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bert-input-representation" class="level2" data-number="35">
<h2 data-number="35" class="anchored" data-anchor-id="bert-input-representation"><span class="header-section-number">35</span> BERT input representation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_39.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 39</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bert-pre-training" class="level2" data-number="36">
<h2 data-number="36" class="anchored" data-anchor-id="bert-pre-training"><span class="header-section-number">36</span> BERT pre-training</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_40.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 40</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bert-fine-tuning" class="level2" data-number="37">
<h2 data-number="37" class="anchored" data-anchor-id="bert-fine-tuning"><span class="header-section-number">37</span> BERT fine-tuning</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_41.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 41</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bert-for-feature-extraction" class="level2" data-number="38">
<h2 data-number="38" class="anchored" data-anchor-id="bert-for-feature-extraction"><span class="header-section-number">38</span> BERT for feature extraction</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_42.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 42</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>With BERT we gained contextualized word embeddings</p>
</section>
<section id="bertology" class="level2" data-number="39">
<h2 data-number="39" class="anchored" data-anchor-id="bertology"><span class="header-section-number">39</span> BERTology</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_43.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 43</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="gpt-1-2-3-4" class="level2" data-number="40">
<h2 data-number="40" class="anchored" data-anchor-id="gpt-1-2-3-4"><span class="header-section-number">40</span> GPT-{1, 2, 3, 4}</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_44.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 44</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>With bert you did not have a generative model, with GPT you can because it only relies on the past to predict the next ones. Bert mask the word in the middle, but sees at the right and left to see the context.</p>
<p>You dont need to have labels, because pred the next word is just looking in the corpus what is the actual word that should fit.</p>
</section>
<section id="gpt-1-2-3" class="level2" data-number="41">
<h2 data-number="41" class="anchored" data-anchor-id="gpt-1-2-3"><span class="header-section-number">41</span> GPT-{1, 2, 3}</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_45.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 45</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="gpt-in--context-learning" class="level2" data-number="42">
<h2 data-number="42" class="anchored" data-anchor-id="gpt-in--context-learning"><span class="header-section-number">42</span> GPT: In- context learning</h2>
The three settings we explore for in-context learning Traditional fine-tuning (not used for GPT-3)
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_46.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 46</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The ability to not train gradients is a cool ability that these hug models have</p>
<p><strong>What is in-context learning?</strong></p>
<p>In natural language processing or conversation, understanding a word or phrase in context means considering the words or sentences that precede and follow it to grasp its intended meaning. This is important because the same word can have different meanings in different situations.</p>
<p>In the context of machine learning, especially with language models like GPT-3, providing information “in context” often involves supplying relevant details or context so that the model can generate responses or perform tasks that take into account the broader context of the input. This is particularly important for tasks that require understanding and generating coherent and contextually appropriate language.</p>
</section>
<section id="discuss" class="level2" data-number="43">
<h2 data-number="43" class="anchored" data-anchor-id="discuss"><span class="header-section-number">43</span> Discuss</h2>
For models like StableDiffusion, Dalle, EMU video etc. T5
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_47.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 47</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Why may encoders models be favorable compared to decoder models?</p>
<ul>
<li><p>Decoders are also trained with masks but if you want to predict the next word, this representation looks at everything that comes before, so in a way if that is what you care about there is no mack really (because you are looking at everything that was looked before)</p></li>
<li><p>Nobody knows the answer for this question</p></li>
<li><p>Hypothesis is that encoders compress the information, while for Large language models, they are basically the job of encoding and decoding at the same time, because th closer you get to the ouput the more you need to go back to i.e correct grammar and very low level features, and somewhere in the middle of these decoder models there is the summary semantics that you could use for the vission models but you don’t know exactly where those features are. So for encoders you know exaclty where th summary is because that is still the bottleneck still but for decoders we dont know where to take the features</p></li>
<li><p>A CNN is an encoder</p></li>
<li><p>Unet also has this decoder then the decoder part like</p></li>
</ul>
</section>
<section id="gpt-vs-bert" class="level2" data-number="44">
<h2 data-number="44" class="anchored" data-anchor-id="gpt-vs-bert"><span class="header-section-number">44</span> GPT vs BERT</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_48.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 48</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="multimodal-transformer-architecture-clip" class="level2" data-number="45">
<h2 data-number="45" class="anchored" data-anchor-id="multimodal-transformer-architecture-clip"><span class="header-section-number">45</span> Multimodal Transformer architecture: CLIP</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_49.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 49</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="multimodal-transformer-architecture-clip-1" class="level2" data-number="46">
<h2 data-number="46" class="anchored" data-anchor-id="multimodal-transformer-architecture-clip-1"><span class="header-section-number">46</span> Multimodal Transformer architecture: CLIP</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_50.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 50</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here we want things to be close but different. That is hard, and with these hard examples we learn new features and learn more</p>
<p>Now differentiating a dog vs a sheep that would be easier and eventually you will not learn anything.</p>
<p>1:32</p>
</section>
<section id="multimodal-transformer-architecture-clip-2" class="level2" data-number="47">
<h2 data-number="47" class="anchored" data-anchor-id="multimodal-transformer-architecture-clip-2"><span class="header-section-number">47</span> Multimodal Transformer architecture: CLIP</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_51.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 51</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Because they use a text encoder, like BERT, they can do Zero-shot for classification images</p>
</section>
<section id="clip-zero-shot-examples" class="level2" data-number="48">
<h2 data-number="48" class="anchored" data-anchor-id="clip-zero-shot-examples"><span class="header-section-number">48</span> CLIP: Zero-Shot Examples</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_52.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 52</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="clip-robustness" class="level2" data-number="49">
<h2 data-number="49" class="anchored" data-anchor-id="clip-robustness"><span class="header-section-number">49</span> CLIP: Robustness</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_53.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 53</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li>Better because of the internet:</li>
</ul>
<p>CLIP is pre-trained on a large dataset with diverse images and associated text from the internet. This diverse pre-training data helps the model learn features that are more transferable across different tasks and domains. In contrast, a supervised ImageNet model might be optimized for the specific categories present in ImageNet, and its features may not generalize as well to new, unseen classes.</p>
<ul>
<li>Better because we can guide it using engineered prompts:</li>
</ul>
<p>In zero-shot learning with CLIP, you can provide textual prompts to guide the model’s behavior. This allows you to specify the task or class you’re interested in, enabling the model to adapt its predictions based on the provided textual information</p>
<ul>
<li>Less prone to overfitting due to have trained in larger dataset:</li>
</ul>
<p>Supervised models trained on specific datasets, such as ImageNet, may be prone to overfitting to the idiosyncrasies of that dataset. CLIP, having been trained on a broader range of data, may be less prone to overfitting to specific dataset biases</p>
<ul>
<li>More data more understanding of semantics:</li>
</ul>
<p>CLIP’s strength lies in its ability to understand the semantic relationships between images and text. A larger dataset provides more examples of diverse language-image pairs, allowing the model to learn richer semantic embeddings</p>
</section>
<section id="clip-usage-in-other-models" class="level2" data-number="50">
<h2 data-number="50" class="anchored" data-anchor-id="clip-usage-in-other-models"><span class="header-section-number">50</span> CLIP: Usage in other models</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_54.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 54</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="clip-shortcomings" class="level2" data-number="51">
<h2 data-number="51" class="anchored" data-anchor-id="clip-shortcomings"><span class="header-section-number">51</span> CLIP: Shortcomings</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_55.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 55</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>CLIP does not have a decoder, so it cannot generate text</p>
</section>
<section id="visual-language-model-flamingo" class="level2" data-number="52">
<h2 data-number="52" class="anchored" data-anchor-id="visual-language-model-flamingo"><span class="header-section-number">52</span> Visual Language Model: Flamingo</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_56.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 56</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Basically with CLIP you give images, out labels in form of a prompt text</p>
<p>With flamingo you give images and text prompts and can generate now the output cpation for an specific image</p>
<p>GPT, you give it some text and is able to see what is next because it uses decoders</p>
</section>
<section id="visual-language-model-flamingo-1" class="level2" data-number="53">
<h2 data-number="53" class="anchored" data-anchor-id="visual-language-model-flamingo-1"><span class="header-section-number">53</span> Visual Language Model: Flamingo</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_57.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 57</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The language model is frozen, but you add this cross attention gates. So the cross atentions is sort of similar to the encoder decoder structure, when this language model can attend to the visual inputs. The pink is what is being learned. The visual encoder are also keept frozen.</p>
<p>The perciver part allows you to change the representation of the encoder</p>
</section>
<section id="visual-language-model-flamingo-2" class="level2" data-number="54">
<h2 data-number="54" class="anchored" data-anchor-id="visual-language-model-flamingo-2"><span class="header-section-number">54</span> Visual Language Model: Flamingo</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_58.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 58</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here there is an encoder and a decoder</p>
</section>
<section id="vision-transformer" class="level2" data-number="55">
<h2 data-number="55" class="anchored" data-anchor-id="vision-transformer"><span class="header-section-number">55</span> Vision Transformer</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_59.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 59</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="understanding-a-figure-1" class="level2" data-number="56">
<h2 data-number="56" class="anchored" data-anchor-id="understanding-a-figure-1"><span class="header-section-number">56</span> Understanding a “Figure 1”</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_60.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 60</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>This is similar to BERT, in bert we have positional embedding.</p>
<p>Here we split the picture but still we conserve the order by remembering the index values which define the value</p>
<p>Bert process information in parallel, like in the paper see image below and why do we need positional embeding is because:</p>
<p>BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for natural language processing tasks. Unlike traditional sequential models, transformers process input data in parallel, which makes them highly efficient but also means they don’t inherently understand the order of the input sequence. To address this limitation and enable transformers to capture sequential information, positional embeddings are used.</p>
<p>Positional embeddings are added to the input embeddings to provide information about the position of each token in a sequence. In BERT, the model processes the input tokens in parallel, and without positional embeddings, it would have no inherent understanding of the order of the tokens. Positional embeddings help the model distinguish between tokens based on their position in the sequence, allowing it to capture the sequential structure of the input.</p>
<p>Coming back to ViT model, here we can see that the model also process information in parallel that’s why we need positional embedding so that we can then learnthe order of how the pic was constructed</p>
<center>
<img src="imgs/2023-11-28-23-58-08.png" class="w550 img-fluid">
</center>
<pre></pre>
</section>
<section id="quiz-from-what-you-now-know-about-attention-what" class="level2" data-number="57">
<h2 data-number="57" class="anchored" data-anchor-id="quiz-from-what-you-now-know-about-attention-what"><span class="header-section-number">57</span> Quiz: From what you now know about attention, what</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_61.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 61</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Both attention mechanisms and convolutions are essential components in neural network architectures, and they each have their advantages and disadvantages. Here’s a comparison of the two:</p>
<section id="attention-mechanisms" class="level3" data-number="57.1">
<h3 data-number="57.1" class="anchored" data-anchor-id="attention-mechanisms"><span class="header-section-number">57.1</span> Attention Mechanisms:</h3>
<section id="advantages" class="level4" data-number="57.1.1">
<h4 data-number="57.1.1" class="anchored" data-anchor-id="advantages"><span class="header-section-number">57.1.1</span> Advantages:</h4>
<ol type="1">
<li><p><strong>Global Context Handling:</strong> Attention mechanisms allow the model to focus on different parts of the input sequence when making predictions, enabling the model to consider global context and dependencies.</p></li>
<li><p><strong>Variable Receptive Field:</strong> Attention doesn’t enforce a fixed receptive field, meaning the model can attend to different parts of the input sequence with varying degrees of focus. This flexibility can be beneficial for tasks where capturing long-range dependencies is crucial.</p></li>
<li><p><strong>Sequence-to-Sequence Tasks:</strong> Attention mechanisms have been particularly successful in sequence-to-sequence tasks, such as machine translation, where the input and output sequences can have varying lengths and alignments.</p></li>
</ol>
</section>
<section id="disadvantages" class="level4" data-number="57.1.2">
<h4 data-number="57.1.2" class="anchored" data-anchor-id="disadvantages"><span class="header-section-number">57.1.2</span> Disadvantages:</h4>
<ol type="1">
<li><p><strong>Computational Complexity:</strong> Attention mechanisms can be computationally expensive, especially with large sequences, as they require pairwise comparisons between all elements in the sequence.</p></li>
<li><p><strong>Memory Requirements:</strong> The model needs to store attention weights for each element in the sequence, leading to increased memory requirements.</p></li>
</ol>
</section>
</section>
<section id="convolutional-operations" class="level3" data-number="57.2">
<h3 data-number="57.2" class="anchored" data-anchor-id="convolutional-operations"><span class="header-section-number">57.2</span> Convolutional Operations:</h3>
<section id="advantages-1" class="level4" data-number="57.2.1">
<h4 data-number="57.2.1" class="anchored" data-anchor-id="advantages-1"><span class="header-section-number">57.2.1</span> Advantages:</h4>
<ol type="1">
<li><p><strong>Parameter Sharing:</strong> Convolutional layers use shared weights, which reduces the number of parameters in the model. This can make convolutional networks more computationally efficient and easier to train, especially on tasks with limited data.</p></li>
<li><p><strong>Local Receptive Field:</strong> Convolutional layers have a fixed-size receptive field, allowing them to capture local patterns and spatial hierarchies efficiently.</p></li>
<li><p><strong>Translation Invariance:</strong> Convolutional layers can provide some degree of translation invariance, meaning they can recognize patterns regardless of their exact position in the input.</p></li>
</ol>
</section>
<section id="disadvantages-1" class="level4" data-number="57.2.2">
<h4 data-number="57.2.2" class="anchored" data-anchor-id="disadvantages-1"><span class="header-section-number">57.2.2</span> Disadvantages:</h4>
<ol type="1">
<li><p><strong>Limited Global Context:</strong> Convolutional layers have a fixed receptive field, which may limit their ability to capture long-range dependencies in the data.</p></li>
<li><p><strong>Not Well-Suited for Sequence Tasks:</strong> While convolutional layers are effective for image-related tasks, they may not be as naturally suited for sequence-to-sequence tasks where the input and output lengths can vary.</p></li>
</ol>
<p>In practice, a combination of both attention mechanisms and convolutional layers is often used in hybrid models to leverage the strengths of each. For example, the Transformer architecture combines self-attention mechanisms with feedforward layers, providing an effective approach for a variety of natural language processing tasks.</p>
</section>
</section>
</section>
<section id="vision-transformer-1" class="level2" data-number="58">
<h2 data-number="58" class="anchored" data-anchor-id="vision-transformer-1"><span class="header-section-number">58</span> Vision Transformer</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_62.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 62</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="vision-transformer-2" class="level2" data-number="59">
<h2 data-number="59" class="anchored" data-anchor-id="vision-transformer-2"><span class="header-section-number">59</span> Vision Transformer</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_63.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 63</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In ViT we actually learn the positional embeddings, compared to Bert, we can actually visualize them and see that</p>
</section>
<section id="attention-as-a-superset-of-convolutions" class="level2" data-number="60">
<h2 data-number="60" class="anchored" data-anchor-id="attention-as-a-superset-of-convolutions"><span class="header-section-number">60</span> Attention as a superset of convolutions</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_64.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 64</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="training-a-vit-is-more-difficult" class="level2" data-number="61">
<h2 data-number="61" class="anchored" data-anchor-id="training-a-vit-is-more-difficult"><span class="header-section-number">61</span> Training a ViT is more difficult</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_65.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 65</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="vil-features" class="level2" data-number="62">
<h2 data-number="62" class="anchored" data-anchor-id="vil-features"><span class="header-section-number">62</span> Vil features</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_66.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 66</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="also-here-imagenet-can-more-or-less-be-solved-with-textures" class="level2" data-number="63">
<h2 data-number="63" class="anchored" data-anchor-id="also-here-imagenet-can-more-or-less-be-solved-with-textures"><span class="header-section-number">63</span> Also here: ImageNet can (more or less) be solved with textures</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_67.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 67</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="swin-transformer-add-hierarchy-back-in" class="level2" data-number="64">
<h2 data-number="64" class="anchored" data-anchor-id="swin-transformer-add-hierarchy-back-in"><span class="header-section-number">64</span> Swin Transformer: add hierarchy back in?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_68.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 68</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="hybrid-architectures-get-best-performances-atm" class="level2" data-number="65">
<h2 data-number="65" class="anchored" data-anchor-id="hybrid-architectures-get-best-performances-atm"><span class="header-section-number">65</span> Hybrid Architectures get best performances (atm)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_69.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 69</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="the-perceiver" class="level2" data-number="66">
<h2 data-number="66" class="anchored" data-anchor-id="the-perceiver"><span class="header-section-number">66</span> The Perceiver</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_70.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 70</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="the-perceiver-main-idea" class="level2" data-number="67">
<h2 data-number="67" class="anchored" data-anchor-id="the-perceiver-main-idea"><span class="header-section-number">67</span> The Perceiver: main idea</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_71.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 71</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="the-perceiver-taming-quadratic-complexity" class="level2" data-number="68">
<h2 data-number="68" class="anchored" data-anchor-id="the-perceiver-taming-quadratic-complexity"><span class="header-section-number">68</span> The Perceiver: Taming quadratic complexity</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_72.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 72</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="title-1" class="level2" data-number="69">
<h2 data-number="69" class="anchored" data-anchor-id="title-1"><span class="header-section-number">69</span> Title</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_73.png" class="w575 img-fluid figure-img"></p>
<figcaption class="figure-caption">Slide 73</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="notes-on-weight-sharing-for-cnn" class="level2" data-number="70">
<h2 data-number="70" class="anchored" data-anchor-id="notes-on-weight-sharing-for-cnn"><span class="header-section-number">70</span> Notes on weight sharing for CNN</h2>
<p>A convolutional layer is generally comprised of many “filters”, which are usually 2x2 or 3x3. These filters are applied in a “sliding window” across the entire layer’s input. The “weight sharing” is using fixed weights for this filter across the entire input. It does not mean that all of the filters are equivalent.</p>
<p>To be concrete, let’s imagine a 2x2 filter 𝐹 striding a 3x3 input 𝑋 with padding, so the filter gets applied 4 times. Let’s denote the unrolled filter 𝛽</p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
  x_{11} &amp; x_{21} &amp; x_{31} \\
  x_{12} &amp; x_{22} &amp; x_{32} \\
  x_{13} &amp; x_{23} &amp; x_{33} \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{F} = \begin{bmatrix}
  w_{11} &amp; w_{21} \\
  w_{12} &amp; w_{22} \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\boldsymbol{\beta} = \begin{bmatrix}
  w_{11} &amp; w_{12} &amp; w_{21} &amp; w_{22} \\
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\mathbf{F} \cdot \boldsymbol{\beta} = \begin{bmatrix}
  \beta \cdot [x_{11}, x_{12}, x_{21}, x_{22}] &amp; \beta \cdot [x_{12}, x_{13}, x_{22}, x_{23}] \\
  \beta \cdot [x_{21}, x_{22}, x_{31}, x_{32}] &amp; \beta \cdot [x_{22}, x_{23}, x_{32}, x_{33}]
\end{bmatrix}
\]</span></p>
<p>“Weight sharing” means when we apply this 2x2 filter to our 3x3 input, we reuse the same four weights given by the filter across the entire input. The alternative would be each filter application having its own set of inputs (which would really be a separate filter for each region of the image), giving a total of 16 weights, or a dense layer with 4 nodes giving 36 weights.</p>
<p>Sharing weights in this way significantly reduces the number of weights we have to learn, making it easier to learn very deep architectures, and additionally allows us to learn features that are agnostic to what region of the input is being considered.</p>
<p><img src="https://i.stack.imgur.com/cWe7D.gif" class="img-fluid"></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"><span class="footerDaniloToapanta">Mantained by Danilo Toapanta</span></a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../coming-soon.html">Newsletter</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../docs/sitemap.xml">RSS</a>
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>

    let navbar = document.getElementsByClassName("navbar-nav")[0]    

    let li2 = document.createElement("li");
    li2.className = "nav-item compact";

    let a2 = document.createElement("a");
    a2.className = "nav-link quarto-color-scheme-toggle";
    a2.style.cursor = "pointer"
    li2.appendChild(a2)

    let i2 = document.createElement("i");
    i2.className = "bi bi-moon"
    a2.append(i2)

    navbar.appendChild(li2);

    i2.onclick = function() {
        window.quartoToggleColorScheme(); return false;
    }
    // <a href="http://localhost:4200/about/" class="quarto-color-scheme-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>

    let li = document.createElement("li");
    li.className = "nav-item compact";

    let a = document.createElement("a");
    a.className = "nav-link";
    a.style.cursor = "pointer"
    li.appendChild(a)

    let i = document.createElement("i");
    i.className = "bi bi-search"
    a.append(i)

    // let span = document.createElement("span");
    // span.className = "menu-text"
    // a.append(span)

    navbar.appendChild(li);

    a.onclick = function() {
        window.quartoOpenSearch()
    }


</script>





</body></html>