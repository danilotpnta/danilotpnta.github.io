<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Danilo Toapanta">
<meta name="dcterms.date" content="2023-11-13">
<meta name="description" content="Description of this Post">

<title>Sequence Labelling – Danilo Toapanta</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/images/danilo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<!-- <link href="../../site_libs/quarto-contrib/material-icons-0.14.2/mi.css" rel="stylesheet"> -->
<script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../css/index-posts.css">
</head>

<body class="floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <div id="quarto-announcement" data-announcement-id="38450707a32148436d55c280293513e2" class="alert alert-success hidden"><i class="bi bi-info-circle quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p><strong>Let op</strong> - Starting Jan 2025 I will carry out my 36 ECs AI master’s thesis. Have an interesting idea to work on. Let us talk, I am open to <span class="same-color"><a href="https://www.danilotpnta.com/contact/">opportunities</a></span>!</p>
</div></div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title"><span id="danilo_topanta_brand"> Danilo Toapanta</span> <a id="mysite" class="mysite" href="../../site-ver-hist/">v1.3</a></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text"><span id="home-welcome-msg">Home</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/danilotpnta?tab=repositories" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full blog-page" style="display: none !important;">
    <div class="quarto-title-banner page-columns page-full">
        <div class="quarto-title column-body">
            <h1 class="title">Sequence Labelling</h1>
                
            <!-- Description Block -->
                        <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
                        
            <!-- Categories Block -->
                                            <div class="quarto-categories">

                    <!-- Display Categories -->
                                            <div class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=NLP">
                                NLP
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div> 
                    
                    <!-- Display Tags if any -->
                                    </div>
                            
        </div>
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">November 13, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    
</header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">
<script>
    var currentUrl = window.location.href;
    var index_init_post = currentUrl.lastIndexOf("/20");
    var string_init_post= currentUrl.slice(index_init_post, index_init_post+3 );

    // console.log("currentUrl: " + currentUrl);
    // console.log("index: " + index_init_post);
    // console.log("string: " + string_init_post);

    // If is equal to /blog/20... then make navbar title READING MODE
    if (string_init_post === "/20"){
        let mysite = document.getElementById("mysite");
        mysite.classList.add("mysite-change");

        let navbar = document.getElementById("danilo_topanta_brand");
        navbar.classList.add("navbar-brand-change");

        // This will render a new title saying READING DANILOS BLOG
        // navbar.innerHTML = 'You are Reading Danilo\'s Blog<span style="font-size:35px; vertical-align: middle; opacity: 0.65; padding-bottom: 6px; padding-left: 14px;" class="material-icons-round"> auto_awesome </span>';
        
        const smallDevice = window.matchMedia("(min-width: 570px)");
        smallDevice.addListener(handleDeviceChange);

        function handleDeviceChange(mediaQuery) {
            if (mediaQuery.matches) {
                navbar.innerHTML = "";
                // navbar.innerHTML = "<-- You are Reading Danilo's Blog -->";
            } else  {
                navbar.innerHTML = "Danilo Toapanta";
            }
        }

        // Run it initially
        handleDeviceChange(smallDevice);

        let link = document.getElementsByClassName("navbar-brand")[0];
        link.classList.add("disablePointerEvents");

        let brand_container = document.getElementsByClassName("navbar-brand-container")[0];
        brand_container.classList.add("navbar-brand-container-new-padding");

    }
</script>


<!-- <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Running my first Marathon</h1>
                  <div>
        <div class="description">
          I will be running at the 42km TCS Amsterdam 2023, 15th October
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">News</div>
              </div>
                  </div>
  </div> -->

  <!-- ---
  coming-soon: true
  tags: [Spanish]
  --- -->








<main id="title-block-header" class="quarto-title-block default page-columns page-full" style="padding-bottom: 40px;">

    <div class="quarto-title column-body" style="margin-bottom: 1em;">
        <h1 class="title" style="padding-bottom:8px" ;="">Sequence Labelling</h1>
        
        <!-- Description Block -->
                    <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
        
        <!-- Categories Block -->
                    
                <!-- Display Categories -->
                <div class="quarto-categories">
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title">
                        <i class="fa-solid fa-hashtag" ></i> Categories:
                    </div> -->

                                            <div id="All" class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div>
                                            <div id="NLP" class="quarto-category">
                            <a href="../../blog/#category=NLP">
                                NLP
                            </a>
                        </div>
                                            <div id="TAGS" class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div>
                                    </div>
                


                <div class="quarto-categories tag-categories">
                    
                    <!-- Tags Icon  -->
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title"> -->
                        <!-- <i class="fa-solid fa-tag" ></i> Tags: -->
                        <!-- <i class="fa-solid fa-hashtag" ></i> Tags: -->
                        <!-- <span class="material-icons-outlined" >local_offer</span> Tags: -->
                        <!-- / -->
                    <!-- </div> -->

                    <!-- Display Tags -->
                                            <div id="All-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=All">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                All
                            </a>
                        </div>
                                            <div id="NLP-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=NLP">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                NLP
                            </a>
                        </div>
                                            <div id="TAGS-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=TAGS">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                TAGS
                            </a>
                        </div>
                    
                    
                </div>

                    
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">November 13, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    <!-- Current link: Font-awesome, Google icons, Bootstrap icons -->
    
</main>


<section id="representing-and-estimating-categorical-distributions-the-logistic-case" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="representing-and-estimating-categorical-distributions-the-logistic-case"><span class="header-section-number">1</span> Representing and estimating Categorical distributions: the logistic case</h2>
<center>
<img src="imgs/2023-11-13-22-47-11.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>W is condition on the Random variable H, which can be a history</p>
<ul>
<li><p><strong>f</strong> = is a function of the history <span class="math inline">\(h\)</span> and has some parameters <span class="math inline">\(\theta\)</span><br>
<strong>f</strong> is not a real value but a collection of probability values</p></li>
<li><p>W = word<br>
It can take on <span class="math inline">\(V\)</span> categories, so W can take on <span class="math inline">\(V\)</span> possible assignments</p></li>
<li><p>V = vocabulary size</p></li>
<li><p>H = history h the history can take as many values as possible i.e (BOS, a), (BOS, an), (BOS, some)</p></li>
</ul>
<p>If you take a single row then all these values would add up to 1</p>
<p>A Tabular representation is table look up operation. The yellow row represents then one <span class="math inline">\(\textbf{f}=\theta_{row=h}^{col=1...V}\)</span></p>
<p>Then we need to think the function</p>
<p><span class="math display">\[
\textbf{f}(h; \theta)
\]</span></p>
<p>as a mechanism to predict vectors from the conditional information</p>
</section>
<section id="logistic-representation-of-categorical-cpds" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="logistic-representation-of-categorical-cpds"><span class="header-section-number">2</span> Logistic Representation of Categorical CPDs</h2>
<p>CPDs= Conditional Probability Distribution</p>
<p>What we are predicting with this Categorical distribution is the probabilities per each W under the condition of an <span class="math inline">\(h\)</span> so on a row. Essentially we are predicting every single entry in the tabular representation. I am interest then to map these to</p>
<center>
<img src="imgs/2023-11-13-22-51-44.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>after each conditional information has been mapped to a point, I will map those points to a different space. I want a vecgor with <span class="math inline">\(V\)</span> probabilities in it, one per each possible outcomes of the random variable <span class="math inline">\(W\)</span>.</p>
<center>
<img src="imgs/2023-11-13-23-03-45.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>Here we have <span class="math inline">\(\phi\)</span>(small,birs) <span class="math inline">\(\in \mathbb{R}^{2}\)</span> so in 2D that means I need my <span class="math inline">\(W_1\)</span> also to be in 2D so when I multiply with <span class="math inline">\(\phi\)</span>(small,birs), I get an scalar value. <span class="math inline">\(b_1\)</span> is just an scalar.</p>
<p>These last row <span class="math inline">\(\textbf{S}\)</span> is called vector of ‘Logits’ these can be large or negative to turn this into a vector inside the probability simplex we can use Softmax. Note we go from the dimensional space to logits throught a lienar model. And then from logits to probabilities with a Softmax function</p>
<p>The <span class="math inline">\(n\)</span>-dimensional probability simplex, denoted as <span class="math inline">\(\Delta^n\)</span>, is defined as:</p>
<p><span class="math display">\[
\Delta^n = \left\{ (p_1, p_2, \ldots, p_n) \mid p_i \geq 0, \sum_{i=1}^n p_i = 1 \right\}
\]</span></p>
<p>Here, <span class="math inline">\(p_i\)</span> represents the probability of event <span class="math inline">\(i\)</span>, and the conditions <span class="math inline">\(p_i \geq 0\)</span> and <span class="math inline">\(\sum_{i=1}^n p_i = 1\)</span> ensure that the vector lies within the simplex.</p>
<center>
<img src="imgs/2023-11-13-23-18-35.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>The model can turn the h into a vector of <span class="math inline">\(V\)</span> probabilities. so the <span class="math inline">\(\textbf{f}_1(h,\theta), \textbf{f}_2(h,\theta)...\)</span></p>
<section id="why-is-this-a-great-idea" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="why-is-this-a-great-idea"><span class="header-section-number">2.1</span> Why is this a great idea?</h3>
<center>
<img src="imgs/2023-11-13-23-26-34.png" class="w550 img-fluid">
</center>
<pre></pre>
<ol type="1">
<li><p>The model size is a function of how many <span class="math inline">\(w\)</span>s we have. So the model is very compact, because it does not depend on how many instances of h, because we have this basis function <span class="math inline">\(\phi\)</span> that maps it to the proper dimensions so that we can multiply it with <span class="math inline">\(W^T\)</span></p></li>
<li><p>Histories are conditional information are no longer treated as unrelated to one another, now in fact are related to each other trhough their features. Because of these relatedness linear models work, because they find patterns that we can code in a tabular reprsentation</p></li>
</ol>
<ul>
<li>Suppose you condition on a sentence to draw a prob of a label/class.</li>
<li>Imagine you have 5 labels/classes, the thing that you condition can be represented by a feature function that gives you D-dimensional features so we are talking about <span class="math inline">\(\phi(h) \in \mathbb{R}^D\)</span>. For each of those dimensions you would weight them towards a class, for instance if you have <span class="math inline">\(K\)</span>-classes, so <span class="math inline">\(KD\)</span>. <strong>Note</strong> <span class="math inline">\(K=V\)</span></li>
<li>Each feature gets a relevance an importance score towards a class and each class get a bias <span class="math inline">\(b\)</span>. You can think of <span class="math inline">\(b\)</span> of how usefull this class is, or how generally present it is in the data regardless of context.</li>
<li>So per feature, per class you have a real number <span class="math inline">\(KD\)</span> + <span class="math inline">\(K\)</span> (from the bias) for each of the possible classes in my hypothetical example</li>
</ul>
</section>
</section>
<section id="estimation-example" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="estimation-example"><span class="header-section-number">3</span> Estimation example</h2>
<p>How do we estimate the W_s and the b_s</p>
<ul>
<li>We get data as pair, where we have <span class="math inline">\(h_n\)</span> as a conditional variable &amp; <span class="math inline">\(w_n\)</span> an assigment of the outcome variable</li>
<li>Our model would be log-linear it will map from the history through a feature function it will map histories to a vector of probability values of the correct dimensionality</li>
<li><span class="math inline">\(s\)</span> = W<span class="math inline">\(\theta\)</span> + b is called the parametric function. Through this <span class="math inline">\(s\)</span> our model predicts from any history h a complete categorical mass function with <span class="math inline">\(V\)</span> values</li>
</ul>
<p>To learn the parameters we initialize it, and then we compute the log likelihood of the parameters <span class="math inline">\(\theta\)</span> give our data <span class="math inline">\(D\)</span>. This can be solve due to the idd assumption.</p>
<center>
<img src="imgs/2023-11-13-23-40-01.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>When we solve for the grad wrt. <span class="math inline">\(\theta\)</span> we do not get a closed-form solution like we get in the tabular representation. Thus we use an iterative procedure aka SGD.</p>
<p>Matmul is the same as doing dot product but for matrices take a look at this <a href="https://mkang32.github.io/python/2020/08/30/numpy-matmul.html#np.dot:~:text=dot%20product%20only.-,(4)%20matrix%20multiplication%3A%20np.matmul,-The%20next%20option">link</a></p>
Example:
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/2023-11-14-11-35-53.png" class="w550 img-fluid figure-img"></p>
<figcaption>code at <a href="https://www.youtube.com/watch?v=DsflDpm5h-c&amp;ab_channel=WilkerAziz">Colab</a></figcaption>
</figure>
</div>
</center>
<!-- <center>![Slide 1](imgs/page_1.jpeg){.w575}</center><pre></pre> -->
</section>
<section id="start-of-lecture" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="start-of-lecture"><span class="header-section-number">4</span> Start of Lecture</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_2.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 2</figcaption>
</figure>
</div>
</center>
<pre></pre>
<!-- <center>![Slide 3](imgs/page_3.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 4](imgs/page_4.jpeg){.w575}</center><pre></pre> -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_5.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_6.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="why-do-we-want-to-classify-words-in-classescategories" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="why-do-we-want-to-classify-words-in-classescategories"><span class="header-section-number">5</span> Why do we want to classify words in classes/categories</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_7.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Reduce the number of dimensions like in bigrams only consider the nouns and adjectives and that is to measure the sentiment of a review. So here we refer to feature as the <span class="math inline">\(\phi(h)\)</span> so if we reduce this feature vector say <span class="math inline">\(D\)</span> to a less than <span class="math inline">\(D\)</span> i.e in bigrams <span class="math inline">\(D=2\)</span> then we save memory and reduce the dimensions</p>
<!-- <center>![Slide 8](imgs/page_8.jpeg){.w575}</center><pre></pre> -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_9.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In out tabular CPDs we are force to treat each words as if they are unrelated to each other, but that is not trueth because as we sa before we can have relatedness i.e adjectives preceed nouns.</p>
<p>So to capture relatedness the categorization of words can capture that.</p>
<ul>
<li>For example how to classify them:</li>
</ul>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_10.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_11.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Once we have categorized we can even not use the words because we know already its categories and we know that we this categorization a sentences would be like noun + adjective, so then we dont need the actual word but rather just its category.</p>
<ul>
<li>Called the above a mapping. Note this could not be good enough in some instances. A reason why to do this is so that we reduce the dimensionality space.</li>
</ul>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_12.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here an example of how people classify unambiguously</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_13.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_14.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="how-to-annotate-words-with-part-of-speech-pos-using-peen-style" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="how-to-annotate-words-with-part-of-speech-pos-using-peen-style"><span class="header-section-number">6</span> How to annotate words with Part Of Speech (POS) using Peen Style</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_15.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
</center>
<pre></pre>
<!-- <center>![Slide 16](imgs/page_16.jpeg){.w575}</center><pre></pre> -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_17.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
</center>
<pre></pre>
This is a join distribution
<center>
<img src="imgs/2023-11-14-13-15-14.png" class="w350 img-fluid">
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_18.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Recall we model text as a join distribution over all available tokens</p>
<p>Now we model POS_tagged text: joint distribution over text and their POS tags</p>
<ul>
<li>Our goal is to assigned probabilities to the sequence paits <span class="math inline">\((w_{1:l}, c_{1:l})\)</span></li>
</ul>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_19.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li><strong>Recall </strong></li>
</ul>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_20.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li><p><em>W</em> Words is a random variable for us, are tokens in Caligraphic_W (a vocabulary of symbols) that I know. The size of the vocabulary is V</p></li>
<li><p>w is an specific word</p></li>
<li><p><em>C</em> is for Category and is a random variable, that are in Caligraphic_C (the tagset). The size of the tagset is K</p></li>
<li><p>c is an specific tag</p></li>
<li><p><em>X</em> is a random sequence (our token sequence from lst week), from words from one until the lenght L in order</p></li>
<li><p><span class="math inline">\(w_{1:l}\)</span> is a sequence of l words from the Caligraphic_W w/ vocabulary size V</p></li>
<li><p><em>Y</em> is a random sequence</p></li>
<li><p><span class="math inline">\(c_{1:l}\)</span> is a sequence of l tags from the Caligraphic_C w/ tagset size K</p></li>
</ul>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_21.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Now we want a distribution over the cross product of all Text and all Tags sequence: <span class="math inline">\(w_{1:l}, c_{1:l}\)</span>. This is an enormous space</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_22.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>You can do two things with this:</p>
<ol type="1">
<li><p>For instance, I give you the ‘little cat’ and you want to know what is the prob sequence under the model. You can compute the possible probability sequence given that you give me a certain token sequence and then I could look for what maximizes this probability</p></li>
<li><p>Another we can do a language model, a language model was a mechanism to assign probabilities to strings without any tags, BUT if you get a distribution over two variables, token sequences and class sequences and you marginalized out one of the variables. ie you marginalized my choice for the first class, for the second class, all the way to the last class. If I sum the probabilities of all possible tag sequences that pair with these word sequence then I get a marginal probability. So in the above equation, the left part:</p></li>
</ol>
<center>
<img src="imgs/2023-11-14-15-02-30.png" class="w250 img-fluid">
</center>
<pre></pre>
<p>So that is the probability of a sentence so <span class="math inline">\(P_X(w_{1:l})\)</span> this is a language model.</p>
<p>So once you design a model that can assign join probabilities to labeled sequences of words then you can use it to either:</p>
<ol type="1">
<li>tag news sequences of words with their respective categories or</li>
<li>assign prob to a sequence of words, regardless of which classes they have been annotated</li>
</ol>
<p>Remember NB classifier you can do:</p>
<ol type="1">
<li>you can annotate what is the most probable class of a document</li>
<li>assign prob to a document regardless of their class (is the denominator of the NB model). Note the den of Bayes rule is the marginal</li>
</ol>
<p>This second step is common to all generative model. Every time you have</p>
<center>
<img src="imgs/2023-11-14-15-13-09.png" class="w350 img-fluid">
</center>
<pre></pre>
<p>C1..C3 is the class tha sits at position 1..3</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_23.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Last time we created a distribution and we created a new one, by chopping out sequences and working with probabilities for words given a short history, now we will do something similar but this time we are gonna built a distribution with pairs of sequences</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_24.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Steps: a way of describing the generation of a tag sequence pair. And wathever we call step that is the unit we are gonna sign probabilities to.</p>
<!-- <center>![Slide 25](imgs/page_25.jpeg){.w575}</center><pre></pre> -->
<center>
<img src="imgs/2023-11-14-15-20-48.png" class="w550 img-fluid">
</center>
<pre></pre>
<pre></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Markov Model Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Here is this image we have assume <strong>Markov</strong> assumption</p></li>
<li><p>At the tag level it looks like a bigram model and not somethingelse, we have made the assumption that when we have generated certain variable I can forget a lot of the precedding context all the classes that ocurred before except for the last one and even all the words that were generated before so this conditional independence is a markov assumption.</p></li>
</ul>
<p>Why is hidden? originally it was decided so that you do not get to see which class is for each on every position, these are latent variable. Latent variables are the variables you pretend there is there but you dont observe its value. We do this because it makes modelling easier or more usefull for a certain purpose. Here we are observing both, the token sequence and the tag sequence but if you would not see the tag sequence yet pretend that it exists a modeling desing choice and it will be hidden for you.</p>
</div>
</div>
<ul>
<li><p>At the class level this looks like a bigram because when we genreate a symbol. For instance when you generates a symbol for isntance when you generate ‘is’ (when VERB is generated) we can use the fact that you have just generated a NOUN this is like a bigram. To generate one word you can condition on the previous one but this time we are not generating words but are generating tag sequences</p></li>
<li><p>This model starts with the tag sequence not from the text , isnt that weird? Not really the NB started from class not from the text you generate the class and given the class you generate the words in the text with a conditional independence</p></li>
<li><p>So at the tag level we are doing bigrams language modelling, the orange stuff exits even without the blue stuff, so the orange stuff is never conditioning on the words. <em>But</em> the words comes from somewhere, from their ‘categories’. I.e if I would generate a verb and I know is a verb, isnt it easier to assign lower probabilities to nouns, I already know is a verb so maybe that in its own is gonna lead to conditional distributions that are much more compact because the set of verbs are more compact than the set of all words</p></li>
</ul>
<p>It is not because of the factorization starts with classes and then generates words, that can be the other way around, we can observe a word sequence and ask: what is the most probable tag sequence.</p>
<p><strong>What is a factorization?</strong> It is a decomposition of products see next slide. So I decomposed the enttire sequence pair (the orange sequence and the blue sequence) into the probabilities of the number of circles with an arrow pointing to them.</p>
<ul>
<li>The trick again you add the BoS and EoS, why do I do that? because I know that for every class I am generating there is something before to condition before (except in the first class that is why BoS fixes that) and the second reason is that I give to every single symbol a chance to be part of the conditional context that is what EoS is helpful for. In the img above it translates to also given teh punctuation a chance to assign the probability of a sequence ending</li>
</ul>
<!-- - For a particular word you are condition on its part of speach and nothing else, the problem with that is that only with PoS is not enough to condition the word. It is not enough because one word could be a noun but also could be a verb. So there is ambiguity. The conditional independence is really strong that no matter how good an HMM we have, we always get confusion     -->
<p>Lets imagine the following, lets take the words away just leave the categories? Now if we were an HHM so a model, then the question is give me a VERB. Then the answer is tha there is no way I would get the same verb twice. Some poeple reply: jump other: run so there is a ot of variation.</p>
<p>Because our model is not that advanced our model can: - describe statistical patterns of pair of classes that ocurr in a relative order, - it can explain the distribution of our words given the categories</p>
<p>In more concrete notes: the model can lear for instance that 80% of the time VERBS appear after right after NOUNS but it cannot sample the noun and the verb, the actual words together. You can get book is or book are, there is no agreement there which verb it will be.</p>
<p>Is there a trap where it goes NOUN VERB NOUN VERB … but this is very unlikely if we do MLE it is very unlikely to fall in this trap because you would not see this traps in the data. It is unlikely if you do MLE, but if you start fideling with smoothing things get really weird.</p>
<p>Every time that a history h is given, this corresping 1:1 to a factorization, which we can get it whit the chain rule as follows:</p>
</section>
<section id="hidden-markov-model" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="hidden-markov-model"><span class="header-section-number">7</span> Hidden Markov Model</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_26.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In the slide above we forgot in the same qual to write above ‘idd’ assumption.</p>
<p>The <span class="math inline">\(w_i\)</span> here is the word that is emitted for that particular class. We call that an emission probability</p>
</section>
<section id="generative-history" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="generative-history"><span class="header-section-number">8</span> Generative History</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_27.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We can sample from an HMM by following the history.</p>
<ol type="1">
<li><p>start two sequence the token sequence (the words) and the label sequence (c_0 = BOS)</p></li>
<li><p>Condition on the previous class so that is c_0 so condition on BOS, and extend Y with BOS. Whatever class you get extend your sequence to Y and move on. Here we use the <strong>transition</strong> distribution. Here the P(c_i|c_i-1) would be the transition distribution</p></li>
</ol>
<p>^ Here we draw a c_i and that we use for the next step</p>
<ol start="3" type="1">
<li>Condition on current class c_i and draw a word w_i with probability P_w|c(w_i_i). So when we say random we meant np.random.choice([vector], weights=P_w|c(w_i_i)). Whatever word you get extend your sequence to X and move on<br>
For instance you can get a Determiner, so do P_w|c(w_i)) and draw a word from it. Here we use the <strong>emission</strong> distribution.</li>
</ol>
<p>If your sequence is too long then stop. And this would be the factorization of the HMM</p>
<center>
<img src="imgs/2023-11-14-15-20-48.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>Here in the picture you start with BoS, then you fo to DET and then to the and then to NOUN and book and so on..</p>
<p>Which tasks is this model use for? in 2023 none.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_28.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In the trasition distribution we would go from the prev class called ‘<span class="math inline">\(r\)</span>’. Once you know it you can retrieve <span class="math inline">\(K\)</span> probabilities one for each of the tags that you may trasition to. For instance <span class="math inline">\(K=10\)</span> so NOUN, VERBS, ADJ, … so given one of them which we call it i.e ‘<span class="math inline">\(r=DETERMINER\)</span>’ then I can retrieve <span class="math inline">\(K\)</span> numbers which I called them <span class="math inline">\(\lambda_1, \lambda_2 ... \lambda_K\)</span> And these are the probability values for the conditional probability of drawing a NOUN given a DETERMINER, a VERB given a DETERMINER, a ADJECTIVE given a DETERMINER, ….</p>
<p>Now for the Emission distribution or W|C, we need to specify:</p>
<ul>
<li>What are you emitting from, which class ie. a VERB, that is the little ‘<span class="math inline">\(c\)</span>’. Once you know what are you emitting from, then you can retrieve a collection of <span class="math inline">\(V\)</span> numbers which are <span class="math inline">\(\theta_1,\theta_2,.. \theta_V\)</span>. These are the probability from which you draw a certain word in the vocabulary of known words given that you are emitting from little ‘<span class="math inline">\(c\)</span>’ category.</li>
</ul>
<p>Once you have all these tabels all the tabular representation, now you can assign a probability to any sequence pair by taking the product of the relevant number. So go to the entry (row) of the table that conditions on the previous tag and find the probability (per each column) of the current tag. Or go to the table of emissions find the row that corresponds to the current class and multiply the probability of generating the current word given that class so multiply it with the (emission distribution). You will find values from each of the tables and you will multiply them together</p>
</section>
<section id="example" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="example"><span class="header-section-number">9</span> Example</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_29.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<img src="imgs/2023-11-14-22-25-47.png" class="w350 img-fluid">
</center>
<pre></pre>
<p>What is the difference between this and what was done last time?</p>
<p>At an abstract level we have two sets of outcomes, we have a token sequence and a class sequence and there is also some difference in the parametrisation like what condition independences we make but the two are familiar and the tables are also familiar</p>
<p>Which then leads to the following question:</p>
</section>
<section id="r-rightarrow-c" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="r-rightarrow-c"><span class="header-section-number">10</span> <span class="math inline">\(r \rightarrow  c\)</span></h2>
<!-- <center>![Slide 30](imgs/page_30.jpeg){.w575}</center><pre></pre> -->
<p>Now I give you data and I want to estimate the values of our parameters and I choose MLE, I give you data, meaning I give you word sequences (aka sentences i.e ‘A nice dog’) for which you know the type sequences (so their PoS: DETERMINANT, ADJECTIVE&lt; NOUN).</p>
<ul>
<li>The question is then what is the probability of transition from a tag r –&gt; to a tag c i.e r=DETERMINANT, and c=NOUN. What is the probability?</li>
</ul>
<p>We do not need Bayes RUle, we would need it if my query is not a parameter, for i.e if I ask what is the prob of the prev tag given that I know the current, then that is reversive the model so that is what you use Bayes Rule for.</p>
<p>Our case is different we are asking a query that is a parameter of the model, the probability of given a class generating the next.</p>
<section id="the-question" class="level3" data-number="10.1">
<h3 data-number="10.1" class="anchored" data-anchor-id="the-question"><span class="header-section-number">10.1</span> The question:</h3>
<blockquote class="blockquote">
<ul>
<li>What is the probability of transition from a tag r –&gt; to a tag c i.e r=DETERMINANT, and c=NOUN. What is the probability?</li>
</ul>
</blockquote>
<!-- <center>![Slide 31](imgs/page_31.jpeg){.w575}</center><pre></pre> -->
</section>
<section id="the-answer" class="level3" data-number="10.2">
<h3 data-number="10.2" class="anchored" data-anchor-id="the-answer"><span class="header-section-number">10.2</span> The answer:</h3>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_32.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>So how many times I have seen a DETERMINANT that was follow by a NOUN divided by the how many times you have count DETERMINANT follwed by anything</p>
<p>This is the <strong>solution for MLE</strong>: count(condition_on=DETERMINANT, the_outcome=NOUN)/sum_k count(DETERMINANT,k). The later means the DETERMINANT pair with any other tag from the tag set</p>
<p>Where k is any possible outcome in the set of things that you know</p>
<!-- <center>![Slide 33](imgs/page_33.jpeg){.w575}</center><pre></pre> -->
</section>
</section>
<section id="c-rightarrow-w" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="c-rightarrow-w"><span class="header-section-number">11</span> <span class="math inline">\(c \rightarrow  w\)</span></h2>
<section id="the-question-1" class="level3" data-number="11.1">
<h3 data-number="11.1" class="anchored" data-anchor-id="the-question-1"><span class="header-section-number">11.1</span> The question:</h3>
<blockquote class="blockquote">
<ul>
<li>What is the probability of generating w given c?</li>
</ul>
</blockquote>
<p>The model choice is an HMM the dataset is the ‘A nice dog’, the algorithm of choice is MLE.</p>
</section>
<section id="the-answer-1" class="level3" data-number="11.2">
<h3 data-number="11.2" class="anchored"><span class="header-section-number">11.2</span> The answer:</h3>
<p>The expression to compute the mission probability. So what is the probaility of emission of w given that have just generated class c.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_34.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Note: here in the denominator is the same, sum over all V with c, being fixed</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_35.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>This approach still suffers from Data Sparcity, so the data contains plenty of zeros. For instance unseen word-tag pairs or unseen tag-tag pairs that are not too frequent and that we have not seen but this could happen. This can also happen as you tag set grows really large and maybe the tag set for a language like Arabic, Turkish, etc, i.e for these languages the tag set is really large so then maybe the transition probabilities aren’t easy to estimate. On the other hand, emission probabilities can be really sparce in the sense that i.e a language like Czeck or Portuguess where you have many word forms that are slight variation of the same thing and all have the same category so you would need a lot of data before you have seen every noun pop up or every verb.</p>
<p>This is less sparce compared to NGram LM where you have more data sparcity i.e because you need to memorize long phrases so is easy to find phrases that you have never seen thus creating sparcity.</p>
<ul>
<li>In this sense, HMM are more compact than Ngram LM, but what is the limitation with this?</li>
</ul>
<p><strong>Limitation</strong></p>
<ol type="1">
<li>When we generate a word all we know is a class, is hard to believe that if I sample from this model I would get nice sentences. I will get words that, if I extract from what they are and think only about the categories, then the trnasition from class to class would make sense, but when we look at the semantics (so the words then make no sense) i.e An furious bottle. The tags (A determinant then an adjective and then a noun) are okay but the semantics do not make sense</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
PoS: DET, Prenomial ADJ Examples
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<section id="examples-of-determinats-can-be" class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="examples-of-determinats-can-be"><span class="header-section-number">12</span> Examples of determinats can be:</h2>
<ol type="1">
<li><strong>Determiners (DET):</strong>
<ul>
<li><em>a:</em> I saw <strong>a</strong> cat in the garden.</li>
<li><em>an:</em> She has <strong>an</strong> apple in her hand.</li>
<li><em>the:</em> <strong>The</strong> sun is shining brightly.</li>
<li><em>this:</em> I would like <strong>this</strong> book, please.</li>
<li><em>some:</em> Can I have <strong>some</strong> water?</li>
</ul></li>
</ol>
</section>
<section id="examples-of-prenomial-adjectives" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="examples-of-prenomial-adjectives"><span class="header-section-number">13</span> Examples of Prenomial adjectives:</h2>
<p>Prenomial adjectives are adjectives that come before the noun they modify. Here are some examples of prenomial adjectives:</p>
<ol type="1">
<li><strong>The red apple</strong>
<ul>
<li>“red” is a prenomial adjective modifying the noun “apple.”</li>
</ul></li>
<li><strong>A beautiful sunset</strong>
<ul>
<li>“beautiful” is a prenomial adjective describing the noun “sunset.”</li>
</ul></li>
<li><strong>Three large elephants</strong>
<ul>
<li>“large” is a prenomial adjective indicating the size of the noun “elephants.”</li>
</ul></li>
<li><strong>An old book</strong>
<ul>
<li>“old” is a prenomial adjective modifying the noun “book.”</li>
</ul></li>
<li><strong>The happy child</strong>
<ul>
<li>“happy” is a prenomial adjective describing the noun “child.”</li>
</ul></li>
<li><strong>Several interesting movies</strong>
<ul>
<li>“interesting” is a prenomial adjective modifying the noun “movies.”</li>
</ul></li>
</ol>
<p>These examples illustrate how adjectives are positioned before the nouns they modify in a sentence.</p>
</section>
</div>
</div>
</div>
<!-- <center>![Slide 36](imgs/page_36.jpeg){.w575}</center><pre></pre> -->
<ol type="1">
<li><strong>Syntactics</strong></li>
</ol>
<ul>
<li>When something is syntactically correct, it means that it adheres to the grammatical rules of a given language</li>
</ul>
<ol start="3" type="1">
<li><p><strong>Syntax</strong> are the rules that dictate the order of words in a sentence and how they are structured to convey meaning.</p></li>
<li><p><strong>Semantic</strong> content of a sentence refers to the meaning or information conveyed by the arrangement of words and the relationships between them.</p></li>
<li><p><strong>Grammar</strong></p></li>
</ol>
<ul>
<li>Grammar rules refer to the set of structural and syntactic principles that govern how words are combined to form meaningful sentences and phrases in a language. These rules define the relationships between different elements of a language, such as nouns, verbs, adjectives, adverbs, and other parts of speech</li>
</ul>
<p>This model main caveat is therefore if I abstract the words to which Category they belong, then they almost look like correct words i.e NOUN, VERB, ADVERB. But when we look at the semantic content it makes no sense</p>
<p>Another example you have a singular NOUN with a plural VERB</p>
<p>HMM then are not able to make sentences but its able to cluster the words according to some vague distribution</p>
<!-- <center>![Slide 37](imgs/page_37.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 38](imgs/page_38.jpeg){.w575}</center><pre></pre> -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_39.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>There is two things you can do with an HMM:</p>
<ol type="1">
<li><p>Obtain a tag sequence when you dont know it. So you know the word sequence and you ask: Give me the most probable tag sequence</p></li>
<li><p>You can use it as a Language Model</p></li>
</ol>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_40.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Under this model, it spits out a tag sequence that is most probable. How do we evaluate this task sequence? you compare it to the tag sequence that is annotated in the dataset for you because PoS tag is almost unambiguous. For instance we will all agree of a tag sequence of english. So you compute the performance of the model as if you would in a classifier but instead of classifying once you have many classifications steps. You have one per step of the sequence so you compute accuracy. So then the performance is evaluate as accuracy across tag set.</p>
<!-- <center>![Slide 41](imgs/page_41.jpeg){.w575}</center><pre></pre> -->
</section>
</section>
<section id="example-with-3-positions" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="example-with-3-positions"><span class="header-section-number">14</span> Example with 3 positions</h2>
<center>
<img src="imgs/2023-11-15-16-48-53.png" class="w550 img-fluid">
</center>
<pre></pre>
<blockquote class="blockquote">
<p>My task: what is the task for the first word, for the second word and for third word - We have a 3-word sentence and I need to know the most probable tag sequence for that sentence - Assume that the tag set contains two tags</p>
</blockquote>
<ul>
<li><p>You are asked what is the most probable tag sequence, the only way to know is to assign prob to each and every one of the options that has been numerated so every row. More concrete you need to assing a prob to the first row to the second and so on. Assigning prob to this is relatively easy, once the table of parameters exist I just go through my sequence (per each row) and collect the relevant probabilities (so multiply all the parameters in one row)</p></li>
<li><p>Suppose we already did that So the table is filled with numerical values, meaning I will have one join prbability value for each one of the options. So now we could sort the list and pick the best</p></li>
</ul>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_43.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li>So the size of this computation would be K_1position, K_2position, K_3position, K_Lposition so K^L</li>
</ul>
<p>Because this is exponential is expensive to compute</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_44.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Dynamic programing is when you program such that it solves smaller problems whose algorithm for solving repeats itsleve and it is not generally available meaning to every programm can be dinamyc, you need to design your model with careful choices such that a dynamic program can be used. HMM can fit into a Dynamic programm. The key to the dynamic program is to look closely to my screenshots (the img above) and realized that most probabilities are the same anyway. So each row is the join probability of assing to one of the tag sequences and all the numbers are same basically everywhere so there is a lot of structure that repeats itself, the idea is to split into subproblems solve the subproblems and combine their solutions</p>
<p>In the following video we see how to do that:</p>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/rVCd7NrGcSI?si=i51GhQs68xIA8neI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="">
</iframe>
</center>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_45.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The key is: pretend there is a table alpha(i, j) which tells you if I were tagging the ith position with the jth tag i.e so the 10th position I will make it a NOUN, then I can solve this problem by combining the solutions to the previous one: the 9th position and I can do that if I know the 8th position and so on, so this becomes recursion. You can solve this if you have solve the ones before and by structuring this in a recursive call</p>
<p>In the <span class="math inline">\({\)</span> the one above is the transition emission pair for that position. You do not need recusion to implement this you can also do for-loop</p>
<p>Recall: the 2. thing can be done with an HMM is to marginalized out the tag sequences.</p>
<section id="the-concept-of-marginalization-out" class="level3" data-number="14.1">
<h3 data-number="14.1" class="anchored" data-anchor-id="the-concept-of-marginalization-out"><span class="header-section-number">14.1</span> The concept of marginalization out</h3>
<p>Imagine there is two variables, a person and a route to the university. Each path to the univerisy will cost an amount of time. If you ask the question how does does it take you on average to get to work and you have no knowledge of the path that was taken then you reason all paths could have done then you sum all the cost of all the paths and averga them out</p>
</section>
<section id="what-is-marginalization" class="level3" data-number="14.2">
<h3 data-number="14.2" class="anchored" data-anchor-id="what-is-marginalization"><span class="header-section-number">14.2</span> What is marginalization</h3>
<p>So if you have two or more variables, marginalization menas fix some of them and for whathever is left enumerate all hte possible outcomes and sum their probabilities.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_46.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We have the join probability distribution <span class="math inline">\(P_{XY(w_{1:l},c_{1:l})}\)</span></p>
<ul>
<li>There is two sequences: words and classes.</li>
<li>If we ask: regardless of class what is the probability of a sentece i.e ‘the nice dog’, they are not asking the nice dog which is a DETERMINATE, ADJECTIVE NOUN, no. They are not asking that. They are asking the prob of the sentence so then we use marginalization like the formula below:</li>
</ul>
<p><span class="math display">\[
P(X = x) = \sum_{y} P(X = x, Y = y)
\]</span></p>
<center>
<img src="imgs/2023-11-15-16-48-53.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>This is an exmaple, so based on the formula from SLide 46, we have L=3 words K=3 tags</p>
<ol type="1">
<li><p>We do not know the categories, so need to try them all. We proved before that there is an exponentially K^L growing number of how to assign tags so then we enumerate all this possible outcomes meeaning we enumerate per each row each of the possible combinations</p></li>
<li><p>For each of these rows, I know how to assing a join probability, I used the relevant <span class="math inline">\(\lambda\)</span> and the relevants <span class="math inline">\(\theta\)</span> and then I multiply them toguther all these (per row) and this gives me a number</p></li>
</ol>
<p>Recall: - Before I was looking for the one tag sequence for which the joint probability was maximum</p>
<p>Now: - I am marginalizing out, now we are summing all the alternative probabilities. So all the alternative ways I summed them up (so in each columns beecause here we have different assignment of tags) and the result of summing along the columns (which then we have like a vector of sequence L, so the sentence lenght) and now we sum this vector( so each element in this vector will correspond to one word with all the classes summed over) is the probability of the sequence aka the sentence regardless of tag sequences because we wa</p>
<center>
<img src="imgs/2023-11-15-17-43-44.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>The algorithms that takes care of this is called the Forward Algorithm where we are interested in find the prob of the sentence regardless of their POS tags</p>
<center>
<img src="imgs/2023-11-15-18-09-07.png" class="w550 img-fluid">
</center>
<pre></pre>
</section>
<section id="in-short" class="level3" data-number="14.3">
<h3 data-number="14.3" class="anchored" data-anchor-id="in-short"><span class="header-section-number">14.3</span> In short:</h3>
<ul>
<li>You are asked what is the most probable tag sequence: Viterbi</li>
<li>Probability of the sentence regardless of their POS tags: Forward</li>
</ul>
<p>Side note: the thing that unites these two algorithm is called Value Recursion explained in the video</p>
</section>
</section>
<section id="coming-back-to-the-yt-video-value-recursion-for-hmms" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="coming-back-to-the-yt-video-value-recursion-for-hmms"><span class="header-section-number">15</span> Coming back to the YT video: Value recursion for HMMs</h2>
<ul>
<li>Formula for HMM:</li>
</ul>
<center>
<img src="imgs/2023-11-15-20-29-57.png" class="w350 img-fluid">
</center>
<pre></pre>
<pre></pre>
<ul>
<li>Here <span class="math inline">\(P_{XY}()\)</span> is the join probability of words and tags</li>
</ul>
<center>
<img src="imgs/2023-11-15-20-35-16.png" class="w150 img-fluid">
</center>
<pre></pre>
<!-- - Tags are in row 
- Words are in column -->
<p>In reality though you do not know the tag sequence and you only have a word sequence which you assume it was generated with an HMM generative history. With this you are interested in predicting:</p>
<ol type="1">
<li>What the most probale sequence could have been that is the task of post tagging</li>
</ol>
<ul>
<li>Here we look at the tag sequence that maximizes the posterior probability for a given input w</li>
</ul>
<ol start="2" type="1">
<li>You may be interested for a language model and what you want to do is evaluate the marginal probability of the word sequence, that is the total probability regardless of what tag sequence may have generated this text. There you would asses the join probaility for each and every configuration from the tag sequence and sum all those probabilities toguether.</li>
</ol>
<!-- <center>![](imgs/2023-11-15-20-45-26.png){.w550}</center><pre></pre> -->
<center>
<img src="imgs/2023-11-15-20-46-43.png" class="w550 img-fluid">
</center>
<pre></pre>
<section id="the-value-recursion" class="level3" data-number="15.1">
<h3 data-number="15.1" class="anchored" data-anchor-id="the-value-recursion"><span class="header-section-number">15.1</span> The Value recursion</h3>
<section id="forward-algorithm" class="level4" data-number="15.1.1">
<h4 data-number="15.1.1" class="anchored" data-anchor-id="forward-algorithm"><span class="header-section-number">15.1.1</span> Forward Algorithm</h4>
<ul>
<li>Imagine you have a sequence c1 trhough c_i which are the tags for sequence w1 throuhg w_i, where w1=the and so on.</li>
</ul>
<p>Alpha would be the marginal probability of all sequences that end with the assigment <span class="math inline">\(c_{i=j}\)</span>. That is making a choice for the ith position and that choice being in the jth tag position. i.e.&nbsp;look at <span class="math inline">\(\alpha(i=3, j=B)\)</span> (recall i=word_position, j+tag_class). Then we are taking about c1, c2 and c3=B, C1 having generated ‘the’</p>
<p>We marginalize C1 and C2, which means we trying all the posibilities:</p>
<ul>
<li>A A B</li>
<li>A B B</li>
<li>B A B</li>
<li>B B B</li>
</ul>
<p>We try all these posibilitis, we asses the joint probabilities, so the whole row of multiplications per each combination so for instance in the first one we would have:</p>
<ul>
<li>A A B</li>
</ul>
<center>
<img src="imgs/2023-11-15-21-04-33.png" class="w250 img-fluid">
</center>
<pre></pre>
<p>A multiplication of all the parameters with classes A A B. Then for the second combination A B B and so on</p>
<p>Once we have calcualted all these combination we add them all toguether and thdt quantity which is the marginal probability is what we store in the function <span class="math inline">\(\alpha (i,j)\)</span></p>
<p>Now with this <span class="math inline">\(\alpha (3,B)\)</span> we can expressed in in therm os <span class="math inline">\(\alpha (2,A)\)</span> and <span class="math inline">\(\alpha (2,B)\)</span> and so on.</p>
<center>
<img src="imgs/2023-11-15-21-54-02.png" class="w550 img-fluid">
</center>
<pre></pre>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Recaping Forward Algorithm
</div>
</div>
<div class="callout-body-container callout-body">
<p>The forward recursing boilds down to <span class="math inline">\(\alpha (i,j)\)</span> which is the marginal probability of the assigment where we have generated all the wrods all they way until the ith, the ith word is tagget with the jth tag in the tagset and the sequence up unil that variable has been marginalized</p>
</div>
</div>
<p>When i is more than one then we have a recursive call to alpha but evaluated in the previous position. So escentially to evaluate alpha(i,j) we check for all candidate tags that could be set in the previous psoition with the fixed word and we asses alpha as if that tag was indeed the tag assign to the previous position and then we transition ot the jth tag and we omit the ith word.</p>
<p>alpha(4, ESO) is the probability that we are looking for because it evaluates the joint probabilities of all sequences and adds them toguether</p>
<center>
<img src="imgs/2023-11-15-22-10-42.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>Complete Forward algorithm:</p>
<center>
<img src="imgs/2023-11-15-22-15-45.png" class="w550 img-fluid">
</center>
<pre></pre>
</section>
</section>
<section id="viterbi-algorithm" class="level3" data-number="15.2">
<h3 data-number="15.2" class="anchored" data-anchor-id="viterbi-algorithm"><span class="header-section-number">15.2</span> Viterbi Algorithm</h3>
<p>Now we want to maximize our choice, we want the probability of the best sequence ending in (c_i=j, w_i=w_i)</p>
<p>The probability of the sequence that ends in c_i=c_j and emits the ith words from the jth tag is either the probability of the transition emission probability pair when i=j, or when is the second term of the alpha function.</p>
<center>
<img src="imgs/2023-11-15-22-23-33.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>So now the best sequence that ends up in the sequence (C_2=A, W_2=nice). Suppose for a moment that going to A is the best choice/the one that maximizes, then to get to alpha(2,A) = alpha(1,A) * <span class="math inline">\(\lambda_A^{A} \theta_{nice}^{A}\)</span> is the biggest among the other path posibility so the path below (aplha(1,B)*…)</p>
<p>Now for alpha(2,B) the best value is by going from alpha(1,A)*<span class="math inline">\(\lambda_A^{B} \theta_{nice}^{A}\)</span></p>
<center>
<img src="imgs/2023-11-15-22-37-09.png" class="w550 img-fluid">
</center>
<pre></pre>
<p>So if you implement a table of alpha(i,j) values and another table of Backpointers, then you have everything that you need to compute the probability (the sums of probs from the first table) and to compute the tag sequence (from the Backpointers table) and thus outputs the best tag sequence to the user.</p>
</section>
<section id="notes-on-these-algorithms" class="level3" data-number="15.3">
<h3 data-number="15.3" class="anchored" data-anchor-id="notes-on-these-algorithms"><span class="header-section-number">15.3</span> Notes on these algorithms</h3>
<p>To solve all htse computation we want to do it in a logarithm scale because logs convert products into sums leading to more numerically stable computations. For that we need to do some extra mods to the alpha function. With these three forumlas in red I can rewrite alpha into somthing called value recursion</p>
<center>
<img src="imgs/2023-11-15-22-46-19.png" class="w550 img-fluid">
</center>
<pre></pre>
<!-- <center>![Slide 47](imgs/page_47.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 48](imgs/page_48.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 49](imgs/page_49.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 50](imgs/page_50.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 51](imgs/page_51.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 52](imgs/page_52.jpeg){.w575}</center><pre></pre> -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_53.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 53</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_54.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 54</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>An HMM tagger would be a joint distribution over both tag sequences and token sequences but if you are not interested in assigninig probabilities to words or generating words then you may model with a different technique</p>
<p>So now the goal is not a generative model that you could use for taggins or for Language Modelling, now the goal is just to have a tagger.</p>
<p>Other types of taggers may fit in another tools. For instance see next slide:</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_55.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 55</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>There is still applications of this things, they tipically power systems for information extraction for question answering. So it is a form of labelling task with a bit of a smactic twist, I am not interest in the syntatic category of a word in its context but rather I am interest in recognizing mentions to something that is an entity in the real word thats why we call it Name-Entity. For instance ‘America Airlines’ we tagged with some Name entity.</p>
<ul>
<li>Why this would be usefull you may ask? suppose you are dealing with question-answering then perhaps by doing name-entity recognition you find the spans of text for which you likely have a wikipedia page or an entry in some knowledge base.</li>
</ul>
<p>The idea is to identify the blue spans, maybe it does not look like labelling, like it looks quite different from Speach tagging but now lets look at the nxt slide:</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_56.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 56</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>It is a transformation of the dataset, so the dataset has been prepoceed slighly and now it is a labelling task</p>
<ul>
<li>I: inside of span</li>
<li>B: beginnig</li>
<li>O: Outside</li>
<li>S: Single token span i.e S-LOC single location</li>
</ul>
<p>Even though is a sequence labelling task we have one label per token in the token sequence, we are construvting little brackets, because now we are setting the inside of a tag or the end of a tag or the beginnig and so on.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_57.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 57</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_58.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 58</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>For example they all have the same semantical meaning but different realizations, so the sentences have in common the same meaning but expressed in a different way</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_59.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 59</figcaption>
</figure>
</div>
</center>
<pre></pre>
<!-- There is various roles that you can place to a sentnce with respect to a verb for instnace -->
<p>Prototypical Semantic Roles</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_60.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 60</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Annother sequence labelling task: see next slide</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_61.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 61</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here you assing semantic roles to spans in sentences so now you think of a sentence that is specific to a VERB, so given the semantics of BROKE for i.e then Jhon is to be labeld as the AGENT and the window acts as a span of THEME</p>
<p>So the sentences from 3 to 5 are different with little change. Where would this information be? it must be in the lexicon it something about Jhon and something about being a rock that makes one thing the agent and another the instrument, so it is not in the syntax of the sentence it is not in the grammar it is really in the selection of preferences of VERBS and the attributes of NOUNS</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_62.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 62</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In these slides are two verbs in their first senses</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_63.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 63</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here if I have 7 verbs then I have sevent semantic role sequences I have: arg0… arg1, till arg6 .In POS tagging their is one sequence for one input sequence. Here is a bit different, here for every verb you have one sequence, because foe every verb you look at the sentence and you can interpret who is the agent, who is doing what to who, but if you focus in a different VERB in a the same sentence then different spans of texts will play different roles. So for every single verb you encounter in the sentence there would be a corresponding tag sequence i.e seeing at the columns on the table #1, #2 … #7. These are the tag sequences for each one of the verbs</p>
<p>Imagine you are designinig a semantic role labeller and you are given a sequence of inputs and there are two settings one settings is somebody tells me please tag the sequence for the verb ‘implement’ and then your model would ideally ouput something like the last column</p>
<p>Or I give you the sentence and I say tag the sentence for semantic roles for the verb ‘lighten up’ and then you would predict soemthin similar like in column number 3</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_64.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 64</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The scope is now to map a sequence of words to a sequence of tags but doing that with respect to a given position because for each tth meanining the verb of interest i.e ‘use’ then wrt to that position this is the output tag sequence</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_65.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 65</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>If we can express a task as annotating tokens in sequence and for every token I have tag, then normally this tags comes from a finite set. That means that an HMM is available. You could develop an HMM for psot tagging, you could develop an HMM for Nameed-Entity recognition. For isntance for pos tagging you change the tag space and maybe you motivate one or the other variant of IOBES and you could do HMMs, you could use it. A not soo god example is SLR because you cannot phrase as every token gets one tag, it is more like every token gets a tag for a certain verb. So if I change the verb then I have another tag sequence so you have a variable number of outputs sequences, one ouput sequence per verb in the token sequence. This lead us to the conlsuion of SLR are note good candidates for HMMs.</p>
<p>Verbs –&gt; SRL</p>
<p>However, even in the case that HMMs is a good choice like in post tagging and named-entity recognition it can be argued that it maynot be a choice in general because I know where I am applying it. I am applying it for the purpose of labelling sequences with some linguistic signals and if I am not interest in Language Modelling I do not care about assigning probs to the text. Thus we dont care about probs then there may be better techniques.</p>
</section>
</section>
<section id="limitation-of-the-hmm" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="limitation-of-the-hmm"><span class="header-section-number">16</span> Limitation of the HMM</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_66.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 66</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ol type="1">
<li>Cannot model long term dependencies, it is very local. Token are not even depent on one another they are conditionally independent given the tags where they were generated from</li>
<li>The context is only of the previous tags it does not use linguistics context it use some sintatic context but very limited one just the previous tags</li>
<li>As a tagger you are given good english and you are mapping to tag sequences and what happens is that if the tag for a certain position depends on a word that is very far away from it then the HMM cannot see. For instance some name entities may have long names for instance United States Airles, or people with long names. Conclusion HMMS cannot reduce the entrophy of what the label of a certain category may be you need to look far and the HMMs cannot do that</li>
</ol>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_67.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 67</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Because the HMMs need to generate text they use very farily limited use of linguistic context in <span class="math inline">\(w_{1:L}\)</span>. Using more of thse context in a generative model it will break certain algorithms that you need for training and using these models like the vitori or forward algorithm</p>
<p>So to allow a category to interact with more words you would break those algorithms you would make then not scalable.</p>
<p>It will also make our CPDs more sparce if we try to memorize more phrases everything gets sparce</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_68.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 68</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Unseen wors and phrases will come out and the HMM would be lost</p>
<p>It may also be cases that we would like to extract the fine features of a word fors instance ending in ‘-ed’ or stating with ‘un’ and so on but this cannot be by the HMM because it cannot analyse more fine-grained features.</p>
</section>
<section id="how-to-move-from-the-hmms" class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="how-to-move-from-the-hmms"><span class="header-section-number">17</span> How to move from the HMMs?</h2>
<section id="first-idea-use-feature-rich-models" class="level3" data-number="17.1">
<h3 data-number="17.1" class="anchored" data-anchor-id="first-idea-use-feature-rich-models"><span class="header-section-number">17.1</span> First idea: use feature-rich models</h3>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_69.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 69</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Let’s imagine I want to tag the 4th word. I could collect features from the surronding context of that position so I could get features from a windows to the left or a window from the right and features from the word cute itself. In the features I have this vector where I only have one <span class="math inline">\(1\)</span>, I could have design more features as: ‘is there capital letters’, ‘does it end in ed?’ and so on more features.</p>
<p>This table is bigger in the linguistic context so its more rich than the HMM These are handmade feature vectors</p>
</section>
<section id="second-ideadesing-one-classifier-and-use-it-many-times" class="level3" data-number="17.2">
<h3 data-number="17.2" class="anchored" data-anchor-id="second-ideadesing-one-classifier-and-use-it-many-times"><span class="header-section-number">17.2</span> Second idea:desing one classifier and use it many times</h3>
<blockquote class="blockquote">
<p>Here we map the features of the context to the prob values of the clases using a log linear model</p>
</blockquote>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_70.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 70</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>From the feature vector ‘cute’ predict a vector of probability values as large as the number of classes that adds up to one. Lets explain: if you have a feature representation of a context and your goal is to spit out a prob vector, then you could use a linear model.</p>
<p>So from however many features you have, you do a linear operation to obtain exaclty the number of classe you are working withs. For sintance here 11 clases and I am working with 3V features, then I do one linear transformation from 3V features to 11. This vector with 11 cordinates in it, these are not probabilities are real values to force them to become prob we use Softmax. K=11 calsses, D is the dimensionality of the feature vector, if we are using the feature vector from the previous slide that is 3V. So then the number of weights would be 3V * K. Because for every feature you wanna get the importance of the feature towards a class. And also you get the biases which you could think of as the margin frequency of the class regardless of any features. One linear transform maps from D dimensions to K dimensions this thigns are called ‘scores’, ‘logits’. Logits can be though of log probabilites that are not normalized and then the softmax functions maps to K probs.</p>
<p>What we achieve by this is that no matter with position you are you can use the same model, it does not get bigger or smaller to tag the first word, the second, the … its just one classifier that can be sued over and over.</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_71.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 71</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>If I have a sentence from <span class="math inline">\(w_{1:l}\)</span> and I am looking at an specific position of it with those two things I can get the prob values for the classes that I may classify. That is the <span class="math inline">\(f\)</span> the prob vector, one prob per each class. Yuo decide how to decide <span class="math inline">\(f\)</span></p>
<!-- <center>![Slide 72](imgs/page_72.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 73](imgs/page_73.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 74](imgs/page_74.jpeg){.w575}</center><pre></pre> -->
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_75.jpeg" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 75</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We have created a tagger by not considering the sequence but only about classification. The idea is that for every position in the sequence that you would want to tag you pretend this is a classification task you dont care that you are actually tagging and entire sequence. You classify one position at the time independently of what you do for other positions. The key for this to work is to featurize the context in which you perfom the clasifications so if I want to classify ‘united’ I have a feature vector that describes best I can ‘united’ in the sentence meaning I am ‘united’ there is adimension for that. There is a feature for ‘my neighboor is an upper case letter’.</p>
<ul>
<li><p>If you do independent classification, so independetly of what you do in other steps do you see a problem in the picture above where I have annotated where is the beginning, inside and end of the span (meaning there is an structure)? If I perform independent decision it is easy to programm but my lead to a problem. The problem is:</p></li>
<li><p>You get nonsense tag sequences: [O I O ] here you cannot be inside if you have not even enter one<br>
The problem with tagging independetly then is that tagging is ins’t made of independent stps and specficially the structure that you want to output it contains constraints so the ouput sequence is constraint not any scramble of tags would do. A better one would be [O S I O] as in otside, start, inside, outside</p></li>
</ul>
<!-- <center>![Slide 76](imgs/page_76.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 77](imgs/page_77.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 78](imgs/page_78.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 79](imgs/page_79.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 80](imgs/page_80.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 81](imgs/page_81.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 82](imgs/page_82.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 83](imgs/page_83.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 84](imgs/page_84.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 85](imgs/page_85.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 86](imgs/page_86.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 87](imgs/page_87.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 88](imgs/page_88.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 89](imgs/page_89.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 90](imgs/page_90.jpeg){.w575}</center><pre></pre> -->
<!-- <center>![Slide 91](imgs/page_91.jpeg){.w575}</center><pre></pre> -->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.danilotpnta\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about/index.html">
<p><span class="footerDaniloToapanta">© 2024 Danilo Toapanta</span></p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../privacy/index.html">
<p>Privacy</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../contact/index.html">
<p>Contact</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license/index.html">
<p>License</p>
</a>
  </li>  
</ul>
    <div class="toc-actions"><ul><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/blob/main/blog/2023-11-13_sequence-labelling/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/edit/main/blog/2023-11-13_sequence-labelling/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/danilotpnta/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/DaniloToapnta">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../docs/sitemap.xml">
      <i class="bi bi-rss-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>

    let navbar = document.getElementsByClassName("navbar-nav")[0]    

    let li2 = document.createElement("li");
    li2.className = "nav-item compact";

    let a2 = document.createElement("a");
    a2.className = "nav-link quarto-color-scheme-toggle";
    a2.style.cursor = "pointer"
    li2.appendChild(a2)

    let i2 = document.createElement("i");
    i2.className = "bi bi-moon"
    a2.append(i2)

    navbar.appendChild(li2);

    i2.onclick = function() {
        window.quartoToggleColorScheme(); return false;
    }
    // <a href="http://localhost:4200/about/" class="quarto-color-scheme-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>

    let li = document.createElement("li");
    li.className = "nav-item compact";

    let a = document.createElement("a");
    a.className = "nav-link";
    a.style.cursor = "pointer"
    li.appendChild(a)

    let i = document.createElement("i");
    i.className = "bi bi-search"
    a.append(i)

    // let span = document.createElement("span");
    // span.className = "menu-text"
    // a.append(span)

    navbar.appendChild(li);

    a.onclick = function() {
        window.quartoOpenSearch()
    }


</script>






</body></html>