<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Danilo Toapanta">
<meta name="dcterms.date" content="2023-11-20">
<meta name="description" content="Description of this Post">

<title>Deep Learning Optimizations II – Danilo Toapanta</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/danilo.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="../../site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet">
<!-- <link href="../../site_libs/quarto-contrib/material-icons-0.14.2/mi.css" rel="stylesheet"> -->
<script>
window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../css/index-posts.css">
</head>

<body class="floating nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <div id="quarto-announcement" data-announcement-id="af66dd50c39dd2ed9de488e10a192054" class="alert alert-primary hidden"><i class="bi bi-info-circle quarto-announcement-icon"></i><div class="quarto-announcement-content">
<p><strong>Let op</strong> - This website is undergoing scheduled maintenance</p>
</div></div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title"><span id="danilo_topanta_brand"> Danilo Toapanta</span> <a id="mysite" class="mysite" href="../../site-ver-hist/">v1.2</a></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text"><span id="home-welcome-msg">Home</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/danilotpnta?tab=repositories" target="_blank"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full blog-page" style="display: none !important;">
    <div class="quarto-title-banner page-columns page-full">
        <div class="quarto-title column-body">
            <h1 class="title">Deep Learning Optimizations II</h1>
                
            <!-- Description Block -->
                        <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
                        
            <!-- Categories Block -->
                                            <div class="quarto-categories">

                    <!-- Display Categories -->
                                            <div class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=Deep Learning">
                                Deep Learning
                            </a>
                        </div> 
                                            <div class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div> 
                    
                    <!-- Display Tags if any -->
                                    </div>
                            
        </div>
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">November 20, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    
</header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">
<script>
    var currentUrl = window.location.href;
    var index_init_post = currentUrl.lastIndexOf("/20");
    var string_init_post= currentUrl.slice(index_init_post, index_init_post+3 );

    // console.log("currentUrl: " + currentUrl);
    // console.log("index: " + index_init_post);
    // console.log("string: " + string_init_post);

    // If is equal to /blog/20... then make navbar title READING MODE
    if (string_init_post === "/20"){
        let mysite = document.getElementById("mysite");
        mysite.classList.add("mysite-change");

        let navbar = document.getElementById("danilo_topanta_brand");
        navbar.classList.add("navbar-brand-change");

        // This will render a new title saying READING DANILOS BLOG
        // navbar.innerHTML = 'You are Reading Danilo\'s Blog<span style="font-size:35px; vertical-align: middle; opacity: 0.65; padding-bottom: 6px; padding-left: 14px;" class="material-icons-round"> auto_awesome </span>';
        
        const smallDevice = window.matchMedia("(min-width: 570px)");
        smallDevice.addListener(handleDeviceChange);

        function handleDeviceChange(mediaQuery) {
            if (mediaQuery.matches) {
                navbar.innerHTML = "";
                // navbar.innerHTML = "<-- You are Reading Danilo's Blog -->";
            } else  {
                navbar.innerHTML = "Danilo Toapanta";
            }
        }

        // Run it initially
        handleDeviceChange(smallDevice);

        let link = document.getElementsByClassName("navbar-brand")[0];
        link.classList.add("disablePointerEvents");

        let brand_container = document.getElementsByClassName("navbar-brand-container")[0];
        brand_container.classList.add("navbar-brand-container-new-padding");

    }
</script>


<!-- <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Running my first Marathon</h1>
                  <div>
        <div class="description">
          I will be running at the 42km TCS Amsterdam 2023, 15th October
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">News</div>
              </div>
                  </div>
  </div> -->

  <!-- ---
  coming-soon: true
  tags: [Spanish]
  --- -->








<main id="title-block-header" class="quarto-title-block default page-columns page-full" style="padding-bottom: 40px;">

    <div class="quarto-title column-body" style="margin-bottom: 1em;">
        <h1 class="title" style="padding-bottom:8px" ;="">Deep Learning Optimizations II</h1>
        
        <!-- Description Block -->
                    <div>
                <div class="description">
                    Description of this Post
                </div>
            </div>
        
        <!-- Categories Block -->
                    
                <!-- Display Categories -->
                <div class="quarto-categories">
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title">
                        <i class="fa-solid fa-hashtag" ></i> Categories:
                    </div> -->

                                            <div id="All" class="quarto-category">
                            <a href="../../blog/#category=All">
                                All
                            </a>
                        </div>
                                            <div id="Deep Learning" class="quarto-category">
                            <a href="../../blog/#category=Deep Learning">
                                Deep Learning
                            </a>
                        </div>
                                            <div id="TAGS" class="quarto-category">
                            <a href="../../blog/#category=TAGS">
                                TAGS
                            </a>
                        </div>
                                    </div>
                


                <div class="quarto-categories tag-categories">
                    
                    <!-- Tags Icon  -->
                    <!-- <div id="tag-icon-blog" class="quarto-category tags-title"> -->
                        <!-- <i class="fa-solid fa-tag" ></i> Tags: -->
                        <!-- <i class="fa-solid fa-hashtag" ></i> Tags: -->
                        <!-- <span class="material-icons-outlined" >local_offer</span> Tags: -->
                        <!-- / -->
                    <!-- </div> -->

                    <!-- Display Tags -->
                                            <div id="All-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=All">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                All
                            </a>
                        </div>
                                            <div id="Deep Learning-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=Deep Learning">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                Deep Learning
                            </a>
                        </div>
                                            <div id="TAGS-from-here" class="quarto-category tags-title" style="display: none;">
                            <a href="../../blog/#category=TAGS">
                               <!-- <span class="hasth-tag">
                                #
                                </span> -->
                                TAGS
                            </a>
                        </div>
                    
                    
                </div>

                    
    </div>


    
    <div class="quarto-title-meta">

        <div>
        <div class="quarto-title-meta-heading">Author</div>
        <div class="quarto-title-meta-contents">
                 <p><a href="https://danilotpnta.github.io/">Danilo Toapanta</a> </p>
              </div>
      </div>
        
        <div>
        <div class="quarto-title-meta-heading">Published</div>
        <div class="quarto-title-meta-contents">
          <p class="date">November 20, 2023</p>
        </div>
      </div>
      
        
      </div>
      

    <!-- Current link: Font-awesome, Google icons, Bootstrap icons -->
    
</main>


<!-- ## Title -->
<!-- <center>![Slide 1](imgs/page_1.png){.w575}</center><pre></pre> -->
<!-- ## Last time: -->
<!-- <center>![Slide 2](imgs/page_2.png){.w575}</center><pre></pre> -->
<!-- ## Lecture overview
<center>![Slide 3](imgs/page_3.png){.w575}</center><pre></pre> -->
<section id="lecture-overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="lecture-overview"><span class="header-section-number">1</span> Lecture overview</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_4.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 4</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="start-with" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="start-with"><span class="header-section-number">2</span> Start with</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_5.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 5</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="quiz" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="quiz"><span class="header-section-number">3</span> Quiz</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_6.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 6</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Why left would be preferred?</p>
<ul>
<li>simpler model</li>
<li>generalize better to unseen data</li>
</ul>
<p>Why the right is better?</p>
<ul>
<li>We are actually fitting all the data points</li>
<li>The right hand side is the ground truth</li>
<li>You dont care about extrapolating</li>
</ul>
</section>
<section id="digression-gravitation" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="digression-gravitation"><span class="header-section-number">4</span> Digression: Gravitation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_7.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 7</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="bias-variance-tradeoff" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="bias-variance-tradeoff"><span class="header-section-number">5</span> Bias-variance tradeoff</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_8.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 8</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The single best prediction of the parameters</p>
<p>A good estimator is a function whose output is close to the true underlying thetha that generated the data</p>
</section>
<section id="bias-variance-tradeoff-1" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="bias-variance-tradeoff-1"><span class="header-section-number">6</span> Bias-variance tradeoff</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_9.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 9</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p><strong>Bias</strong></p>
<p>Estimator’s expected value= so the ouput of our thetha estimate and the true value of that parameter</p>
<p>Bias comes from not being able to model the real model in a correct way</p>
<p><strong>Variance</strong></p>
<p>So if you use a different training set or a different split how different are the learnt NNs parameters. If differ a lot then high variance</p>
</section>
<section id="bias-variance-tradeoff-2" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="bias-variance-tradeoff-2"><span class="header-section-number">7</span> Bias-variance tradeoff</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_10.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 10</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>High variance &amp; low bias, your model on average get it right but it has a high variance because it wants to model all noise, so overfilling, it spread all over the place</p>
<p>High bias and low variance, in average it does not even get it correct so underfitting</p>
</section>
<section id="overfitting" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="overfitting"><span class="header-section-number">8</span> Overfitting</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_11.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 11</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="overfitting-1" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="overfitting-1"><span class="header-section-number">9</span> Overfitting</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_12.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 12</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>It will learn to recognize that the img contains hourses not by the hourses but the watermark. This is seen in the heat-map where the point of attention is in the watermark. This is overfitting</p>
</section>
<section id="overfitting-2" class="level2" data-number="10">
<h2 data-number="10" class="anchored" data-anchor-id="overfitting-2"><span class="header-section-number">10</span> Overfitting</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_13.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 13</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>To avoid overfitting we use <strong>regularization</strong></p>
</section>
<section id="overfitting-how-powerful-are-neural-networks" class="level2" data-number="11">
<h2 data-number="11" class="anchored" data-anchor-id="overfitting-how-powerful-are-neural-networks"><span class="header-section-number">11</span> Overfitting: how “powerful” are neural networks?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_14.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 14</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="this-isnt-the-full-story.." class="level2" data-number="12">
<h2 data-number="12" class="anchored" data-anchor-id="this-isnt-the-full-story.."><span class="header-section-number">12</span> This isn’t the full story..</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_15.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 15</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="update-2019-double-descent" class="level2" data-number="13">
<h2 data-number="13" class="anchored" data-anchor-id="update-2019-double-descent"><span class="header-section-number">13</span> Update 2019: Double Descent</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_16.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 16</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Dash line is where the data samples equals the number of parameters</p>
<p>So in the top right image even if we have increase the number of hidden layers and our training error is zero then our test set is still decreasing that is weird.</p>
<p>So here we then presume that bigger model will have lower error.</p>
<p>Before we use to have a curve upwrads from the tipical bias/variance curve in the prev slide, but the weird thing is that where the dash line meets then this error starts to decrease again.</p>
<p>Two answers: smoothness and regularization</p>
</section>
<section id="double-descent-smoothness-from-bigger-models" class="level2" data-number="14">
<h2 data-number="14" class="anchored" data-anchor-id="double-descent-smoothness-from-bigger-models"><span class="header-section-number">14</span> Double-descent: Smoothness from bigger models</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_17.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 17</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In the x axis you have the number of times SGD was proceed, so the amount of units of texts akak ‘words’ were processed</p>
<p>We can see that with larger models I require fewer samples to reach a lower test loss</p>
<p>Also when they reach stability they can provide with a test loss which is the best shot they can give and this depends on their size, larger models give more accurate results</p>
<p>Also it shows that the quicker models learns quickler than the smaller model. So while the smaller model for a given number of tokens the large model learns more quickler.</p>
<p>This is Language models still not applicable for vision.</p>
<p>We also can say that (in language models) in terms of flops is more efficient to learn large models for fewer steps than to learn small models for larger amount of steps</p>
</section>
<section id="double-descent-in-practice" class="level2" data-number="15">
<h2 data-number="15" class="anchored" data-anchor-id="double-descent-in-practice"><span class="header-section-number">15</span> Double-descent in practice?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_18.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 18</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>In practice if you increase the number of neurons you may be closer to overfitting. For that we need regularization</p>
</section>
<section id="regularization" class="level2" data-number="16">
<h2 data-number="16" class="anchored" data-anchor-id="regularization"><span class="header-section-number">16</span> Regularization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_19.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 19</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here we reduce the complexity of a NN and avoid ovverfitting</p>
</section>
<section id="l2-regularization" class="level2" data-number="17">
<h2 data-number="17" class="anchored" data-anchor-id="l2-regularization"><span class="header-section-number">17</span> 1) L2-regularization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_20.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 20</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Referred to weight decay or Ritch regression in the linear case</p>
<p>Omega is proportional to how large the weights are</p>
<p>Minimizing this is also the same as if you assume a Gaussian prior on your weight, here you assume the weights are Gaussian distributed</p>
</section>
<section id="l2-regularization-1" class="level2" data-number="18">
<h2 data-number="18" class="anchored" data-anchor-id="l2-regularization-1"><span class="header-section-number">18</span> 1) L2-regularization</h2>
<section id="l2-l1-formula" class="level3" data-number="18.1">
<h3 data-number="18.1" class="anchored" data-anchor-id="l2-l1-formula"><span class="header-section-number">18.1</span> L2 &amp; L1 Formula</h3>
<center>
<img src="imgs/2023-11-21-00-30-48.png" class="w250 img-fluid">
</center>
<pre></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
L2 example Python
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>The L2 loss, also known as the Euclidean loss or Mean Squared Error (MSE), is a common loss function used in regression problems. It measures the average squared difference between the predicted values and the actual values.</p>
<p>The formula for L2 loss is given by:</p>
<p><span class="math inline">\(L2\_loss = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2\)</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N\)</span> is the number of data points.</li>
<li><span class="math inline">\(y_i\)</span> is the true (ground truth) value for the i-th data point.</li>
<li><span class="math inline">\(\hat{y}_i\)</span> is the predicted value for the i-th data point.</li>
</ul>
<p>Here’s a simple example in Python using NumPy:</p>
<div id="449260e3" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some example data</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>true_values <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">5</span>])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>predicted_values <span class="op">=</span> np.array([<span class="fl">1.5</span>, <span class="fl">3.5</span>, <span class="fl">4.8</span>, <span class="fl">4.2</span>, <span class="fl">5.2</span>])</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate L2 loss</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>l2_loss <span class="op">=</span> np.mean((true_values <span class="op">-</span> predicted_values)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"L2 Loss:"</span>, l2_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L2 Loss: 0.12400000000000003</code></pre>
</div>
</div>
<p>The L2 loss is calculated by taking the mean of the squared differences between the true and predicted values. The smaller the L2 loss, the better the model’s predictions align with the true values.</p>
</div>
</div>
</div>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_21.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 21</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Lamda tells you how important the regularization is, if:</p>
<ul>
<li>Lambda high then puts all the weights towards zero</li>
<li>Lambda is zero, then it learns the typical loss and do not care about regularization</li>
</ul>
<p>Lambda during trainnig is fixed, if you want to find the best one you need to run lots of experiments</p>
</section>
</section>
<section id="l1-regularization" class="level2" data-number="19">
<h2 data-number="19" class="anchored" data-anchor-id="l1-regularization"><span class="header-section-number">19</span> 2) L1-regularization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_22.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 22</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Isotropics means is equal in all directions</p>
<p>Now if you take the gradient of this, now if you take derivative of w, because you want to optimize then it end ups a 1/2. So you end up with a constant factor in the loss that keeps getting substracted. So what happens is that we end up substracting a constant to our weights.</p>
<p>So it substract a bit from the positive weights and adds a bit to the negative weights and pushed them towards zero</p>
</section>
<section id="l1-regularization-1" class="level2" data-number="20">
<h2 data-number="20" class="anchored" data-anchor-id="l1-regularization-1"><span class="header-section-number">20</span> 2) L1-regularization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_23.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 23</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>L1 leads to sparce weights, so that means more weights will be closer to zero</p>
<p>If alpha here increases then that means weights become zero</p>
</section>
<section id="why-do-l1-and-l2-regularizations-work" class="level2" data-number="21">
<h2 data-number="21" class="anchored" data-anchor-id="why-do-l1-and-l2-regularizations-work"><span class="header-section-number">21</span> Why do L1 and L2 Regularizations work?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_24.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 24</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>L2 regularization has basically a circular constraint area because you have w1^2 + w2^2 needs to be constant so all these combinations would be a circle</p>
<p>So the contours of the loss function in red will intercept the constrains regions at an axis. What that means is that if you are trying to find in this case the optimal loss, then it touches the constraint region where one of the values is set to zero while with L2 regularization there is no particular point where you could make one of the weights zero</p>
<p>This is because it needs to have the sum of the squares to be a small value but there is no particular motivation to have a similar weigth dimension to be equal to zero, so there is no reason to have sparse weights in L2</p>
</section>
<section id="effect-linear-regression-example" class="level2" data-number="22">
<h2 data-number="22" class="anchored" data-anchor-id="effect-linear-regression-example"><span class="header-section-number">22</span> Effect: linear regression example</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_25.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 25</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here the alpha goes from strong to weak</p>
<p>In L1 it puches some weights to zero and then stay zero aftwer a while and it is not like all of them get smaller at the same time but some of them stay quite high for a loong time, and now increasing alpha here you are making individual weights close to zero.</p>
<p><strong>L2 Regularization (Weight Decay):</strong> Encourages smaller weights but does not force them to be exactly zero. It smoothens the weights but doesn’t induce sparsity.</p>
<p><strong>L1 Regularization:</strong> Promotes sparsity by adding a penalty term based on the absolute values of the weights. This can lead to some weights being exactly zero.</p>
</section>
<section id="early-stopping" class="level2" data-number="23">
<h2 data-number="23" class="anchored" data-anchor-id="early-stopping"><span class="header-section-number">23</span> 3) Early stopping</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_26.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 26</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="early-stopping-1" class="level2" data-number="24">
<h2 data-number="24" class="anchored" data-anchor-id="early-stopping-1"><span class="header-section-number">24</span> 3) Early stopping</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_27.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 27</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>*Typo: with better <strong>test</strong> set error</p>
<p>The model at this stage have low variance because they are not overfitting</p>
</section>
<section id="early-stopping-2" class="level2" data-number="25">
<h2 data-number="25" class="anchored" data-anchor-id="early-stopping-2"><span class="header-section-number">25</span> 3) Early stopping</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_28.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 28</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="why-does-early-stopping-work-as-regularization" class="level2" data-number="26">
<h2 data-number="26" class="anchored" data-anchor-id="why-does-early-stopping-work-as-regularization"><span class="header-section-number">26</span> Why does early-stopping work as regularization?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_29.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 29</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="why-does-early-stopping-work-as-regularization-1" class="level2" data-number="27">
<h2 data-number="27" class="anchored" data-anchor-id="why-does-early-stopping-work-as-regularization-1"><span class="header-section-number">27</span> Why does early-stopping work as regularization?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_30.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 30</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here weight decay they mean by L2-regularization</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Weight decay (L2-loss) vs Early Stopping
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Weight decay, also known as L2 regularization, is a technique to prevent overfitting by adding a penalty term to the loss function that is proportional to the squared magnitudes of the weights. This regularization term discourages the model from learning very large weights and encourages a smoother and more generalized solution.</p>
<p><strong>Early Stopping:</strong></p>
<p>Early stopping is a regularization technique used during the training of a machine learning model, typically in the context of iterative optimization algorithms like gradient descent. The idea behind early stopping is to monitor the model’s performance on a validation set during training and stop the training process when the performance on the validation set starts to degrade, even if the performance on the training set continues to improve.</p>
<ul>
<li><strong>Mechanism:</strong> Monitor a performance metric (e.g., validation loss) on a separate validation set during training.</li>
<li><strong>Decision Criteria:</strong> Stop training when the performance on the validation set starts to worsen or fails to improve for a certain number of consecutive epochs.</li>
<li><strong>Purpose:</strong> Prevent overfitting by terminating training before the model starts to memorize noise in the training data.</li>
</ul>
<p><strong>Weight Decay (L2 Regularization):</strong></p>
<p>Weight decay, also known as L2 regularization, is a technique to prevent overfitting by adding a penalty term to the loss function that is proportional to the squared magnitudes of the weights. This regularization term discourages the model from learning very large weights and encourages a smoother and more generalized solution.</p>
<ul>
<li><strong>Mechanism:</strong> Add a term to the loss function that penalizes large weights by adding the sum of squared weights multiplied by a regularization strength.</li>
<li><strong>Decision Criteria:</strong> No specific stopping criterion; regularization is applied throughout the training process.</li>
<li><strong>Purpose:</strong> Encourage the model to have smaller and more evenly distributed weights, preventing overfitting.</li>
</ul>
<p><strong>Key Differences:</strong></p>
<ol type="1">
<li><strong>Focus:</strong>
<ul>
<li>Early stopping focuses on monitoring the model’s performance during training and stopping when the validation performance indicates potential overfitting.</li>
<li>Weight decay focuses on adjusting the optimization objective by penalizing large weights, aiming to prevent overfitting from the beginning of training.</li>
</ul></li>
<li><strong>Decision Criteria:</strong>
<ul>
<li>Early stopping makes decisions based on the validation performance, and the training stops when the validation performance degrades.</li>
<li>Weight decay does not have a specific stopping criterion; it is a continuous regularization technique applied throughout training.</li>
</ul></li>
<li><strong>Implementation:</strong>
<ul>
<li>Early stopping involves monitoring and interrupting the training loop.</li>
<li>Weight decay involves adding a regularization term to the loss function during each iteration of the optimization algorithm.</li>
</ul></li>
</ol>
<p>In practice, these techniques can be used together to enhance the regularization effect and improve the generalization performance of a machine learning model.</p>
</div>
</div>
</div>
</section>
<section id="dropout-the-problem-it-addresses" class="level2" data-number="28">
<h2 data-number="28" class="anchored" data-anchor-id="dropout-the-problem-it-addresses"><span class="header-section-number">28</span> 4) Dropout: the problem it addresses</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_31.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 31</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-why-does-it-work" class="level2" data-number="29">
<h2 data-number="29" class="anchored" data-anchor-id="dropout-why-does-it-work"><span class="header-section-number">29</span> 4) Dropout: why does it work?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_32.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 32</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-why-does-it-work-1" class="level2" data-number="30">
<h2 data-number="30" class="anchored" data-anchor-id="dropout-why-does-it-work-1"><span class="header-section-number">30</span> 4) Dropout: why does it work?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_33.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 33</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-how-is-it-implemented" class="level2" data-number="31">
<h2 data-number="31" class="anchored" data-anchor-id="dropout-how-is-it-implemented"><span class="header-section-number">31</span> 4) Dropout: how is it implemented?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_34.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 34</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>You switch the activations to 0, and now say with Bernulli you have 50% neurons working</p>
<p>During testing you are not learning so you use all the neurons</p>
<ul>
<li>Now with Dropout you cannot have neurons that are inactive, because you drop all other neurons so now they need to work</li>
<li>Decreases overfitting</li>
</ul>
</section>
<section id="dropout" class="level2" data-number="32">
<h2 data-number="32" class="anchored" data-anchor-id="dropout"><span class="header-section-number">32</span> Dropout</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_35.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 35</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-1" class="level2" data-number="33">
<h2 data-number="33" class="anchored" data-anchor-id="dropout-1"><span class="header-section-number">33</span> Dropout</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_36.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 36</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-2" class="level2" data-number="34">
<h2 data-number="34" class="anchored" data-anchor-id="dropout-2"><span class="header-section-number">34</span> Dropout</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_37.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 37</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-3" class="level2" data-number="35">
<h2 data-number="35" class="anchored" data-anchor-id="dropout-3"><span class="header-section-number">35</span> Dropout</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_38.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 38</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-4" class="level2" data-number="36">
<h2 data-number="36" class="anchored" data-anchor-id="dropout-4"><span class="header-section-number">36</span> Dropout</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_39.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 39</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-5" class="level2" data-number="37">
<h2 data-number="37" class="anchored" data-anchor-id="dropout-5"><span class="header-section-number">37</span> Dropout</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_40.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 40</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="dropout-us.-bagging" class="level2" data-number="38">
<h2 data-number="38" class="anchored" data-anchor-id="dropout-us.-bagging"><span class="header-section-number">38</span> Dropout US. Bagging</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_41.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 41</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Bagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. It is train in parallel</p>
<center>
<img src="imgs/2023-11-21-11-01-40.png" class="w350 img-fluid">
</center>
<pre></pre>
<p>Bagging:</p>
<ul>
<li>Has its corresponding training set that is different from the whole training set</li>
<li>Uses all neurons</li>
</ul>
<p>Dropout:</p>
<ul>
<li>It does not employ all neurons</li>
<li>They are not trained they only get one SGD because you have many infinitely subnetworks, so if you apply dropout it is very unlikely that you trian the same subentwork many times</li>
</ul>
</section>
<section id="dropout-beyond-bagging" class="level2" data-number="39">
<h2 data-number="39" class="anchored" data-anchor-id="dropout-beyond-bagging"><span class="header-section-number">39</span> Dropout beyond Bagging</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_42.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 42</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="data-augmentation" class="level2" data-number="40">
<h2 data-number="40" class="anchored" data-anchor-id="data-augmentation"><span class="header-section-number">40</span> 5) Data augmentation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_43.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 43</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="data-augmentation-1" class="level2" data-number="41">
<h2 data-number="41" class="anchored" data-anchor-id="data-augmentation-1"><span class="header-section-number">41</span> Data augmentation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_44.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 44</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="one-note-about-backtranslation-though" class="level2" data-number="42">
<h2 data-number="42" class="anchored" data-anchor-id="one-note-about-backtranslation-though"><span class="header-section-number">42</span> One note about backtranslation though:</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_45.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 45</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="common-computer-vision-augmentations-visualised" class="level2" data-number="43">
<h2 data-number="43" class="anchored" data-anchor-id="common-computer-vision-augmentations-visualised"><span class="header-section-number">43</span> Common computer vision augmentations visualised</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_46.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 46</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="data-augmentation-2" class="level2" data-number="44">
<h2 data-number="44" class="anchored" data-anchor-id="data-augmentation-2"><span class="header-section-number">44</span> Data augmentation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_47.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 47</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="data-augmentation-3" class="level2" data-number="45">
<h2 data-number="45" class="anchored" data-anchor-id="data-augmentation-3"><span class="header-section-number">45</span> Data augmentation</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_48.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 48</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="other-regularizations" class="level2" data-number="46">
<h2 data-number="46" class="anchored" data-anchor-id="other-regularizations"><span class="header-section-number">46</span> Other regularizations</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_49.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 49</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="various-ways-to-regularise" class="level2" data-number="47">
<h2 data-number="47" class="anchored" data-anchor-id="various-ways-to-regularise"><span class="header-section-number">47</span> Various ways to regularise</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_50.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 50</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="normalization" class="level2" data-number="48">
<h2 data-number="48" class="anchored" data-anchor-id="normalization"><span class="header-section-number">48</span> Normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_51.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 51</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Putting data into a common shape without distorting tis shape</p>
</section>
<section id="data-preprocessing" class="level2" data-number="49">
<h2 data-number="49" class="anchored" data-anchor-id="data-preprocessing"><span class="header-section-number">49</span> Data preprocessing</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_52.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 52</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here basically if we have in same scale then our weights will not be elongated like in the elipse, now they would be able to take the same step size in the correct direction</p>
</section>
<section id="normalizing-input-data" class="level2" data-number="50">
<h2 data-number="50" class="anchored" data-anchor-id="normalizing-input-data"><span class="header-section-number">50</span> Normalizing Input Data</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_53.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 53</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>This we apply in the input stage:</p>
<p>Normalization is a linear operator so you can put this back into the NN after you have trained if you want to</p>
</section>
<section id="normalizing-intermediate-layers" class="level2" data-number="51">
<h2 data-number="51" class="anchored" data-anchor-id="normalizing-intermediate-layers"><span class="header-section-number">51</span> Normalizing intermediate layers</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_54.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 54</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Here we talked about the normalization within the NN</p>
</section>
<section id="batch-normalization" class="level2" data-number="52">
<h2 data-number="52" class="anchored" data-anchor-id="batch-normalization"><span class="header-section-number">52</span> Batch normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_55.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 55</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="batch-normalization-1" class="level2" data-number="53">
<h2 data-number="53" class="anchored" data-anchor-id="batch-normalization-1"><span class="header-section-number">53</span> Batch normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_56.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 56</figcaption>
</figure>
</div>
</center>
<pre></pre>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How does batch normalization works?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Batch Normalization (BatchNorm) is a technique used in neural networks to improve the training stability and speed by normalizing the inputs of each layer. It was introduced by Sergey Ioffe and Christian Szegedy in their paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”</p>
<p>Here’s a high-level overview of how Batch Normalization works:</p>
<section id="steps-of-batch-normalization" class="level3" data-number="53.1">
<h3 data-number="53.1" class="anchored" data-anchor-id="steps-of-batch-normalization"><span class="header-section-number">53.1</span> Steps of Batch Normalization:</h3>
<ol type="1">
<li><p><strong>Normalization:</strong></p>
<ul>
<li>For each mini-batch during training, normalize the input by subtracting the mean and dividing by the standard deviation. The normalization is applied independently to each feature (dimension) in the input.</li>
</ul>
<p><span class="math inline">\(\hat{x}^{(k)} = \frac{x^{(k)} - \mu}{\sqrt{\sigma^2 + \epsilon}}\)</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{x}^{(k)}\)</span> is the normalized output for the k-th feature.</li>
<li><span class="math inline">\(x^{(k)}\)</span> is the input for the k-th feature.</li>
<li><span class="math inline">\(\mu\)</span> is the mean of the mini-batch.</li>
<li><span class="math inline">\(\sigma^2\)</span> is the variance of the mini-batch.</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant added for numerical stability.</li>
</ul></li>
<li><p><strong>Scale and Shift:</strong></p>
<ul>
<li>Introduce learnable parameters (scale and shift) for each feature to allow the model to adapt during training.</li>
</ul>
<p><span class="math inline">\(y^{(k)} = \gamma \hat{x}^{(k)} + \beta\)</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(y^{(k)}\)</span> is the final output for the k-th feature.</li>
<li><span class="math inline">\(\gamma\)</span> is a learnable scale parameter.</li>
<li><span class="math inline">\(\beta\)</span> is a learnable shift parameter.</li>
</ul></li>
<li><p><strong>Training and Inference:</strong></p>
<ul>
<li>During training, the mean and variance are computed for each mini-batch and used for normalization.</li>
<li>During inference, running averages of mean and variance from the training phase are typically used for normalization to ensure consistency.</li>
</ul></li>
</ol>
</section>
<section id="benefits-of-batch-normalization" class="level3" data-number="53.2">
<h3 data-number="53.2" class="anchored" data-anchor-id="benefits-of-batch-normalization"><span class="header-section-number">53.2</span> Benefits of Batch Normalization:</h3>
<ol type="1">
<li><strong>Improved Training Stability:</strong>
<ul>
<li>Helps mitigate the internal covariate shift problem, leading to more stable and faster convergence during training.</li>
</ul></li>
<li><strong>Reduced Sensitivity to Initialization:</strong>
<ul>
<li>Reduces the sensitivity of the model to the choice of initial weights.</li>
</ul></li>
<li><strong>Allows Higher Learning Rates:</strong>
<ul>
<li>Enables the use of higher learning rates, which can accelerate training.</li>
</ul></li>
<li><strong>Acts as Regularization:</strong>
<ul>
<li>Introduces a slight regularization effect, reducing the need for other regularization techniques.</li>
</ul></li>
<li><strong>Applicability to Various Architectures:</strong>
<ul>
<li>Can be applied to various types of neural network architectures, including fully connected layers, convolutional layers, and recurrent layers.</li>
</ul></li>
</ol>
<p>Batch Normalization has become a standard component in many deep learning architectures due to its effectiveness in improving training stability and convergence speed.</p>
</section>
</div>
</div>
</div>
</section>
<section id="batch-normalization-the-algorithm" class="level2" data-number="54">
<h2 data-number="54" class="anchored" data-anchor-id="batch-normalization-the-algorithm"><span class="header-section-number">54</span> Batch normalization — The algorithm</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_57.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 57</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="how-does-batch-normalization-help-optimization" class="level2" data-number="55">
<h2 data-number="55" class="anchored" data-anchor-id="how-does-batch-normalization-help-optimization"><span class="header-section-number">55</span> How does batch normalization help optimization?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_58.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 58</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Because some layers will push ouputs in one direction, then other layers will not use this. With batch norm we centered data so that all layers train around these centered inputs</p>
</section>
<section id="how-does-batch-normalization-help-optimization-1" class="level2" data-number="56">
<h2 data-number="56" class="anchored" data-anchor-id="how-does-batch-normalization-help-optimization-1"><span class="header-section-number">56</span> How does batch normalization help optimization?</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_59.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 59</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="benefits-of-batch-normalization-1" class="level2" data-number="57">
<h2 data-number="57" class="anchored" data-anchor-id="benefits-of-batch-normalization-1"><span class="header-section-number">57</span> Benefits of Batch normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_60.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 60</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ol type="1">
<li>Train faster because all layers train similarly quickly</li>
<li>Allows you to have high learning rates because you wont have vanishing or exploding gradients because everything is 0-1 distributed</li>
<li>Makes weights easier to initialize, because you know everything will be between 0-1</li>
<li>Make activations function sensible because all the activation functions have something special about zero</li>
<li>Have added noise that comes from estimating the batch statistics, any noise that may help is regularization. Here the noise reduces overfitting and that acts as a regularization so that your model does not overfit.<br>
Put it simple: Noise Disrupts Patterns</li>
</ol>
</section>
<section id="quiz-1" class="level2" data-number="58">
<h2 data-number="58" class="anchored" data-anchor-id="quiz-1"><span class="header-section-number">58</span> Quiz</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_61.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 61</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>There is <strong>no</strong> answer here</p>
</section>
<section id="batch-normalization-at-test-time" class="level2" data-number="59">
<h2 data-number="59" class="anchored" data-anchor-id="batch-normalization-at-test-time"><span class="header-section-number">59</span> Batch normalization at test time</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_62.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 62</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>The important thing is that when you go to test time, you dont have batches (you dont want anything that is depended on how you construct the batch) because that means if you take another batch is not the same which then is not reproducible.</p>
<p>So what we usually do is keep a moving average of the mean and variance during training, and then at test time you plug them</p>
<p>Basically you extract the mean and the variance form the training and use it in test data</p>
</section>
<section id="disadvantages-of-batch-normalization" class="level2" data-number="60">
<h2 data-number="60" class="anchored" data-anchor-id="disadvantages-of-batch-normalization"><span class="header-section-number">60</span> Disadvantages of batch normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_63.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 63</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li>It requires you to have large batch sizes because otherwhise the estimate of mean and variance is too noisy</li>
<li>Problematic if you have discrepancy of the training and tst data</li>
<li>Now the loss you get from training sample A, depends on what other training sample are present in the batch via this normalization of the mean and variance</li>
<li>One disadvantage is that is usually the reason for bugs, because if you keep estimating the mean and variances for the test data but now it is not reproducible because it will depend on the batch itself</li>
</ul>
</section>
<section id="disadvantages-of-batch-normalization-with-distributed-training" class="level2" data-number="61">
<h2 data-number="61" class="anchored" data-anchor-id="disadvantages-of-batch-normalization-with-distributed-training"><span class="header-section-number">61</span> Disadvantages of batch normalization with distributed training</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_64.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 64</figcaption>
</figure>
</div>
</center>
<pre></pre>
<ul>
<li>Different values across GPU</li>
<li>If you batch size is small in a single GPU, but maybe you have 10 GPUs running and for 10 GPUs your batch size is bigger, it will be a stupid idea to estimate 10 very noisy estimate of the mean and variance but instead you should compute across the GPU</li>
</ul>
</section>
<section id="layer-normalization" class="level2" data-number="62">
<h2 data-number="62" class="anchored" data-anchor-id="layer-normalization"><span class="header-section-number">62</span> Layer normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_65.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 65</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Better explained <a href="https://www.pinecone.io/learn/batch-layer-normalization/">here</a></p>
<p>Here mean and variance are not computed across batch but across all channels and spatial dimensions.</p>
<p>So now the statistics are independent of the batch size because now they depend on the feature dimensions see example below:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Example Layer normalization
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Layer Normalization is a normalization technique similar to Batch Normalization but operates on a per-sample basis rather than per-minibatch. It normalizes the inputs of a layer across the features (dimensions) for each individual sample. Here’s an example of how Layer Normalization is typically applied:</p>
<div id="280218f0" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming input has shape (batch_size, num_features)</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Layer Normalization</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>layer_norm <span class="op">=</span> nn.LayerNorm(normalized_shape<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layer_norm(input_data)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display input and output shapes</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input shape:"</span>, input_data.shape)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output shape:"</span>, output.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Input shape: torch.Size([32, 64])
Output shape: torch.Size([32, 64])</code></pre>
</div>
</div>
<p>In this example:</p>
<ul>
<li><p><code>input_data</code> is a random tensor with shape (32, 64), representing a batch of 32 samples, each with 64 features.</p></li>
<li><p><code>nn.LayerNorm</code> is the Layer Normalization layer provided by PyTorch. The <code>normalized_shape</code> parameter specifies the number of features in the input tensor.</p></li>
<li><p>The <code>output</code> tensor is the result of applying Layer Normalization to the input data.</p></li>
</ul>
<p>Layer Normalization normalizes the values along the feature dimension independently for each sample. This means that each feature in a sample is normalized based on its mean and standard deviation across the entire sample, rather than across a batch as in Batch Normalization.</p>
<p>Layer Normalization is useful when the batch size is small or when working with sequences of varying lengths, as it normalizes each sample independently. It has been widely used in natural language processing tasks and recurrent neural networks.</p>
</div>
</div>
</div>
<p>Basically, we have that our mean and std will be computed across dimensions, so on the columns, not on each input. So then each sample (so each row) we make it with this mean and std to be distributed between 0 and 1.</p>
<p>This is great for RNN, or other stuff that requires small batch sizes.</p>
<p>Here the same operations happens at training and test time.</p>
<p>So now instead of normalizing across data samples now we basically i.e.&nbsp;if the input is an image that each color should be roughly be ocurring the same amount of spread across all the image, because now we normalize it across channel dimensions</p>
</section>
<section id="layer-normalization-ln" class="level2" data-number="63">
<h2 data-number="63" class="anchored" data-anchor-id="layer-normalization-ln"><span class="header-section-number">63</span> Layer normalization (LN)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_66.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 66</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="instance-normalization-in" class="level2" data-number="64">
<h2 data-number="64" class="anchored" data-anchor-id="instance-normalization-in"><span class="header-section-number">64</span> Instance normalization (IN)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_67.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 67</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Instance normalization now you do layer normalization but per channel and per training example.</p>
<p>So now the network should be agnostic to the constract of the original iamge and of the constrast whithin the channels</p>
<p>Not used that often</p>
<p>Here we compute the mean and var per sample but not per channel.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How Instance Normalization works
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Instance Normalization is a normalization technique similar to Batch Normalization and Layer Normalization but operates on a per-instance basis. It normalizes the activations of each individual sample independently. Here’s an explanation of how Instance Normalization works:</p>
<section id="instance-normalization-steps" class="level3" data-number="64.1">
<h3 data-number="64.1" class="anchored" data-anchor-id="instance-normalization-steps"><span class="header-section-number">64.1</span> Instance Normalization Steps:</h3>
<ol type="1">
<li><p><strong>Input Tensor:</strong></p>
<ul>
<li>Assume you have an input tensor <span class="math inline">\(X\)</span> with shape <span class="math inline">\((N, C, H, W)\)</span>, where:
<ul>
<li><span class="math inline">\(N\)</span> is the batch size.</li>
<li><span class="math inline">\(C\)</span> is the number of channels.</li>
<li><span class="math inline">\(H\)</span> is the height of the feature map.</li>
<li><span class="math inline">\(W\)</span> is the width of the feature map.</li>
</ul></li>
</ul></li>
<li><p><strong>Calculate Mean and Variance:</strong></p>
<ul>
<li>For each instance (sample) in the batch, calculate the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> along each channel independently. This is done for each channel and each instance separately.</li>
</ul>
<p><span class="math display">\[\mu_c = \frac{1}{H \cdot W} \sum_{i=1}^{H} \sum_{j=1}^{W} X_{n,c,i,j}\]</span></p>
<p><span class="math display">\[\sigma^2_c = \frac{1}{H \cdot W} \sum_{i=1}^{H} \sum_{j=1}^{W} (X_{n,c,i,j} - \mu_c)^2\]</span></p></li>
<li><p><strong>Normalize:</strong></p>
<ul>
<li>Normalize each channel of each instance independently using the calculated mean and standard deviation:</li>
</ul>
<p><span class="math display">\[\hat{X}_{n,c,i,j} = \frac{X_{n,c,i,j} - \mu_c}{\sqrt{\sigma^2_c + \epsilon}}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\hat{X}_{n,c,i,j}\)</span> is the normalized value.</li>
<li><span class="math inline">\(X_{n,c,i,j}\)</span> is the original input value.</li>
<li><span class="math inline">\(\mu_c\)</span> is the mean of the channel <span class="math inline">\(c\)</span> for the instance <span class="math inline">\(n\)</span>.</li>
<li><span class="math inline">\(\sigma^2_c\)</span> is the variance of the channel <span class="math inline">\(c\)</span> for the instance <span class="math inline">\(n\)</span>.</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant added for numerical stability.</li>
</ul></li>
<li><p><strong>Scale and Shift:</strong></p>
<ul>
<li>Introduce learnable scale (<span class="math inline">\(\gamma\)</span>) and shift (<span class="math inline">\(\beta\)</span>) parameters for each channel:</li>
</ul>
<p><span class="math display">\[Y_{n,c,i,j} = \gamma_c \hat{X}_{n,c,i,j} + \beta_c\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(Y_{n,c,i,j}\)</span> is the final normalized output.</li>
<li><span class="math inline">\(\gamma_c\)</span> is a learnable scale parameter for channel <span class="math inline">\(c\)</span>.</li>
<li><span class="math inline">\(\beta_c\)</span> is a learnable shift parameter for channel <span class="math inline">\(c\)</span>.</li>
</ul></li>
</ol>
</section>
<section id="benefits-of-instance-normalization" class="level3" data-number="64.2">
<h3 data-number="64.2" class="anchored" data-anchor-id="benefits-of-instance-normalization"><span class="header-section-number">64.2</span> Benefits of Instance Normalization:</h3>
<ul>
<li><strong>Normalization Across Samples:</strong>
<ul>
<li>Instance Normalization normalizes each instance independently, making it suitable for scenarios where batch sizes may vary or are small.</li>
</ul></li>
<li><strong>Reduces Covariate Shift:</strong>
<ul>
<li>Similar to Batch Normalization, Instance Normalization helps reduce internal covariate shift, leading to more stable training.</li>
</ul></li>
<li><strong>Applicability to Style Transfer:</strong>
<ul>
<li>Instance Normalization has found applications in style transfer tasks in computer vision.</li>
</ul></li>
</ul>
<p>Instance Normalization is often used in computer vision tasks, especially in scenarios where the batch size may be small or when normalization across instances is desired.</p>
</section>
</div>
</div>
</div>
</section>
<section id="group-normalization-gn" class="level2" data-number="65">
<h2 data-number="65" class="anchored" data-anchor-id="group-normalization-gn"><span class="header-section-number">65</span> Group normalization (GN)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_68.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 68</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We are gonna group different certain channels toguether. So we are not normalizing per channel but per groups, for instance 5 channels.</p>
<ol type="1">
<li><p>If you have only one group then you recover layer nomalization because layer normal normalizes across all channels and not per channel</p></li>
<li><p>If you have more number of channel group toguether i.e 3 channels group together then you do Instance normalization. meaning you compute the mean per sample across each channel</p></li>
</ol>
<ul>
<li><p>In Grouped Convs, you are basically separating the hidden layers or the hidden channels in hidden groups which makes computations more easier</p></li>
<li><p>This is better than batch normalization for small batches &lt;32</p></li>
</ul>
</section>
<section id="a-comparison-of-different-normalizations" class="level2" data-number="66">
<h2 data-number="66" class="anchored" data-anchor-id="a-comparison-of-different-normalizations"><span class="header-section-number">66</span> A comparison of different normalizations</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_69.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 69</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="weight-normalization" class="level2" data-number="67">
<h2 data-number="67" class="anchored" data-anchor-id="weight-normalization"><span class="header-section-number">67</span> Weight normalization</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_70.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 70</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>You can thing the weights like a vector, you can have its magnitude and its direction. Now with this g can learn this parameter which tells you how long you want to go in that direction</p>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_72.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 72</figcaption>
</figure>
</div>
</center>
<pre></pre>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_73.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 73</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="learning-rate" class="level2" data-number="68">
<h2 data-number="68" class="anchored" data-anchor-id="learning-rate"><span class="header-section-number">68</span> Learning rate</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_74.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 74</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="convergence" class="level2" data-number="69">
<h2 data-number="69" class="anchored" data-anchor-id="convergence"><span class="header-section-number">69</span> Convergence</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_75.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 75</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>To achieve convergence you need tehse two equations:</p>
<ol type="1">
<li>all learning over time should be infinitely to allow for exploration</li>
<li>The quadratic term should be less than infite to converge</li>
</ol>
</section>
<section id="learning-rate-schedules" class="level2" data-number="70">
<h2 data-number="70" class="anchored" data-anchor-id="learning-rate-schedules"><span class="header-section-number">70</span> Learning rate schedules</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_76.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 76</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>You also make a warmpup learning rate so you start by going up in a linear fashion</p>
</section>
<section id="in-practice" class="level2" data-number="71">
<h2 data-number="71" class="anchored" data-anchor-id="in-practice"><span class="header-section-number">71</span> In practice</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_77.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 77</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="quiz-2" class="level2" data-number="72">
<h2 data-number="72" class="anchored" data-anchor-id="quiz-2"><span class="header-section-number">72</span> Quiz</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_78.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 78</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>We do all of the above,</p>
<ul>
<li>It is good that if the loss explode you check the individual values of the gradients,</li>
<li>or was the batch size was so small that something could have affect it</li>
<li>or was the learning rate too high that even a small batch it was not too but but the large learning rate was too big tha then it end up in a completely broken spot in the NNs</li>
</ul>
</section>
<section id="dropout-rate" class="level2" data-number="73">
<h2 data-number="73" class="anchored" data-anchor-id="dropout-rate"><span class="header-section-number">73</span> Dropout rate</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_79.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 79</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="batch-size" class="level2" data-number="74">
<h2 data-number="74" class="anchored" data-anchor-id="batch-size"><span class="header-section-number">74</span> Batch size</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_80.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 80</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="designing-cnns-to-become-even-better.-dont-try-this-at-home" class="level2" data-number="75">
<h2 data-number="75" class="anchored" data-anchor-id="designing-cnns-to-become-even-better.-dont-try-this-at-home"><span class="header-section-number">75</span> Designing CNNs to become even better. (Don’t try this at home)</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_81.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 81</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Architecture, which model do you use?</p>
</section>
<section id="number-of-layers-and-neurons" class="level2" data-number="76">
<h2 data-number="76" class="anchored" data-anchor-id="number-of-layers-and-neurons"><span class="header-section-number">76</span> Number of layers and neurons</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_82.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 82</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>Progress is not only in the architecture side, for example if you want to develop better algorithms, you want to have just a neural network which can identify between cats and dogs, there is multiple ways:</p>
<ol type="1">
<li>Come up with a new architecture</li>
<li>Come up with better gradients and better weights of the NNs</li>
</ol>
</section>
<section id="babysitting-deep-nets" class="level2" data-number="77">
<h2 data-number="77" class="anchored" data-anchor-id="babysitting-deep-nets"><span class="header-section-number">77</span> Babysitting Deep Nets</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_83.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 83</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p>for classifying 8 classes, the loss should be -log(1/8) and you check whether is true, if it gets worst performance that randomly guessing so something is wrong</p>
</section>
<section id="logging-tools" class="level2" data-number="78">
<h2 data-number="78" class="anchored" data-anchor-id="logging-tools"><span class="header-section-number">78</span> Logging tools</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_84.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 84</figcaption>
</figure>
</div>
</center>
<pre></pre>
</section>
<section id="babysitting-deep-nets-1" class="level2" data-number="79">
<h2 data-number="79" class="anchored" data-anchor-id="babysitting-deep-nets-1"><span class="header-section-number">79</span> Babysitting Deep Nets</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_85.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 85</figcaption>
</figure>
</div>
</center>
<pre></pre>
<p><a href="http://karpathy.github.io/2019/04/25/recipe/">Link1</a></p>
</section>
<section id="reading-material" class="level2" data-number="80">
<h2 data-number="80" class="anchored" data-anchor-id="reading-material"><span class="header-section-number">80</span> Reading material</h2>
<center>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/page_86.png" class="w575 img-fluid figure-img"></p>
<figcaption>Slide 86</figcaption>
</figure>
</div>
</center>
<pre></pre>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.danilotpnta\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about/index.html">
<p><span class="footerDaniloToapanta">© 2024 Danilo Toapanta</span></p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../privacy/index.html">
<p>Privacy</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../contact/index.html">
<p>Contact</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../license/index.html">
<p>License</p>
</a>
  </li>  
</ul>
    <div class="toc-actions"><ul><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/blob/main/blog/2023-11-20_deep-learning-optimizations-ii/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/edit/main/blog/2023-11-20_deep-learning-optimizations-ii/index.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/danilotpnta/danilotpnta.github.io/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="../../docs/sitemap.xml">
      <i class="bi bi-rss-fill" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>

    let navbar = document.getElementsByClassName("navbar-nav")[0]    

    let li2 = document.createElement("li");
    li2.className = "nav-item compact";

    let a2 = document.createElement("a");
    a2.className = "nav-link quarto-color-scheme-toggle";
    a2.style.cursor = "pointer"
    li2.appendChild(a2)

    let i2 = document.createElement("i");
    i2.className = "bi bi-moon"
    a2.append(i2)

    navbar.appendChild(li2);

    i2.onclick = function() {
        window.quartoToggleColorScheme(); return false;
    }
    // <a href="http://localhost:4200/about/" class="quarto-color-scheme-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>

    let li = document.createElement("li");
    li.className = "nav-item compact";

    let a = document.createElement("a");
    a.className = "nav-link";
    a.style.cursor = "pointer"
    li.appendChild(a)

    let i = document.createElement("i");
    i.className = "bi bi-search"
    a.append(i)

    // let span = document.createElement("span");
    // span.className = "menu-text"
    // a.append(span)

    navbar.appendChild(li);

    a.onclick = function() {
        window.quartoOpenSearch()
    }


</script>






</body></html>