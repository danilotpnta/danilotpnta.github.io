[
  {
    "objectID": "0257e11a/blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.out.html",
    "href": "0257e11a/blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.out.html",
    "title": "Script to extract pages of PDF as images & apply crop",
    "section": "",
    "text": "| Danilo Toapanta\n2023-11-05"
  },
  {
    "objectID": "0257e11a/blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.out.html#goal",
    "href": "0257e11a/blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.out.html#goal",
    "title": "Script to extract pages of PDF as images & apply crop",
    "section": "Goal",
    "text": "Goal\nImagine you want to extract pages from a PDF as images and then you want to crop them to an specific size. The following scrip does exactly that\npip install Pillow\npip install pdf2image\nbrew install poppler\n\n\nCode\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport os\n\n# Function to crop an image to specific dimensions\ndef crop_image(input_path, output_path, left, top, right, bottom):\n    image = Image.open(input_path)\n    cropped_image = image.crop((left, top, right, bottom))\n    cropped_image.save(output_path)\n\n# Function to extract all pages from a PDF and crop them as images\ndef extract_and_crop_pdf(pdf_path, output_dir, width, crop_up, crop_down):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    images = convert_from_path(pdf_path)\n\n    for page_number, pdf_image in enumerate(images):\n        output_path = os.path.join(output_dir, f'page_{page_number + 1}.png')\n\n        # Calculate the crop coordinates\n        left = (pdf_image.width - width) // 2\n        top = crop_up\n        right = left + width\n        bottom = pdf_image.height - crop_down\n\n        # Save the image as a temporary file\n        temp_image_path = os.path.join(output_dir, f'temp_page_{page_number + 1}.png')\n        pdf_image.save(temp_image_path)\n\n        # Crop the temporary image and save the final cropped image\n        crop_image(temp_image_path, output_path, left, top, right, bottom)\n\n        # Clean up the temporary image\n        os.remove(temp_image_path)\n\nif __name__ == '__main__':\n    input_pdf = 'input.pdf'  # Specify the input PDF file\n    output_directory = 'output_images'  # Specify the output folder\n    crop_up = 177  # Change this to your desired crop for height up\n    crop_down = 118  # Change this to your desired crop for height down\n    target_width = 2667  # Change this to your desired width\n\n    extract_and_crop_pdf(input_pdf, output_directory, target_width, crop_up, crop_down)"
  },
  {
    "objectID": "0257e11a/blog/2023-11-13_sequence-labelling/notebooks/index.out.html",
    "href": "0257e11a/blog/2023-11-13_sequence-labelling/notebooks/index.out.html",
    "title": "Notebook",
    "section": "",
    "text": "| Danilo Toapanta\n2023-11-07\n\n\nCode\nimport numpy as np\nw1 = np.array([[1,2,3]])\nw2 = np.array([[4,5,6]])\nw_stack = np.stack([w1,w2])\nprint(w_stack.shape)\n\n\n(2, 1, 3)"
  },
  {
    "objectID": "dev/index.html",
    "href": "dev/index.html",
    "title": "Danilo Toapanta",
    "section": "",
    "text": "In this section I have recompiled the sources, links and all the material that were more helpfull during the creation of this website."
  },
  {
    "objectID": "notes/2023-09-03_commands-terminal/index.html",
    "href": "notes/2023-09-03_commands-terminal/index.html",
    "title": "Commands Terminal",
    "section": "",
    "text": "Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 3, 2023"
  },
  {
    "objectID": "notes/2023-09-03_commands-terminal/index.html#terminal",
    "href": "notes/2023-09-03_commands-terminal/index.html#terminal",
    "title": "Commands Terminal",
    "section": "1 Terminal",
    "text": "1 Terminal\ncd -    # one folder back\ncd ..   # one up\ncd ../  # two up\ncd      # home directory \ncd /    # root directory\npwd         # print working directory\nopen &lt;PATH&gt;\nopen .     # open current folder you're in\ntouch &lt;file.txt&gt; # to create a file"
  },
  {
    "objectID": "notes/2023-09-03_commands-terminal/index.html#git-hub",
    "href": "notes/2023-09-03_commands-terminal/index.html#git-hub",
    "title": "Commands Terminal",
    "section": "2 Git Hub",
    "text": "2 Git Hub\ngit clone &lt;HTTPS&gt;    # copy repo from the HTTPS of the GitHub repository \ngit status                      # to check whether the tree line has been updated\ngit add .                       # add all new folders and files to git repo\ngit add &lt;name&gt;       # add specific folder for the git repo\n                                         # this is only to tell git taht there has been changes, we still need to commit\ngit commit -m \"&lt;message&gt;\"   # this save the files to git repo\ngit commit -m \"&lt;message&gt;\" -m \"&lt;message for the description box&gt;\"   # this save the files to git repo\nSo the way it works\n\nYou change the code\nYou need to save to the git file –&gt; git add .\nYou need to commit –&gt; git commit - m “message”\nYou push it to the repo –&gt; git push main\n\n\n2.1 Working with branches\n\nhttps://youtu.be/QV0kVNvkMxc\n\n\n\n2.2 Key Generation\nssh-keygen -t rsa -b 4096 -C \"danilotpnta@gmail.com\"   # to generate a key SSH\nls | grep git-key   # to check the keys available\n                    # key.pub pub stands for public \ncat &lt;example_key.pub&gt;                     # to print the key\n\n\n2.3 Generating a new SSH key\nssh-keygen -t ed25519 -C \"danilotpnta@gmail.com\" \n\n\n2.4 Adding your SSH key to the ssh-agent\neval \"$(ssh-agent -s)\"\nopen ~/.ssh/config\nvim ~/.ssh/config\nssh-add ~/.ssh/id_ed25519   # to add the key with name \"id_ed25519\"\ncat ~/.ssh/id_ed25519.pub   # to print the key that we created"
  },
  {
    "objectID": "notes/2023-09-03_commands-terminal/index.html#vim",
    "href": "notes/2023-09-03_commands-terminal/index.html#vim",
    "title": "Commands Terminal",
    "section": "3 Vim",
    "text": "3 Vim\n:wq     # to write/save and quit\nESC     # to scape from __INSERT__\n\n3.1 Prompt Message\nhe Bash command prompt looks like this by default:\n[USERNAME]@[HOSTNAME]:[PATH][SYMBOL]\n\n[USERNAME] is the username of the currently operating user. normally this is your user, but when you run sudo su or similar commands, you get a “root shell”, that means the user is “root”.\n[HOSTNAME] is your hostname. It’s the name of your computer. You had to enter that during the system installation.\n[PATH] is your current working directory, the directory you’re currently operating on. When you open a new terminal, the default directory is your current user’s home directory. A synonym for /home/YOURUSERNAME is ~.\n[SYMBOL] is usually either $ if you’re operating as any normal user, or # if you’re operating as “root” user.\n\nSo your Bash prompt looks like this:\nganesh@ganesh:~$\nThat means you’re logged in as user ganesh on a computer called ganesh as well, currently operating in your own home directory (~). Of course you’re not “root”, therefore the $."
  },
  {
    "objectID": "notes/2023-09-11_how-to-create-conda-environment-using-yml-file/index.html",
    "href": "notes/2023-09-11_how-to-create-conda-environment-using-yml-file/index.html",
    "title": "How to create Conda environment using YML file?",
    "section": "",
    "text": "Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Environment\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Conda\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Environment\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Conda\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 11, 2023"
  },
  {
    "objectID": "notes/2023-09-11_how-to-create-conda-environment-using-yml-file/index.html#define-yml-requirements",
    "href": "notes/2023-09-11_how-to-create-conda-environment-using-yml-file/index.html#define-yml-requirements",
    "title": "How to create Conda environment using YML file?",
    "section": "1 Define YML requirements",
    "text": "1 Define YML requirements\nFor instance the following file: environment.yml\nname: cv1\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.7\n  - pip=21.3.1\n  - pip:\n    - numpy==1.19.5\n    - opencv-contrib-python==3.4.2.17\n    - matplotlib==3.3.4\n    - jupyter==1.0.0\n    - scikit-learn==0.23.0\n    - scipy==1.5.4"
  },
  {
    "objectID": "notes/2023-09-11_how-to-create-conda-environment-using-yml-file/index.html#create-conda-env-from-terminal",
    "href": "notes/2023-09-11_how-to-create-conda-environment-using-yml-file/index.html#create-conda-env-from-terminal",
    "title": "How to create Conda environment using YML file?",
    "section": "2 Create Conda ENV from terminal",
    "text": "2 Create Conda ENV from terminal\nconda env create -f path/to/environment.yml"
  },
  {
    "objectID": "notes/2023-08-22_command_for_environments/index.html",
    "href": "notes/2023-08-22_command_for_environments/index.html",
    "title": "Command Environments",
    "section": "",
    "text": "A useful list of conda commands\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Environments\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Conda\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Environments\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Conda\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 22, 2023"
  },
  {
    "objectID": "notes/2023-08-22_command_for_environments/index.html#conda",
    "href": "notes/2023-08-22_command_for_environments/index.html#conda",
    "title": "Command Environments",
    "section": "Conda",
    "text": "Conda\n\nA conda environment is a directory that contains a specific collection of conda packages that you have installed. For example, you may have one environment with NumPy 1.7 and its dependencies, and another environment with NumPy 1.6 for legacy testing."
  },
  {
    "objectID": "notes/2023-08-22_command_for_environments/index.html#anaconda",
    "href": "notes/2023-08-22_command_for_environments/index.html#anaconda",
    "title": "Command Environments",
    "section": "Anaconda",
    "text": "Anaconda\n\nIt gives you all the standard packages used in scientific computing in a convenient package without having to worry about installing them all individually with their dependencies."
  },
  {
    "objectID": "notes/2023-08-22_command_for_environments/index.html#in-short-conda-anaconda",
    "href": "notes/2023-08-22_command_for_environments/index.html#in-short-conda-anaconda",
    "title": "Command Environments",
    "section": "In short Conda + Anaconda",
    "text": "In short Conda + Anaconda\nConda is a package manager. It helps you take care of your different packages by handling installing, updating and removing them. Anaconda contains all of the most common packages (tools) a data scientist needs and can be considered the hardware store of data science tools.\nconda install anaconda"
  },
  {
    "objectID": "notes/2023-08-22_command_for_environments/index.html#how-to-erase-an-environment",
    "href": "notes/2023-08-22_command_for_environments/index.html#how-to-erase-an-environment",
    "title": "Command Environments",
    "section": "How to erase an environment",
    "text": "How to erase an environment\nconda remove --name ml1labs --all"
  },
  {
    "objectID": "notes/2023-08-22_command_for_environments/index.html#echo-path",
    "href": "notes/2023-08-22_command_for_environments/index.html#echo-path",
    "title": "Command Environments",
    "section": "echo $PATH",
    "text": "echo $PATH\n\n$PATH (or the search path) is the list of directories that will be searched for anything that you type on the command line\nWe use sometimes ls or pwd or echo this is because there is a $PATH where this was. This list of pre-designated directories is stored in a special variable called “PATH”\nThis is a colon-delimited list of all the directories the command line looks in by default for programs on the particular computer.\nExample\n$ (progLab) datoapanta@Danilos-MacBook-Pro ~ % echo $PATH\n/Users/datoapanta/opt/anaconda3/envs/progLab/bin:/Users/datoapanta/opt/anaconda3/condabin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/share/dotnet:~/.dotnet/tools:/Library/Frameworks/Mono.framework/Versions/Current/Commands:/Users/datoapanta/.local/bin:/Users/datoapanta/.local/bin\necho $PATH | tr \":\" \"\\n\"\n/Users/datoapanta/opt/anaconda3/envs/progLab/bin /Users/datoapanta/opt/anaconda3/condabin /usr/local/bin /usr/bin /bin /usr/sbin /sbin /Library/TeX/texbin /usr/local/share/dotnet ~/.dotnet/tools /Library/Frameworks/Mono.framework/Versions/Current/Commands /Users/datoapanta/.local/bin /Users/datoapanta/.local/bin\n\nWe can now more clearly see this is a list of directories. All of these places, stored in the variable called “PATH”, are searched whenever we are typing a command in the terminal window.\nIf the command we are trying to use is present in any of the directories listed in our PATH, we don’t need to point at its specific location in full (its path, lowercase) when we are trying to use it – which is of course nice for things we use often."
  },
  {
    "objectID": "notes/2023-09-04_how-to-create-a-jupyther-kernel/index.html",
    "href": "notes/2023-09-04_how-to-create-a-jupyther-kernel/index.html",
    "title": "How to Create a Jupyther Kernel",
    "section": "",
    "text": "How to Create a Jupyther Kernel\n        \n        \n                    \n                \n                    Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 4, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\npython3 -m pip install tensorflow\nconda install -c anaconda ipykernel\npython -m ipykernel install --user --name=firstEnv"
  },
  {
    "objectID": "notes/2023-08-22_commands-for-jupyter/index.html",
    "href": "notes/2023-08-22_commands-for-jupyter/index.html",
    "title": "Commands Jupyter",
    "section": "",
    "text": "Commands Jupyter\n        \n        \n                    \n                \n                    Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 22, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\njupyter kernelspec list              \npython -m ipykernel install --user --name=git-pages"
  },
  {
    "objectID": "notes/2023-08-22_commands-for-github/index.html",
    "href": "notes/2023-08-22_commands-for-github/index.html",
    "title": "Commands GitHub",
    "section": "",
    "text": "Commands GitHub\n        \n        \n                    \n                \n                    A useful list of Github commands\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                GitHub\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                GitHub\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 22, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n\nGithub\ngh repo create\ngit add . && git commit - 'update'\ngit push origin master \n\nmkdir repo-name\ncd repo-name\ntouch file.js\ngit init\ngit add . && git commit -m 'update'\ngh repo create &gt; existing\n\ngit push origin master\n\ngit pull origin master\n\n# To remove files from terminal\ngit rm -r fil\n\n# To change of branch\n\n\ngh\ngh repo delete name-repo\ngh repo create\ngh browse         # Open the repo in a browser Tab\n\n# To update changes to the website\ncd danilotpnta.github.io/\ngit checkout main   # Make sure you are in main branch \nquarto publish gh-pages"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "\nPython\n",
    "section": "",
    "text": "Showing All Notes\n\n \n\nShow All Notes \n\n\n\n\n\n\n\nBlog\n\n\n\n\n\n\n\n\n\n #   published: \n\n\nPython\n\n\n\n\n\n\n\n\n\n     \n    \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n            \n            1\n        \n            \n                \n                1\n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n        \n            \n        \n        \n            \n            2\n            \n            3\n        \n            \n                \n                2\n                \n                3\n        \n        \n            \n            4\n        \n            \n        \n        \n        \n            \n                \n                4\n        \n        \n            \n            5\n        \n            \n                \n                5\n        \n        \n            \n            6\n        \n            \n\nNotes\n    \n    \n                    \n                            \n                                \n                                    Oct 11\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Basic Terminal Commands in Windows NEW\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Oct 11\n                            \n                    \n                    \n                            \n                                \n                                    Oct 09\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Numpy Arrays\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Oct 09\n                            \n                    \n                    \n                            \n                                \n                                    Oct 06\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            How to embed ML application?\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Oct 06\n                            \n                    \n                    \n                            \n                                \n                                    Sep 11\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            How to create Conda environment using YML file?\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Sep 11\n                            \n                    \n                    \n                            \n                                \n                                    Sep 04\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            How to Create a Jupyther Kernel\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Sep 04\n                            \n                    \n                    \n                            \n                                \n                                    Sep 03\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Create Website using Local\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Sep 03\n                            \n                    \n                    \n                            \n                                \n                                    Sep 03\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Commands Terminal\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Sep 03\n                            \n                    \n                    \n                            \n                                \n                                    Aug 27\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Changes to Vanilla Quarto\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 27\n                            \n                    \n                    \n                            \n                                \n                                    Aug 27\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Features, tools & improvements website\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 27\n                            \n                    \n                    \n                            \n                                \n                                    Aug 26\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            How to highlight lines of code in Github\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 26\n                            \n                    \n                    \n                            \n                                \n                                    Aug 22\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Commands Shells\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 22\n                            \n                    \n                    \n                            \n                                \n                                    Aug 22\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Commands GitHub\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 22\n                            \n                    \n                    \n                            \n                                \n                                    Aug 22\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Commands Jupyter\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 22\n                            \n                    \n                    \n                            \n                                \n                                    Aug 22\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Command Environments\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 22\n                            \n                    \n                    \n                            \n                                \n                                    Aug 22\n                                \n                                \n                                \n                            \n                            \n                                \n                                \n                                    \n                                            Commands Quarto\n                                    \n                                \n                                \n                            \n                            \n                                \n                                \n                                \n                                        \n                                        \n                                                \n                                                    All\n                                                \n                                                                            \n                                \n                            \n                            \n                                \n                                \n                                \n                                \n                                    Aug 22\n                            \n                    \n    \n\n\n         \n            Environment\n        \n         \n            Shells\n        \n         \n            Scripting\n        \n         \n            GitHub\n        \n         \n            Environments\n        \n         \n            Quarto\n        \n         \n            Conda\n        \n         \n            Bash\n        \n         \n            Zsh\n        \n         \n            Python\n        \n         \n            Conda\n        \n         \n            Notes-table\n        \n\n\nNo matching items\n\n\n\n\n\n  \n  \n    \n    CATEGORIES\n      \n    \n      \n  \n  \n  \n    \n      TAGS"
  },
  {
    "objectID": "notes/2023-08-22_commands-for-shells/index.html",
    "href": "notes/2023-08-22_commands-for-shells/index.html",
    "title": "Commands Shells",
    "section": "",
    "text": "Commands Shells\n        \n        \n                    \n                \n                    A useful list of Shell commands\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Shells\n                            \n                        \n                                            \n                            \n                                Scripting\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Bash\n                            \n                        \n                                            \n                            \n                                Zsh\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Shells\n                            \n                        \n                                            \n                            \n                               \n                                Scripting\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Bash\n                            \n                        \n                                            \n                            \n                               \n                                Zsh\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 22, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n\nBash\n# Creates an alias function that can be called from terminal\nfunction(){\n    \n    # $1 first argument passed from terminal\n    FOLDER=$(python create_dir.py $1 $2) \n\n    # To print a variable\n    echo $FOLDER\n\n    # Use var inside another var uses ${}\n    BACKUPDIR=$(ls -td ${FOLDER}/*/ | head -1)\n\n}\n\n\nZsh\n\n# Eliminate the last line of history\ncd\ncode .zsh_history\nreload_shell\n\n# Makes reload the page\nreload_shell(){\n  cd \n  source .zshrc   \n}\n\n# Folder for plugin's\nopen ~/.oh-my-zsh/plugins\n\n# Show all including empty files\nls -a            \n\n\nSyntax\n# Valid naming\n_ALI\nTOKEN_A\n\n# Invalid naming\n2_VAR\n-VARIABLE\nVAR1-VAR2"
  },
  {
    "objectID": "notes/2023-10-11_basic-terminal-commands-in-windows/index.html",
    "href": "notes/2023-10-11_basic-terminal-commands-in-windows/index.html",
    "title": "Basic Terminal Commands in Windows",
    "section": "",
    "text": "Basic Terminal Commands in Windows\n        \n        \n                    \n                \n                    Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 11, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n# Windows                   # MacOS\ndir                         # ls \necho.&gt; environment.yml      # touch\nstart .                     # open .\nnotepad environment.yml     # open env w/ app"
  },
  {
    "objectID": "notes/2023-08-26_how-to-highlight-lines-of-code-in-github/index.html",
    "href": "notes/2023-08-26_how-to-highlight-lines-of-code-in-github/index.html",
    "title": "How to highlight lines of code in Github",
    "section": "",
    "text": "How to highlight lines of code in Github\n        \n        \n                    \n                \n                    Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 26, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\nYou use at the end of an html:\n#L5-L6\n\n[Demo](https://github.com/danilotpnta/hammerspoon-config/blob/master/init.lua#L5-L6)\nDemo"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html",
    "title": "Deep Feedforward Networks",
    "section": "",
    "text": "Notes for course DL1 at University of Amsterdam 3 Nov 2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 5, 2023"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#from-linear-functions-to-nonlinear-from-shallow-to-deep",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#from-linear-functions-to-nonlinear-from-shallow-to-deep",
    "title": "Deep Feedforward Networks",
    "section": "1 From linear functions to nonlinear = from shallow to deep",
    "text": "1 From linear functions to nonlinear = from shallow to deep\n\n\n\n\nHere to apply first a Relu and the output of that another Relu: \\(\\sigma(B\\sigma(A\\textbf{x}))\\)"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#deep-feedforward-networks",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#deep-feedforward-networks",
    "title": "Deep Feedforward Networks",
    "section": "2 Deep feedforward networks",
    "text": "2 Deep feedforward networks\n\n\n\n\nMLP is just a combination of multiple linear perceptrons, in each layer there would be parameters ie \\(A\\) is \\(\\theta_1\\), \\(B\\) is \\(\\theta_2\\)"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#neural-networks-in-blocks",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#neural-networks-in-blocks",
    "title": "Deep Feedforward Networks",
    "section": "3 Neural networks in blocks",
    "text": "3 Neural networks in blocks"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#non-linear-feature-learning-perspective",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#non-linear-feature-learning-perspective",
    "title": "Deep Feedforward Networks",
    "section": "4 Non-linear feature learning perspective",
    "text": "4 Non-linear feature learning perspective\n\n\n\n\nHere we are saying that at the end we have just have a linear function \\(C\\) “the linear model” ie.\n\\[\nC(\\sigma(B\\sigma(A\\textbf{x})))\n\\]\nto a transformed input \\(\\varphi(x\\textbf{x})\\) = \\(\\sigma(B\\sigma(A\\textbf{x}))\\)"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#how-to-get-w-gradiend-based-learning",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#how-to-get-w-gradiend-based-learning",
    "title": "Deep Feedforward Networks",
    "section": "5 How to get w? gradiend-based learning",
    "text": "5 How to get w? gradiend-based learning\n\nDue to nonlinearity our loss function would be nonconvex\nWe use then SGD\nNo guarantee of convergence, and sensitive to initialization of the parameters"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#cost-function",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#cost-function",
    "title": "Deep Feedforward Networks",
    "section": "6 Cost Function",
    "text": "6 Cost Function\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere saturated means that the function becomes very flat so then the gradient of this is very minimal so then we cannot compute optimize because all the derivatives would look like very similar"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#activation-functions",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#activation-functions",
    "title": "Deep Feedforward Networks",
    "section": "7 Activation Functions",
    "text": "7 Activation Functions\n\n\n\n\n\nDefined how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.\nIf output range limited, then called a “squashing function.”\nThe choice of activation function has a large impact on the capability and performance of the neural network.\nDifferent activation functions may be combined, but rare\nAll hidden layers typically use the same activation function\nNeed to be differentiable at most points"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#linear-units-fully-connected-layer",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#linear-units-fully-connected-layer",
    "title": "Deep Feedforward Networks",
    "section": "8 Linear Units/ “Fully connected layer”",
    "text": "8 Linear Units/ “Fully connected layer”"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#advantages-of-relu",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#advantages-of-relu",
    "title": "Deep Feedforward Networks",
    "section": "9 Advantages of ReLU",
    "text": "9 Advantages of ReLU\n\n\n\nReLu is better for propagation because if i.e take a sin as activation function when we derive close to zero the gradient would be very small, we keep doing this over multiple layers and essentially multiplying small times smalls at the end the result would be close to \\(0\\)."
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#disadvantages-of-relu",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#disadvantages-of-relu",
    "title": "Deep Feedforward Networks",
    "section": "10 Disadvantages of ReLU",
    "text": "10 Disadvantages of ReLU"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#leaky-relu",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#leaky-relu",
    "title": "Deep Feedforward Networks",
    "section": "11 Leaky ReLU",
    "text": "11 Leaky ReLU"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#exponential-linear-unit-elu",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#exponential-linear-unit-elu",
    "title": "Deep Feedforward Networks",
    "section": "12 Exponential Linear Unit (ELU)",
    "text": "12 Exponential Linear Unit (ELU)\n\n\n\nUsed in Language models like BERT"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#gaussian-error-linear-unit",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#gaussian-error-linear-unit",
    "title": "Deep Feedforward Networks",
    "section": "13 Gaussian Error Linear Unit",
    "text": "13 Gaussian Error Linear Unit\n\n\n\n\nWe are not bounded by the computation power but by memory to be loaded in the computation units of the GPUs on Snellius"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#sigmoid-and-tanh",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#sigmoid-and-tanh",
    "title": "Deep Feedforward Networks",
    "section": "14 Sigmoid and Tanh",
    "text": "14 Sigmoid and Tanh"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#softmax",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#softmax",
    "title": "Deep Feedforward Networks",
    "section": "15 Softmax",
    "text": "15 Softmax\n\n\n\n\nIt outputs a probability distribution because it depends on the denominator which means all the variables would be taken into account. For instance this can be the last activation function to output probabilities of predicting a cat, a dog, a bird and so on.\nIf we now introduce a term \\(\\tau\\) in Softmax (by dividing the \\(e^{\\frac{x_i}{\\tau}}\\) in the numerator and denominator) then we have the following:\n\n\n\n\n\n\nSoftamx and \\(\\tau\\)\n\n\n\nIf τ is introduced, it can be used to control the temperature or the “sharpness” of the softmax distribution.\nWhen τ is set to a value greater than 1, it has the effect of “softening” the probabilities, making the distribution more uniform. In other words, it makes the probability of all categories more similar to each other. This can be useful in scenarios where you want to explore a wider range of possibilities or reduce the impact of extreme values in the input vector.\nConversely, when τ is set to a value less than 1, it has the effect of “sharpening” the probabilities, making the distribution more peaky, and emphasizing the largest values in the input vector. This can be useful when you want to make more distinct predictions and reduce uncertainty."
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#how-to-choose-and-activation-function",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#how-to-choose-and-activation-function",
    "title": "Deep Feedforward Networks",
    "section": "16 How to Choose and Activation Function",
    "text": "16 How to Choose and Activation Function\n\n\n\nHere inference is testing or predicting\n\n16.1 Difference between Multiclass classification vs Multilabel calssification\n\nMulticlass Classification:\n\nIn multiclass classification, the task is to assign an input data point to one and only one class or category from a set of multiple mutually exclusive classes.\nEach data point belongs to exactly one class, and the goal is to determine which class that data point most likely belongs to.\nExamples of multiclass classification problems:\n\nHandwritten digit recognition: Given an image of a handwritten digit (0-9), determine which digit it represents.\nSpecies classification: Given a photo of an animal, classify it into one of several species (e.g., dog, cat, bird, etc.).\n\n\nExample: Suppose you have a multiclass classification problem where you want to classify fruits into three categories: apples, oranges, and bananas. If you input an image of an apple, the model should predict that it belongs to the “apples” class.\nMultilabel Classification:\n\nIn multilabel classification, each data point can be associated with one or more classes or labels. It’s not limited to assigning a single class per data point.\nThis is used when a data point can have multiple attributes or characteristics simultaneously, and you want to predict all relevant labels.\nExamples of multilabel classification problems:\n\nDocument categorization: Tagging a document with multiple topics or subjects that are present in it.\nImage tagging: Assigning tags to an image to describe its content, where an image may contain multiple objects or scenes.\n\n\nExample: Consider an image tagging problem. You have an image containing a beach scene with a dog and a sunset. In a multilabel classification scenario, the model might predict the following labels: “beach,” “dog,” and “sunset,” because all three labels are relevant to the image.\n\n\n\n\n\n\n\nHow to use Softmax in a Multiclass Classification problem?\n\n\n\n\n\nIn a multiclass classification problem, the softmax function is commonly used to convert the raw output scores of a model into a probability distribution over multiple classes. This probability distribution allows you to determine the likelihood of each class for a given input data point. Here’s how you use the softmax function in a multiclass classification problem:\n\nModel Output:\n\nAfter training your multiclass classification model, you will typically have a final layer in your neural network or a scoring function that produces raw scores (logits) for each class. These scores are not yet probabilities but represent the model’s confidence in each class.\nLet’s say you have N classes, and the model’s output for a particular input is a vector of raw scores \\(z\\) with N elements, one for each class.\n\nApply Softmax Function:\n\nTo convert the raw scores into probabilities, apply the softmax function to the \\(z\\) vector. The softmax function transforms the scores into a probability distribution where the sum of the probabilities for all classes equals 1.\nThe softmax function for class \\(i\\) is given by: \\(\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}}\\)\nCalculate the softmax value for each class \\(i\\) in the \\(z\\) vector to obtain a probability distribution.\n\nPredict the Class:\n\nThe class with the highest probability in the softmax distribution is typically chosen as the predicted class for the input data point. In other words, the class with the highest \\(\\text{softmax}(z)_i\\) value is the model’s prediction for that input.\n\n\nHere’s a step-by-step example of using softmax for multiclass classification:\nSuppose you have a multiclass classification problem with three classes: “cat,” “dog,” and “bird.” After processing an input image, your model produces the following raw scores:\n\\(z = [2.1, 0.9, 1.5]\\)\n\nApply the softmax function:\n\nCalculate the softmax values for each class:\n\\(\\text{softmax}(z)_1 = \\frac{e^{2.1}}{e^{2.1} + e^{0.9} + e^{1.5}}\\)\n\\(\\text{softmax}(z)_2 = \\frac{e^{0.9}}{e^{2.1} + e^{0.9} + e^{1.5}}\\)\n\\(\\text{softmax}(z)_3 = \\frac{e^{1.5}}{e^{2.1} + e^{0.9} + e^{1.5}}\\)\n\nPredict the class:\n\nThe class with the highest softmax probability is the predicted class. In this case, if \\(\\text{softmax}(z)_1\\) is the highest probability, the model predicts “cat.”\n\n\nUsing the softmax function in multiclass classification allows you to obtain a probability distribution over classes and select the most likely class as the model’s prediction."
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#width-and-depth",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#width-and-depth",
    "title": "Deep Feedforward Networks",
    "section": "17 Width and Depth",
    "text": "17 Width and Depth\n\n\n\n\nIf we have a single big hidden layer (so a lot of parameters) can approximate any functions but in practice this infeasible.\n\nWidth is how many neurons in a single layer\nDepth is the number of layers\n\nThis also does not tell you how many hidden units you would need. The theorem just says to approx any function you need infinitely.\n\n\n\n\nHere we are saying that Deeper models reduce the loss functions (reduce the generalization error) because they generalize better meaning do not overfit. So do not need one single layer with a lot of neurons but instead multiple layers with fewer neurons.\n\n17.1 Convolution Neural Networks vsFully Connected Neural Networks\n\n\n\n\nConvolutional needs lees parameters as their inputs are not fully connected to every neuron see pic above\n\n\n\n\n\n3 Convolutional is worse than 11 Convolutional, because the latter is more deeper"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#neural-network-architectures-jungle",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#neural-network-architectures-jungle",
    "title": "Deep Feedforward Networks",
    "section": "18 Neural Network architectures (jungle)",
    "text": "18 Neural Network architectures (jungle)"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#computational-graph",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#computational-graph",
    "title": "Deep Feedforward Networks",
    "section": "19 Computational graph",
    "text": "19 Computational graph\n\n\n\n\n\n19.1 Example\n\n\n\n\nProblem with activation function ReLU"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#chain-rule-of-calculus",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#chain-rule-of-calculus",
    "title": "Deep Feedforward Networks",
    "section": "20 Chain Rule of Calculus",
    "text": "20 Chain Rule of Calculus"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#the-jacobian",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#the-jacobian",
    "title": "Deep Feedforward Networks",
    "section": "21 The Jacobian",
    "text": "21 The Jacobian\n\n\n\n\n\n\n\n\n\nThe Jacobian measures how a change in the input changes the output\nThe shape of the Jacobian is ouputs x inputs"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#computing-gradients-in-complex-functions-chain-rule",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#computing-gradients-in-complex-functions-chain-rule",
    "title": "Deep Feedforward Networks",
    "section": "22 Computing gradients in complex functions: Chain rule",
    "text": "22 Computing gradients in complex functions: Chain rule"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#chain-rule-and-tensors-intuitively",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#chain-rule-and-tensors-intuitively",
    "title": "Deep Feedforward Networks",
    "section": "23 Chain rule and tensors, intuitively",
    "text": "23 Chain rule and tensors, intuitively"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#example-1",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#example-1",
    "title": "Deep Feedforward Networks",
    "section": "24 Example",
    "text": "24 Example"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#backpropagation-chain-rule",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#backpropagation-chain-rule",
    "title": "Deep Feedforward Networks",
    "section": "25 Backpropagation Chain Rule",
    "text": "25 Backpropagation Chain Rule\n\n\n\n\n\n\n\n\n\n\n\n25.1 Backpropagation in summary\n\n\n\n\n\n\n\n\nNote: in the figure above, we know what is \\(h_0\\), \\(h_1\\), \\(h_2\\) and \\(L\\) because we have initialized our \\(w_1\\) and \\(w_2\\) and we also know how the sigmoid function works:\n\n\n\n\n\n\nThat means all these values are known so that when we calculate the derivatives below everything is know and then we can use SGD to update the weights\nThen the derivatives wrt. \\(w\\) are:\n\n\n\n\n\n\nThese last equations is what we need to do SGD\n\nBackprogation allows us to generally reduce the amount of space we need in order to compute all the gradients for all the layers, because storing the Jacobian takes a lot of space."
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#chain-rule-visualized",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#chain-rule-visualized",
    "title": "Deep Feedforward Networks",
    "section": "26 Chain rule visualized",
    "text": "26 Chain rule visualized\n\n\n\n\n\nBut now if the ouput is an scalar, we get in the ouput a vector:\n\n\n\n\n\n\nNow from the back to front you are only multiplying vector times matrices instead of large matrices with large matrices"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#but-we-still-need-the-jacobian",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#but-we-still-need-the-jacobian",
    "title": "Deep Feedforward Networks",
    "section": "27 But we still need the Jacobian",
    "text": "27 But we still need the Jacobian"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#computational-graphs-forward-graph",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#computational-graphs-forward-graph",
    "title": "Deep Feedforward Networks",
    "section": "28 Computational graphs: Forward graph",
    "text": "28 Computational graphs: Forward graph"
  },
  {
    "objectID": "blog/2023-11-05_deep-feedforward-networks/index.html#computational-graphs-forward-graph-1",
    "href": "blog/2023-11-05_deep-feedforward-networks/index.html#computational-graphs-forward-graph-1",
    "title": "Deep Feedforward Networks",
    "section": "29 Computational graphs: Forward graph",
    "text": "29 Computational graphs: Forward graph"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html",
    "title": "Deep Learning Optimizations I",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 8, 2023"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#optimizing-neural-networks",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#optimizing-neural-networks",
    "title": "Deep Learning Optimizations I",
    "section": "1 Optimizing neural networks",
    "text": "1 Optimizing neural networks\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#lecture-overview",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#lecture-overview",
    "title": "Deep Learning Optimizations I",
    "section": "2 Lecture overview",
    "text": "2 Lecture overview\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#optimization-us.-learning",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#optimization-us.-learning",
    "title": "Deep Learning Optimizations I",
    "section": "3 Optimization US. Learning",
    "text": "3 Optimization US. Learning\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#risk-minimization",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#risk-minimization",
    "title": "Deep Learning Optimizations I",
    "section": "4 Risk minimization",
    "text": "4 Risk minimization\n\n\n\n\nSlide 5\n\n\n\n\nOmega is a regularization factor\nY_hat is the prediction, each module i.e h1 comes with a set of parameters\nThe expectation is taken over the true data distribution which is not available. Then how do we put this into practice ## Minimizing the empirical risk, –&gt; minimise “loss”\n\n\n\n\nSlide 6\n\n\n\n\n\nWe give up to find the real minimum of the data because we have not access to all the data in the world\nWe take a look at what we have: training data aka empirical data distribution.\n\nThe loss is for a single sample and the empirical risk is the loss over the whole data set. We are not optimizing the real risk but we are optimizing the empirical risk which is an estimate for the real risk. If you take that for a a sample, then a sample, or a batch, we call it loss.\nWe take an step in the direction of minimizing the loss that is in the negative gradient of the loss\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a simple quadratic loss function\ndef loss_function(x):\n    return x**2\n\n# Define the derivative of the loss function (gradient)\ndef gradient(x):\n    return 2 * x\n\n# Generate x values\nx_values = np.linspace(-5, 5, 100)\n\n# Compute corresponding y values for the loss function\ny_values = loss_function(x_values)\n\nplt.figure(figsize=(4, 3))\n\n# Plot the loss function\nplt.plot(x_values, y_values, label='Loss Function')\n\n# Choose a point on the curve\nx_point = 3\ny_point = loss_function(x_point)\n\n# Plot the point on the curve\nplt.scatter(x_point, y_point, color='red', label='Current Point')\n\n# Compute the gradient at the chosen point\ngrad_at_point = gradient(x_point)\n\n# Plot the gradient vector as an arrow\narrow_start = (x_point, y_point)\narrow_end = (x_point - grad_at_point, y_point - grad_at_point**2)\nplt.arrow(*arrow_start, *(np.array(arrow_end) - np.array(arrow_start)),\n          color='green', width=0.1, head_width=0.5, head_length=0.5, length_includes_head=True, label='Gradient')\n\n# Add labels and legend\nplt.xlabel('Model Parameter')\nplt.ylabel('Loss')\nplt.title('Gradient Direction in Steepest Ascent')\nplt.grid(alpha=0.1)\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\n\nIn the context of optimization and gradient descent:\n\nSteepest Ascent: The gradient of the loss function at a particular point indicates the direction in which the function increases the most rapidly. If you were to take a step in the direction of the gradient, you would be moving uphill along the loss function. This is why it’s often referred to as the direction of “steepest ascent” because the function value increases most quickly in that direction.\nSteepest Descent: Conversely, to minimize the loss function, we move in the opposite direction of the gradient. This is called the direction of “steepest descent” because it leads us downhill along the loss function, toward lower values.\n\n“Loss increases the fastest,” means that if you move in the direction of the gradient, you are moving in the direction where the loss function grows most rapidly, or where the function value increases the fastest so moving uphill. However, during optimization, we take steps in the opposite direction to decrease the loss and reach a minimum."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent",
    "title": "Deep Learning Optimizations I",
    "section": "5 Gradient descent",
    "text": "5 Gradient descent\n\n\n\n\nSlide 7\n\n\n\n\nSGD: for mini-batch, in textbooks they say is for only one sample, but in practice you do SGD in a mini-batch, here the samples you picked they are random thus stochastic gradient descent"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#batch-gradient-descent-for-neural-nets-loss-surfaces",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#batch-gradient-descent-for-neural-nets-loss-surfaces",
    "title": "Deep Learning Optimizations I",
    "section": "6 Batch gradient descent for neural nets’ loss surfaces",
    "text": "6 Batch gradient descent for neural nets’ loss surfaces\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#httpslosslandscape.comexplorer",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#httpslosslandscape.comexplorer",
    "title": "Deep Learning Optimizations I",
    "section": "7 https://losslandscape.com/explorer",
    "text": "7 https://losslandscape.com/explorer\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent-vs.-stochastic-gradient-descent",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent-vs.-stochastic-gradient-descent",
    "title": "Deep Learning Optimizations I",
    "section": "8 Gradient descent vs. Stochastic Gradient Descent",
    "text": "8 Gradient descent vs. Stochastic Gradient Descent\n\n\n\n\nSlide 10\n\n\n\n\nWhy dont we use the whole dataset to compute gradient descent?\nUsing the entire dataset to compute the gradient at each step of the optimization process can be computationally expensive, especially when dealing with large datasets. Also, in many cases, using the entire dataset in every iteration introduces redundancy because the information contained in the dataset might already be captured in the gradient computed from a smaller subset.\nFor NNs, using SGD are NOT guaranteed to find the global minimum"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-properties",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-properties",
    "title": "Deep Learning Optimizations I",
    "section": "9 SGD properties",
    "text": "9 SGD properties\n\n\n\n\nSlide 11\n\n\n\n\nSGD estimates the gradient. And if you do any estimation, you can calculate the estandard error, as the std deviation \\(\\sigma\\) divided by the sqrt of the batch size\nFor ie, if you want to get an estimate of the gradient which is twice as good we need 4 times more data"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quiz",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quiz",
    "title": "Deep Learning Optimizations I",
    "section": "10 Quiz",
    "text": "10 Quiz\n\n\n\n\nSlide 12\n\n\n\n\n\nThe fact that videos are correlated i.e in 30fps it does have any to do with the batch size, it actually require higuer batch size because you have a lot less happening so that means all of your data samples are highly correlated. If you data samples are highly correlated that means you are actually very bias i.e. if you are training with a sampled dataset from those biased data then your gradient would be much more worse than if you sample randomly from all classes\nThis is True, it depends on the GPU. For instance for videos, a single sample may contain 30 frames so that means for a batch size not the individual number of pixels but the number of datasamples, and if you datasample its two seconds long and each second contains 30frames so 30 images, then you are basically reducing the batch size by 60 already. Another thing that lead to be GPU not able to handled is because to process videos the architecture takes more to compute, so there is a lot more computation happening.\nVideo in DL are currently handled by using small resolution i.e 100x100. Images are usually 224x224 or if you do object detection then images can go up to 1000x1000 pixels. So that means videos are lower resolutions\n\nYou can compute the variance in terms of low level statistics, like RGB but that does not mean anything about the variance within the NN, and if you need to do a whole forward pass before doing a backward pass then it becomes slow."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-properties-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-properties-1",
    "title": "Deep Learning Optimizations I",
    "section": "11 SGD properties",
    "text": "11 SGD properties\n\n\n\n\nSlide 13\n\n\n\n\nyou can keep suffleing after every epoch, as one epoch is defined as you go through the training set once completely\nRandomness leads to randomness in the class and the data. That means this help us not just optimizing for one or two classes and then in the next batch optimize for one or two classes and then leads you to jump around but instead lead to decreased a good estimate for the wholedataset"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#batch-size",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#batch-size",
    "title": "Deep Learning Optimizations I",
    "section": "12 Batch size",
    "text": "12 Batch size\n\n\n\n\nSlide 14\n\n\n\n\nIt is more efficient than in multiple linear passes because it is more efficient to do it in one fully connected linear layer.\nSo one big data multiplication is faster than doing two small matrix multiplications\nSmall batches usually add more noise to the learning process, therefore can get stuck less in local minimas and sometimes can lead to better performance but this is not supper common.\nWith the increase of batch size the effective gradient would be less strong because is a better estimation across all samples so to make the model still train as quickly is we need to increase th learning rate"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#why-does-mini-batch-sgd-work",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#why-does-mini-batch-sgd-work",
    "title": "Deep Learning Optimizations I",
    "section": "13 Why does mini-batch SGD work?",
    "text": "13 Why does mini-batch SGD work?\n\n\n\n\nSlide 15\n\n\n\n\nREducing the sample size does not imply reduced gradient wuality that is because the training samples could be noisy, or have biases or outliers (recall in every dataset you have lots of mislabel classes) so these noisy data allows you not to get stuck in local minimas while the real gradient may do this.\nFor i.e if the real gradient points in one direction and you are stuck in a local minima thne it wont help you because the the gradient descent for the whole set only have points in one direction at a certain location and if it happens to be a place where you get stuck htne you are stuck. This is contrary to stochastic where you keep taking random samples, yeah you can be stuck for the next 20 steps where you pick random samples but then you pick something that migh hav some outlier and now suddenly the gradient is pretty large and you end up scapping this local minima."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization",
    "title": "Deep Learning Optimizations I",
    "section": "14 Stochastic gradient-based optimization",
    "text": "14 Stochastic gradient-based optimization\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-1",
    "title": "Deep Learning Optimizations I",
    "section": "15 Stochastic gradient-based optimization",
    "text": "15 Stochastic gradient-based optimization\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-2",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-2",
    "title": "Deep Learning Optimizations I",
    "section": "16 Stochastic gradient-based optimization",
    "text": "16 Stochastic gradient-based optimization\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-3",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-3",
    "title": "Deep Learning Optimizations I",
    "section": "17 Stochastic gradient-based optimization",
    "text": "17 Stochastic gradient-based optimization\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-4",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#stochastic-gradient-based-optimization-4",
    "title": "Deep Learning Optimizations I",
    "section": "18 Stochastic gradient-based optimization",
    "text": "18 Stochastic gradient-based optimization\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent-vs.-stochastic-gradient-descent-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent-vs.-stochastic-gradient-descent-1",
    "title": "Deep Learning Optimizations I",
    "section": "19 Gradient descent vs. stochastic gradient descent",
    "text": "19 Gradient descent vs. stochastic gradient descent\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent-vs.-stochastic-gradient-descent-2",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#gradient-descent-vs.-stochastic-gradient-descent-2",
    "title": "Deep Learning Optimizations I",
    "section": "20 Gradient descent vs. stochastic gradient descent",
    "text": "20 Gradient descent vs. stochastic gradient descent\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#in-a-nutshell",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#in-a-nutshell",
    "title": "Deep Learning Optimizations I",
    "section": "21 In a nutshell",
    "text": "21 In a nutshell\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#lets-see-this-in-practice",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#lets-see-this-in-practice",
    "title": "Deep Learning Optimizations I",
    "section": "22 Let’s see this in practice",
    "text": "22 Let’s see this in practice\n\n\n\n\nSlide 24"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#httpsplayground.tensorflow.or",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#httpsplayground.tensorflow.or",
    "title": "Deep Learning Optimizations I",
    "section": "23 https://playground.tensorflow.or",
    "text": "23 https://playground.tensorflow.or\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#challenges-in-optimization",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#challenges-in-optimization",
    "title": "Deep Learning Optimizations I",
    "section": "24 Challenges in optimization",
    "text": "24 Challenges in optimization\n\n\n\n\nSlide 26\n\n\n\n\nNN training is non-covex optimization, these functions have lots of local optima, but we do not care about the global minimum because we care how it will perform in real life data. We care something that is optimum and that generalizes well."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#why-are-nn-losses-not-convex",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#why-are-nn-losses-not-convex",
    "title": "Deep Learning Optimizations I",
    "section": "25 Why are NN losses not convex?",
    "text": "25 Why are NN losses not convex?\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#challenges-in-optimization-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#challenges-in-optimization-1",
    "title": "Deep Learning Optimizations I",
    "section": "26 Challenges in optimization",
    "text": "26 Challenges in optimization\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning",
    "title": "Deep Learning Optimizations I",
    "section": "27 1. Ill-conditioning",
    "text": "27 1. Ill-conditioning\n\n\n\n\nSlide 29\n\n\n\n\nThe Hessian at a particular point in the function measures how well you can fit a quadratic function to this point.\nThe Jacobian measure how well you can fit a plane trough that point."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-1",
    "title": "Deep Learning Optimizations I",
    "section": "28 1. Ill-conditioning",
    "text": "28 1. Ill-conditioning\n\n\n\n\nSlide 30\n\n\n\n\nSo curvature is determine by the second derivative so its determined by the Hessian\nBottom Left plot:\n\nIf you have a negative curvature that means the cost function decreases faster than the gradient predicts.\n\nBottom Right plot:\n\nIf you have a positive curvature that means the cost function decreases slower than expected and eventually starts to increase"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-2",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-2",
    "title": "Deep Learning Optimizations I",
    "section": "29 1. Ill-conditioning",
    "text": "29 1. Ill-conditioning\n\n\n\n\nSlide 31\n\n\n\n\nCritical points are where, the gradient is zero and you can define with the Hessian what kind of critical points they are, you have a local minimum, a local maximum and saddle point\nMost points in high dimensions are saddle points, this is beccause it becomes expontially (\\(2^n\\) combinations of th signs) to have at least one positive and at least one negative. So this is more likely."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#linear-algebra-recap",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#linear-algebra-recap",
    "title": "Deep Learning Optimizations I",
    "section": "30 Linear Algebra Recap:",
    "text": "30 Linear Algebra Recap:\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-3",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-3",
    "title": "Deep Learning Optimizations I",
    "section": "31 1. Ill-conditioning",
    "text": "31 1. Ill-conditioning\n\n\n\n\nSlide 33\n\n\n\n\n\n\n\n\n\n\nCondition number\n\n\n\n\n\nThe condition number of a matrix is a measure of how sensitive the matrix is to changes in its input. A high condition number indicates that the matrix is ill-conditioned, which can lead to numerical instability. In the context of optimization problems like gradient descent, ill-conditioned matrices can slow down convergence and make the optimization process more sensitive to small changes in the input.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define a quadratic loss function\ndef loss_function(x):\n    return x**2\n\n# Define the derivative of the loss function (gradient)\ndef gradient(x):\n    return 2 * x\n\n# Define the second derivative of the loss function (Hessian)\ndef hessian(x):\n    return 2 * np.ones_like(x)\n\n# Generate x values\nx_values = np.linspace(-5, 5, 100)\n\n# Set up subplots\nfig, axes = plt.subplots(1, 2, figsize=(7, 3))\n\n# Plot the loss function and its gradient for different condition numbers\nfor i, condition_number in enumerate([1, 10]):\n    hessian_values = condition_number * hessian(x_values)\n\n    # Compute the gradient at the chosen point\n    x_point = 3\n    grad_at_point = gradient(x_point)\n\n    # Compute the change in x\n    delta_x = np.linspace(-1, 1, 100)\n\n    # Compute the change in y based on the quadratic loss function\n    delta_y = 0.5 * hessian_values[x_point] * delta_x**2\n\n    # Plot the loss function\n    axes[i].plot(x_values, loss_function(x_values), label='Loss Function')\n\n    # Plot the gradient vector as an arrow\n    axes[i].arrow(x_point, loss_function(x_point), -grad_at_point, 0,\n                  color='green', width=0.1, head_width=0.5, head_length=0.5, length_includes_head=True, label='Gradient')\n\n    # Plot the change in x and corresponding change in y\n    axes[i].plot(x_point + delta_x, loss_function(x_point) + delta_y, '--', label='Approximation')\n\n    axes[i].set_xlabel('Model Parameter')\n    axes[i].set_ylabel('Loss')\n    axes[i].set_title(f'Condition Number = {condition_number}')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nAn ill-conditioned problem, system, or matrix refers to a situation where small changes or perturbations in the input data or parameters can lead to large changes in the output or solution. In the context of linear algebra and optimization, the condition number is a measure of how sensitive a mathematical problem is to changes in its input.\nMathematically, the condition number of a matrix \\(A\\) is defined as the product of the matrix norm and the norm of its inverse. It is denoted as \\(\\text{cond}(A) = \\|A\\| \\cdot \\|A^{-1}\\|\\).\n\nIf \\(\\text{cond}(A)\\) is close to 1, the matrix is well-conditioned.\nIf \\(\\text{cond}(A)\\) is much greater than 1, the matrix is ill-conditioned.\n\nAn ill-conditioned matrix is problematic for several reasons:\n\nSensitivity to Input Perturbations: Small changes in the input data or parameters can result in large changes in the solution, making the problem numerically unstable.\nNumerical Instability: In numerical computations, ill-conditioning can lead to loss of precision, rounding errors, and difficulties in obtaining accurate solutions.\nSlow Convergence: In optimization problems, ill-conditioning can slow down the convergence of iterative optimization algorithms like gradient descent.\nNumerical Issues: When solving linear systems or performing matrix inversion, ill-conditioned matrices can lead to numerical instability and inaccurate results.\n\nIn the context of optimization problems, the Hessian matrix (second derivative of the loss function) plays a crucial role. If the Hessian matrix is ill-conditioned, it can make optimization algorithms more sensitive to the choice of step size and direction, potentially leading to slow convergence or convergence to suboptimal solutions.\nAddressing ill-conditioning often involves using regularization techniques, preconditioning, or carefully selecting optimization algorithms that can handle such numerical challenges.\n\n\n\nYou can think of how much the matrix distort the space.\nWhy is now a large condition number bad? - In the case of a small condition number, going to the local minimum is quite straight forward, but in the case of a very bad conditioned number you know where to go because the gradient tells you this but you do not know the right step size. You can keep oscillating, which slows the convergence of the algorithm.\nWith a large condition number, Gradient Descent performs poorly because it is difficult to make a good step size.\nSo it is like optimizing one direction at the time if you are luckily but the ill-conditioning to the point that you will overshoot"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-4",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ill-conditioning-4",
    "title": "Deep Learning Optimizations I",
    "section": "32 1. Ill-conditioning",
    "text": "32 1. Ill-conditioning\n\n\n\n\nSlide 34\n\n\n\n\nif in the tailor expansion we see that the curvature term is higher than the linear component of the Tailor function then, taking a small step size (updating w) it will increase the loss instead of decreasing it. So we end up with a higher loss."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#local-minima",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#local-minima",
    "title": "Deep Learning Optimizations I",
    "section": "33 2. Local minima",
    "text": "33 2. Local minima\n\n\n\n\nSlide 35\n\n\n\n\nModel identifiability deals with the ability to uniquely determine the values of model parameters based on the available data. In other words, a model is identifiable if the true values of its parameters can be uniquely recovered or estimated from observed data.\nIf you have a network that is equivalent between different sets of parameters you can switch them around."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#local-minima-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#local-minima-1",
    "title": "Deep Learning Optimizations I",
    "section": "34 2. Local minima",
    "text": "34 2. Local minima\n\n\n\n\nSlide 36\n\n\n\n\nThese are the one we would have trouble (bottom left)\nyou reach some place and you are stuck in this place but there is a much better solution in the loss surface. That is why noisy SGD works better"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#local-minima-tricky-thing-about-blindness",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#local-minima-tricky-thing-about-blindness",
    "title": "Deep Learning Optimizations I",
    "section": "35 2. Local minima: tricky thing about “blindness”",
    "text": "35 2. Local minima: tricky thing about “blindness”\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ravines",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#ravines",
    "title": "Deep Learning Optimizations I",
    "section": "36 3. Ravines",
    "text": "36 3. Ravines\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#plateausflat-areas",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#plateausflat-areas",
    "title": "Deep Learning Optimizations I",
    "section": "37 3. Plateaus/Flat areas",
    "text": "37 3. Plateaus/Flat areas\n\n\n\n\nSlide 39\n\n\n\n\nIn these surfaces you have zero gradients, no updates, no learning. If you have converged and that minimum tends to be flat those tend to be the ones that generalized better to new data"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quizz",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quizz",
    "title": "Deep Learning Optimizations I",
    "section": "38 Quizz",
    "text": "38 Quizz\n\n\n\n\nSlide 40\n\n\n\n\nWhy does flat minima generalize better on test data?\n\nWe cannot say that generally they have lower loss values\nIt is a true statement but it is not the reason. So it does not matter whether you add more precision. For i.e you can add float64 instead and still does not change things\nTrue\nWe do not use test for training"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#why-are-flat-minima-preferred",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#why-are-flat-minima-preferred",
    "title": "Deep Learning Optimizations I",
    "section": "39 Why are “flat” minima preferred?",
    "text": "39 Why are “flat” minima preferred?\n\n\n\n\nSlide 41\n\n\n\n\nWhy are they less likely to be the result of overfilling to train distribution?\nsmall batches tend to converge to flat minimizers that have small eigenvalues of the Hessian"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#flat-areas-steep-minima",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#flat-areas-steep-minima",
    "title": "Deep Learning Optimizations I",
    "section": "40 4. Flat areas, steep minima",
    "text": "40 4. Flat areas, steep minima\n\n\n\n\nSlide 42\n\n\n\n\nIf you have lots of flat areas with very steep minima, for example when you have logits that are scaled by a very very small number that means these numbers tend to be extremelly higuer, so they are almost on-hot like and at that point you are not getting much of the gradients from the other classes anymore.\nTherefore by changing the temperature you can change how wide these locals deeps are."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#cliffs-and-exploding-gradients",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#cliffs-and-exploding-gradients",
    "title": "Deep Learning Optimizations I",
    "section": "41 4. Cliffs and Exploding Gradients",
    "text": "41 4. Cliffs and Exploding Gradients\n\n\n\n\nSlide 43\n\n\n\n\nWe clip it meaning we set the gradient of this eta to a treshold, you still are going in the same direction but now witch a scaled version. Escentially you reduce the size of the gradient"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#long-term-dependencies",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#long-term-dependencies",
    "title": "Deep Learning Optimizations I",
    "section": "42 5. Long term dependencies",
    "text": "42 5. Long term dependencies\n\n\n\n\nSlide 44\n\n\n\n\nThis is related to Recurrent NNs where you apply a matrix W^t over and over again, so you apply it to the input multiple times. Then you can decompose the matrix, in this case t stands for the power of t and you can see that if you apply this eigen value decomposition is simple the eigenvalues are taken to the power of t. So if you have eigenvalues larger than one then they will get insanely high, but if they are small then these eigenvalues will plummed to zero, almost vanishing.\nAs a product we would have a training-trajectory dependency which would be hard to recover from a bad start, IF you keep applying the same weights\nExample, in time series you use Recurrent NNs and for example your prediction at time 30 depends on predition at time zero. These are the long term dependencies are.\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#revisit-of-gradient-descent",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#revisit-of-gradient-descent",
    "title": "Deep Learning Optimizations I",
    "section": "43 Revisit of gradient descent",
    "text": "43 Revisit of gradient descent\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#setting-the-learning-rate",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#setting-the-learning-rate",
    "title": "Deep Learning Optimizations I",
    "section": "44 Setting the learning rate",
    "text": "44 Setting the learning rate\n\n\n\n\nSlide 47\n\n\n\n\ngenerally we go from a high to low lr either by:\n\nstep decay: ie divide lr by 10 every x number of epochs\ngradually going low the learning rate\n\nThe heuristics for this is that you first will find some general area in which the loss is pretty good but then this large learning step size keeps jumping around the local minimum, and now if you decrease the learning rate now it can optimize whithin this valley, and whithin this value can go further down"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#advanced-optimizers",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#advanced-optimizers",
    "title": "Deep Learning Optimizations I",
    "section": "45 Advanced optimizers",
    "text": "45 Advanced optimizers\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum.",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum.",
    "title": "Deep Learning Optimizations I",
    "section": "46 Momentum.",
    "text": "46 Momentum.\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum-designed-to-accelerate-learning-especially-when-loss-is-high-curvature",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum-designed-to-accelerate-learning-especially-when-loss-is-high-curvature",
    "title": "Deep Learning Optimizations I",
    "section": "47 Momentum: designed to accelerate learning, especially when loss is high curvature",
    "text": "47 Momentum: designed to accelerate learning, especially when loss is high curvature\n\n\n\n\nSlide 50\n\n\n\n\nTo mimic the momentum we need to recall the exponentially weighted moving averages are: - You have your noisy data and you want to approximate some smooth average of this"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum",
    "title": "Deep Learning Optimizations I",
    "section": "48 Momentum",
    "text": "48 Momentum\n\n\n\n\nSlide 51\n\n\n\n\nA higher Beta means that the algorithm relies more heavily on the past gradients.\nA value too close to 1 might lead to not adapting quickly enough to changes, while a value too low might lead to too much fluctuation in the gradients\nIt is easy to compute it recursively: Take your previous point * Beta + (1-Beta)*current_observation.\nYou can compute this from left to right so that you dont keep track of the points.\nHere the momentum is of your previous gradient Here we notice that if you have:\n\nLarge beta then it is more smoother so it tends to have more momentum so it is basically tends to be the values that you have seen\nSmaller beta so less smooth, which means that it reacts more to the current observation\n\nBecause we set V_0 = 0 then it will always be baised towards V_0, but you can correct for thi using the formulas above"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#momentum-1",
    "title": "Deep Learning Optimizations I",
    "section": "49 Momentum",
    "text": "49 Momentum\n\n\n\n\nSlide 52\n\n\n\n\nWhat moving average tends to, it basically removes the effect of stuff that is way in the past. For example if you have beta=0.9 if you do 0.9^10 then you already end up at 0.35. So even if you have a high Beta like 0.98 after 50 steps it basically decays so it onluy has the effect of dividing 1/e"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum",
    "title": "Deep Learning Optimizations I",
    "section": "50 SGD with momentum",
    "text": "50 SGD with momentum\n\n\n\n\nSlide 53\n\n\n\n\nYou do not want be switching directions all the time, this is what happens in the half pipe, you want to mantain the momentum from previous updates, so now you will see that\nNow we have in addition the gamma*v_t, this is imply the gradient from the last steps. You can see how much gradients from the last step is taken\nFor isntance if gamma is 0.9 and then we say v_0 = 0. Then the first v1, so the first step is just the normal update.\nFor v2, now we take the gradient at update 2 plus the term 0.9*gradient_1 which is from the previous step.\nFor v3 we take more into account the current gradient3 but also the previous gradients. It is only changing the direction of where we are going so now we do not only consider the current gradient at tha point but also the previous gradients causing momentum"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum-1",
    "title": "Deep Learning Optimizations I",
    "section": "51 SGD with Momentum",
    "text": "51 SGD with Momentum\n\n\n\n\nSlide 54\n\n\n\n\nHere rho = gamma, give us the friction of how much we can change direction. It is friction because where if we have it as 0 then we are only calculating one sample so no velocity, if we let it to be o.9 then we have more velocity.\nThis cancels out oscillating gradients. And it gives more weight to the recent updates, this leads to much faster convergence"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum-2",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum-2",
    "title": "Deep Learning Optimizations I",
    "section": "52 SGD with momentum",
    "text": "52 SGD with momentum\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum-3",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-momentum-3",
    "title": "Deep Learning Optimizations I",
    "section": "53 SGD with momentum",
    "text": "53 SGD with momentum\n\n\n\n\nSlide 56\n\n\n\n\nFor the parameter update:\n\nThe momentum term increases for dimensions where the gradients points in the same direction. One way that we have extracted away from this formulas is that this formulas is that the gradient is multidimensional for every parameter. So it means if your gradient for one particular weight keeps going left and right, left and right it averages out to not taking a step in this direction, but if it keep going left left and left, then it will stay that way and take more steps in this direction.\n\nSo in this case, it will keep going left and right but it will always in the one direction will keep up going down and this will gradually build up and go downwards.\nAll optimizers use momentum, clipping is less common, momentum nothing is done without it. Clipping is when for some instance your gradients explode for some reason."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#nesterov-momentum",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#nesterov-momentum",
    "title": "Deep Learning Optimizations I",
    "section": "54 Nesterov momentum",
    "text": "54 Nesterov momentum\n\n\n\n\nSlide 57\n\n\n\n\nThis is an extension to momemntum, so in the case for nesterov momentum, we use the future gradient instead of the current gradient\nAnother approach is first take the step that momentum tell us and at this point calculate the gradient, and then add this together So now the gradient is not computed at the current location but at the current location + the step we go according to the momentum\nThis should gives a better approximation of to the gradient, because we are going in that direction anyways so how about we first apply the momentum and then calculate the gradient\nThis results in better responsiveness and better guarantees"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#nesterov-momentum-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#nesterov-momentum-1",
    "title": "Deep Learning Optimizations I",
    "section": "55 Nesterov momentum",
    "text": "55 Nesterov momentum\n\n\n\n\nSlide 58\n\n\n\n\nBlue would be what normal gradient descent does with standard momentum. You can think of Nesterov as a correction that you do to. the typically momentum, So you take momentum and you take the gradient that you have at that location.\nIn practice not use too often"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-adaptive-step-sizes",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#sgd-with-adaptive-step-sizes",
    "title": "Deep Learning Optimizations I",
    "section": "56 SGD with adaptive step sizes",
    "text": "56 SGD with adaptive step sizes\n\n\n\n\nSlide 59\n\n\n\n\nWhat is in practice very common is SGD with adaptive step sizes\n\nLearning rates directly affect the step size\n\nIn NNs for clasifying dogs, the lower layers classify rgbs pixels the upper classify whether is a dog, so instead of having equal learning rates for all modules why not having learning rates per parameter. We can use this by using the following: Adagrad … see next slides"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adagrad",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adagrad",
    "title": "Deep Learning Optimizations I",
    "section": "57 Adagrad",
    "text": "57 Adagrad\n\n\n\n\nSlide 60\n\n\n\n\nHere we adapt the learning rate per component, so for every paramter it adapts the learning rate to incorporate the knowledge of the past observations\nHere the dot in a circle represent the element wise product, r is the gradient for this parameter summed up over time, so if summed overtime it keeps getting bigger. So that means the gradient according with time just keeps getting lower.\nSo the parameters that have large gradients quickly decrease in effective learning rate because r would be very big so the term in from of the gradient would be very small, because we have eta/r. and we saw that r was big, so eta/r would be small\n\nRapid decrease in learning rate for parameters with large partial derivatives\n\nSo the parameters that have large gradients will not be updated anymore because we will have the learning rate eta/r to be close to zero like 0.00001..\n\nSmaller decrease in learning rate for parameters with small partial derivatives"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adagrad-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adagrad-1",
    "title": "Deep Learning Optimizations I",
    "section": "58 Adagrad",
    "text": "58 Adagrad\n\n\n\n\nSlide 61\n\n\n\n\nso here with Adadelta:\n\nThere is another extension to this which seeks to reduce its aggressive, monotonically decreasing learning rate. This could be somewhat problematic because if you do not finish by \\(x\\) number of steps then all of your learning rates will be zero so Adadelta simply makes a sliding winwdow instead to use past gradients. It does so by restricting the window of acummulated past gradients to some fixed size instead of acummulating all past squared gradients.\nWe do not need to set a default learning rate. as it has been eliminated from the updated rule"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#rmsprop",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#rmsprop",
    "title": "Deep Learning Optimizations I",
    "section": "59 RMSprop",
    "text": "59 RMSprop\n\n\n\n\nSlide 62\n\n\n\n\n\nIt is just a modification of Adagrad, and it simply uses the exponentially weighted average to accumulate gradients\n\nSo before r was jus the sum of square gradients, now we take the exponentially weighed average. we can also use standard momentum and Nesterov momentum and so on."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#rmsprop-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#rmsprop-1",
    "title": "Deep Learning Optimizations I",
    "section": "60 RMSprop",
    "text": "60 RMSprop\n\n\n\n\nSlide 63\n\n\n\n\n\nLarge gradients here the updates are are detained, tammed, interrumped,\nSmall gradients here the updates are are exacerbated, more aggresive"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adam",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adam",
    "title": "Deep Learning Optimizations I",
    "section": "61 Adam",
    "text": "61 Adam\n\n\n\n\nSlide 64\n\n\n\n\n\ncoombines RMSProp and momentum\nuses adaptive learning rate for each parameter (higher memory)\nIt keeps an exponentially decaying average of past gradient like momentum\nIt introduces bias correction terms i.e if we have smoothing average, it tends to have some dependency for the first v_0, but at the beginning you dont have a really first value so we need to set it to zero which is a bias, but we can have a formula that compensates for this.\nit is more popular specially for transformers architecture\nso popular that is not even cited"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adam-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#adam-1",
    "title": "Deep Learning Optimizations I",
    "section": "62 Adam",
    "text": "62 Adam\n\n\n\n\nSlide 65\n\n\n\nMomentum —&gt; Adagrad —&gt; RMSprop\n\n  \n\n\nWe have the exponentiall average of gradients so the \\(\\sqrt{v_t}\\), the square is use to rescale again one learning rate at the beginning and then automatic learning rates per parameter and then the moving average of gradient is use for the update itself.\nWhat is new is the combination fo the two."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#notice-something",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#notice-something",
    "title": "Deep Learning Optimizations I",
    "section": "63 Notice something?",
    "text": "63 Notice something?\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#visual-overview",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#visual-overview",
    "title": "Deep Learning Optimizations I",
    "section": "64 Visual overview",
    "text": "64 Visual overview\n\n\n\n\nSlide 67\n\n\n\n\nAdam is a heavy ball with a lot of friction and all the other, like the yellow one tends to overshoot a lot and adam introduces this friction term to the optimizer"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#which-optimizer-to-use",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#which-optimizer-to-use",
    "title": "Deep Learning Optimizations I",
    "section": "65 Which optimizer to use?",
    "text": "65 Which optimizer to use?\n\n\n\n\nSlide 68\n\n\n\n\n\nTypically SGD + momentum often works best\nAdam is often the easy choice but it tends to not perform best.\nAdam + weigth decay is standard for optimizing transformers\nEven in optimizers like Adam we do learning rate decay"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#approximate-second-order-methods",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#approximate-second-order-methods",
    "title": "Deep Learning Optimizations I",
    "section": "66 Approximate Second-Order Methods",
    "text": "66 Approximate Second-Order Methods\n\n\n\n\nSlide 69\n\n\n\n\nThis is another whole level of optimizers, these are of second order:\nThis does not look only at the gradeitn but try to see how is the gradient changing at this location. It is trying to get some approximations to the Hessian just to haver a feeling of where to go.\nWe will only talk about Newtons Method"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#newtons-method",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#newtons-method",
    "title": "Deep Learning Optimizations I",
    "section": "67 Newton’s method",
    "text": "67 Newton’s method\n\n\n\n\nSlide 70\n\n\n\n\nWe approximate the gradient at some point with Taylor expansion - Now if we want to solve for the critical point, which means the gradient is zero, then we get the second eq as an update formula. - If the function is like quadratic becuase we are learning a quadratic approximation to the function Newton method will only need one step directly to get to the solution - If is convex but not quadratic, we keep on iterating and it will get us to the minimum"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#newtons-method-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#newtons-method-1",
    "title": "Deep Learning Optimizations I",
    "section": "68 Newton’s method",
    "text": "68 Newton’s method\n\n\n\n\nSlide 71\n\n\n\n\n\nOnly works if Hessian is positive definite, if near saddle point, the Hessian are not all positive. So it does not work.\nThe solution for this is to add an identity matrix times this expression and then solve for the update\nStill computationally expensive"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quasi-newton-methods",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quasi-newton-methods",
    "title": "Deep Learning Optimizations I",
    "section": "69 Quasi-Newton methods",
    "text": "69 Quasi-Newton methods\n\n\n\n\nSlide 72\n\n\n\n\nBecause it is very computational expensive people have came with Quasi-Newton methods which trie to reduce the expensive computations of the inversion of the Hessian in the previous method\n\nThey approx these matrices by lower rank matrices, then less storage and complexity\n\nBut not really used"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#interactive-session",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#interactive-session",
    "title": "Deep Learning Optimizations I",
    "section": "70 Interactive session",
    "text": "70 Interactive session\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#reading-materials",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#reading-materials",
    "title": "Deep Learning Optimizations I",
    "section": "71 Reading materials",
    "text": "71 Reading materials\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#how-research-gets-done-at-il-marie-curie.",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#how-research-gets-done-at-il-marie-curie.",
    "title": "Deep Learning Optimizations I",
    "section": "72 How research gets done at Il Marie Curie.",
    "text": "72 How research gets done at Il Marie Curie.\n“Nothing in life is to be\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quiz-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#quiz-1",
    "title": "Deep Learning Optimizations I",
    "section": "73 Quiz",
    "text": "73 Quiz\n\n\n\n\nSlide 76\n\n\n\n\n\nSetting all the weights to “42”\n\nAnswers:\n\nis wrong as setting the weights to zero is hard to learn, but if all same then all evolve in the similar way\nYes, it does train but very slowly because at least the bias will add some variance\nThis works as long as all the neurons are set to some number between 0 and 100, but this does not matter to much so this will be able to train\nSame explanation to 1. The same number then all evolve in the same way and will not train properly"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#re-constant-init-see-tutorial",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#re-constant-init-see-tutorial",
    "title": "Deep Learning Optimizations I",
    "section": "74 Re: constant init: see Tutorial",
    "text": "74 Re: constant init: see Tutorial\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#title",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#title",
    "title": "Deep Learning Optimizations I",
    "section": "75 Title",
    "text": "75 Title\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#weight-initialization",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#weight-initialization",
    "title": "Deep Learning Optimizations I",
    "section": "76 Weight initialization",
    "text": "76 Weight initialization\n\n\n\n\nSlide 79\n\n\n\n\n\n\n\n\n\n\nWhy init weights to zero is bad?\n\n\n\n\n\nThe use of random values for weight initialization in neural networks is a common practice and serves several important purposes in the training process. Here are some reasons why random initialization is preferred:\n\nBreaking Symmetry: If all the weights in a neural network are initialized to the same value, each neuron in a given layer would receive the same input and learn the same features during training. This symmetry problem makes it difficult for neurons to learn diverse and meaningful features. Random initialization breaks this symmetry by providing each neuron with a unique starting point.\nAvoiding Zero Gradients: If all weights are initialized to zero, the gradients with respect to each weight will be the same during backpropagation. This means that all weights will be updated by the same amount in each iteration, leading to symmetrical weight updates and slow convergence. Random initialization ensures that each weight starts with a different value, preventing this issue.\nEncouraging Exploration: Random initialization introduces diversity in the initial state of the neural network, promoting exploration in the weight space. This is particularly important when using optimization algorithms like gradient descent, as it helps the algorithm escape local minima and find better solutions.\nDealing with Dead Neurons: If weights are initialized to zero, neurons in a network with certain activation functions (e.g., ReLU) may become “dead” and stay inactive (always outputting zero) for all inputs. Random initialization helps mitigate this issue, ensuring that neurons have a chance to receive different inputs and learn meaningful features.\nImproving Generalization: Random initialization contributes to the generalization ability of the neural network. Different initializations allow the network to learn diverse representations of the input data, which can lead to better performance on unseen data.\n\nWhat about biases set to zero?\nWhile it’s common to initialize weights with random values, the initialization of biases is often done differently. Setting biases to zero is a common and reasonable practice, and it generally does not lead to the same issues as initializing weights to zero"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#random-yes.-but-how",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#random-yes.-but-how",
    "title": "Deep Learning Optimizations I",
    "section": "77 Random: yes. But how?",
    "text": "77 Random: yes. But how?\n\n\n\n\nSlide 80\n\n\n\n\nBy controlling the spread of initial weights (variance), we aim to avoid extreme values that could hinder the training process.\n\n\n\n\n\n\nWhy do we even want to preserve the variance of the activations?\n\n\n\n\n\nConserving the variance of activations during the training of neural networks is an important consideration for several reasons:\n\nPreventing Vanishing Gradients:\n\nIf the variance of activations becomes too small as the information passes through the layers during forward propagation, it may lead to vanishing gradients during backpropagation.\nVanishing gradients make it challenging for the optimization algorithm to update the weights effectively, hindering the learning process.\n\nPreventing Exploding Gradients:\n\nConversely, if the variance of activations becomes too large, it may lead to exploding gradients during backpropagation.\nExploding gradients can cause the weights to be updated by very large values, leading to numerical instability and making it difficult for the model to converge to a solution.\n\nFacilitating Learning Across Layers:\n\nConserving the variance helps in maintaining a suitable range of activations throughout the layers of the network.\nA consistent variance allows each layer to make meaningful contributions to the learning process, preventing issues where some layers become overly dominant or inactive.\n\nSmoothing the Optimization Landscape:\n\nA stable and consistent variance in activations contributes to a smoother optimization landscape.\nA smoother landscape makes it easier for optimization algorithms to navigate and converge, leading to more stable and efficient training.\n\nEncouraging Exploration and Learning:\n\nA controlled variance ensures that the network can effectively explore the solution space during training.\nThe ability to explore different configurations and update weights based on meaningful gradients helps the model to learn representative features from the data.\n\nBetter Generalization:\n\nMaintaining a reasonable variance helps in producing more robust models that generalize well to unseen data.\nOverly small or large activations may result in a model that is sensitive to minor variations in the training data, leading to poor generalization.\n\nMitigating Sensitivity to Weight Initialization:\n\nA consistent variance makes the training process less sensitive to the specific choice of weight initialization.\nWhen the variance is carefully controlled, the network is more likely to exhibit stable behavior during training, irrespective of the initial weights.\n\n\n\n\n\n\n\n\n\n\n\nWhy do we even mean by variance of the weights?\n\n\n\n\n\nWhen we refer to “variance” in the context of neural networks and weight initialization, we are typically talking about the spread or dispersion of values. Specifically, it refers to the spread of the weights’ initial values. The term “variance” in this context does not directly relate to statistical variance, but rather it’s used in a more general sense to describe the range of values.\nHere’s a breakdown of the concept:\n\nWeight Variance:\n\nEach weight in a neural network has an associated value.\nThe “variance” in weight initialization refers to how spread out or varied these initial weight values are across the neurons in a layer.\n\nConsistent Spread Across Layers:\n\nWhen initializing weights, especially in deep neural networks, it’s desirable to have a consistent spread of initial values across layers.\nThe goal is to avoid situations where the weights in some layers are much larger or smaller than in others.\n\nAvoiding Extreme Values:\n\nExtreme weight values can lead to numerical instability during training, causing issues like exploding or vanishing gradients.\nBy controlling the spread of initial weights (variance), we aim to avoid extreme values that could hinder the training process.\n\nMaintaining Activation Variance:\n\nThe idea is to set the initial weights in a way that the variance of activations (outputs of neurons after applying weights and activation functions) remains reasonably constant across layers.\nThis helps prevent issues like vanishing or exploding gradients, as mentioned earlier."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#bad-initialization-can-cause-problems",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#bad-initialization-can-cause-problems",
    "title": "Deep Learning Optimizations I",
    "section": "78 Bad initialization can cause problems",
    "text": "78 Bad initialization can cause problems\n\n\n\n\nSlide 81\n\n\n\n\nLow variance = high peak\nHigh variance = smooth out bell\nIn the upper row, if we initialize every length with weigths that have same constant variance then in further layers we dimished the variance, so it tends to smooth out –&gt; diminsh variance\nIn the opposite if every layer has an increase variance, then we end up with a very spiky peak because we can can explode the variance in activations"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#initializing-weights-by-preserving-variance",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#initializing-weights-by-preserving-variance",
    "title": "Deep Learning Optimizations I",
    "section": "79 Initializing weights by preserving variance",
    "text": "79 Initializing weights by preserving variance\n\n\n\n\nSlide 82"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#initializing-weights-by-preserving-variance-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#initializing-weights-by-preserving-variance-1",
    "title": "Deep Learning Optimizations I",
    "section": "80 Initializing weights by preserving variance",
    "text": "80 Initializing weights by preserving variance\n\n\n\n\nSlide 83\n\n\n\n\nHere then we are saying that our weight to preserve the variance we will be draw from a Gaussian with mean = 0 and variance = 1/d, where d is the number of input variables to the layer."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#initialization-for-relus",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#initialization-for-relus",
    "title": "Deep Learning Optimizations I",
    "section": "81 Initialization for ReLUs",
    "text": "81 Initialization for ReLUs\n\n\n\n\nSlide 84\n\n\n\n\nFor Relu or variants we use Kaiming\nThe Kaiming initialization sets the initial weights with a variance of 2/n, where n is the number of input units. This choice helps prevent issues like vanishing or exploding gradients, particularly in deep neural networks."
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#xavier-initialization",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#xavier-initialization",
    "title": "Deep Learning Optimizations I",
    "section": "82 Xavier initialization",
    "text": "82 Xavier initialization\n\n\n\n\nSlide 85"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#interesting-results-with-randomly-initialized-networks",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#interesting-results-with-randomly-initialized-networks",
    "title": "Deep Learning Optimizations I",
    "section": "83 Interesting results with randomly initialized networks",
    "text": "83 Interesting results with randomly initialized networks\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-11-08_deep-learning-optimizations-i/index.html#reading-materials-1",
    "href": "blog/2023-11-08_deep-learning-optimizations-i/index.html#reading-materials-1",
    "title": "Deep Learning Optimizations I",
    "section": "84 Reading materials",
    "text": "84 Reading materials\n\n\n\n\nSlide 87"
  },
  {
    "objectID": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html",
    "href": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html",
    "title": "Automating the creation of Blog posts",
    "section": "",
    "text": "A Python solution to create blog posts from the command line\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Extension\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Quarto\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Extension\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Quarto\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 21, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                    \n                 Code\n                        \n                                    \n                 Repository\nVideo"
  },
  {
    "objectID": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#motivation",
    "href": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#motivation",
    "title": "Automating the creation of Blog posts",
    "section": "1 Motivation",
    "text": "1 Motivation\nWhen putting together a blog post, there are several steps that are time consuming. For instance, creating a directory with the date for the slug, then a title name and followed a YAML header that suits the format of the article. All this can be time consuming and hence in this post I will show how I have automated the creation of template posts to write code right away rather that setting this up\n\nGoal: from a simple command line i.e. mkblog notebook 'How to rock' a folder with the current date and the title-name i.e. 2023-08-21_how-to-rock will be created. Then the script should copy a pre-build post model that will modify the header’s date, title, href (for downloable Jupyter Notebooks) i.e:\n\ntitle: \"How to create Dynamic plots\"\ndate: \"2023-08-21\""
  },
  {
    "objectID": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#creating-the-script",
    "href": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#creating-the-script",
    "title": "Automating the creation of Blog posts",
    "section": "2 Creating the script",
    "text": "2 Creating the script\nThe first thing that came to my mind when creating the script was to the type of post. On a general level I have classify them in three categories\n\npost: this is a normal qmd file that can hold articles and is capable to run R, Python, JavaScript and so on.\npoems: this is a qmd modified version that enables the formatting that you see in my poems.\nnotebook: this is a ipynb file that can be executed using my conda environments\n\nThe question is now how to set all this up. Reviewing at scripting languages and ways on how to create extension for html files I realised all this could be done with python.\nHere down below the script that makes all this possible\n\n2.1 Make a Directory\n\n\ncreate_dir.py\n\n\nimport os \nimport sys\nfrom datetime import datetime\nimport shutil\n\ndef create_post(type_file, post_name):\n\n    # Save a copy only name\n    raw_post_name = post_name\n\n    # Example: \"how-to-rock\"\n    post_name = post_name.lower().replace(\" \", \"-\")\n\n    # Current Date: \"2023-08-21\"\n    current_date = datetime.today().strftime('%Y-%m-%d')\n\n    # Folder name: \"2023-08-21_how-to-rock\"\n    folder_name = current_date + '_' + post_name\n\n    # Current Directory path\n    current_dir = os.getcwd()\n    \n    # Output Directory\n    output_path = current_dir + \"/out\"\n\n    # Folder Path: \"../out/2023-08-21_how-to-rock\"\n    path_folder = os.path.join(output_path, folder_name)\n\n    # Make a Directory\n    os.mkdir(path_folder)\n    print(\"Directory '% s' created!\" % folder_name)\n\n# Name of Post\n# 0 is the name of the py file\n# 1 is the first argument\n\ntype_file = \"notebook\" # sys.argv[1]\npost_name = \"how to rock\" # sys.argv[2]             \n\ncreate_post(type_file, post_name)\n\nDirectory '2023-08-21_how-to-rock' created!\n\n\n\n\n2.2 Copy, move & edit a Blog’s Template\nAfter we have created a folder, we can proceed to make a copy of a user-defined template with settings that we may want to use for that specific type of post. For instance for my notebook posts I want to modify the date of the header, its title name and the ref link to download the output of the Jupyter notebook.\nFor this to scale and be able to fire the script from any directory I will point to a specific folder where I will make all this changes and then move those modified files to my ../blog section.\nThe structure of my files where the script is stored is as follows:\n\n\nTerminal\n\n_extensions\n└── mkpost\n    ├── create_dir.py\n    ├── out\n    ├── splitWin.app\n    └── templates\n        ├── notebook\n        │   └── index.ipynb\n        ├── poems\n        │   └── index.qmd\n        └── post\n            └── index.qmd\n\n\n\n\n\n\n\nNote\n\n\n\nTo print the tree structure you see above I have used: brew install tree and then tree &lt;path&gt;\n\n\nIn the folder view you find mkpost as the folder that holds the script previously shown. There is also out folder which contains the folder just created 2023-08-21_how-to-rock. There is also splitWin.app which will talk in section  and lastly a folder called templates where you can see index.* files that will be copied to our newly created folder.\nFollowing the continuation of the python script that does all the moving, copying and editing.\n\n\ncreate_dir.py\n\n\ndef create_file(type_file):\n    # -------------- CREATE FILE --------------\n    # Holds available extensions\n    dic = {'post': 'qmd', 'poems':'qmd', 'notebook':'ipynb'}\n    file_extension = dic[type_file]\n    \n    # Source path: \"../template/file_type/index.ext\"\n    source_file = f\"templates/{type_file}/index.{file_extension}\"\n    \n    # File name \"index.ext\"\n    file_name = f\"index.{file_extension}\"\n\n    # Destination path: \"../out/2023-08-21_how-to-rock/index.ext\"\n    destination_file = os.path.join(path_folder, file_name)\n\n    # Creating a Copy from Template folder\n    shutil.copy(source_file, destination_file)\n\n    # -------------- UPDATE HEADER --------------\n    lines = open(destination_file, 'r').readlines()\n    \n    if (type_file == \"notebook\"):\n        title_row = 9\n        lines[title_row] = f'    \\\"title: \\\\\"{raw_post_name.capitalize()}\\\\\"\\\\n\\\",'\n\n        date_row = 11\n        lines[date_row] = f'    \\\"date: \\\\\"{current_date}\\\\\"\\\\n\\\",'\n\n        code_name_row = 16\n        post_code_name = post_name + '.out.ipynb'\n        lines[code_name_row] = f'    \\\"      file-name: {post_code_name}\\\\n\\\",'\n\n    else:\n        title_row = 1\n        lines[title_row] = f'title: \"{raw_post_name.capitalize()}\"\\n'\n\n        date_row = 3\n        lines[date_row] = f'date: \"{current_date}\"\\n'\n\n    file = open(destination_file, 'w')\n    file.writelines(lines)\n    file.close()\n    \n    # -------------- MOVE FILE --------------\n    # Folder Path: \"../out/2023-08-21_how-to-rock\"\n    src = path_folder\n\n    # Path to Blog\n    dst = \"../../blog\"\n    shutil.move(src, dst)       \n    \n\nif (type_file == \"post\"):\n    create_file(\"post\")\nelif (type_file == \"notebook\"):\n    create_file(\"notebook\")\nelif (type_file == \"poems\"):\n    create_file(\"poems\")\nelse:\n    print(f\"Wrong type_file: {type_file}. Available: 'post', 'poems' or 'notebook'\")\n    sys.exit()\n    \n\nThe code above does the following:\n\nWhen the user inputs the name and the type_file it looks at the template folder with the specified extension and does the copying\nUpdates the header changing date to the current date at the moment of creating the directory, title with the name we input in the terminal and, lastly if is a notebook it changes the href to enable the user download the notebook.\nOnce all changes are done, the folder with the index.* file are move to the blog section."
  },
  {
    "objectID": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#fire-it-up-from-terminal",
    "href": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#fire-it-up-from-terminal",
    "title": "Automating the creation of Blog posts",
    "section": "3 Fire it up from terminal",
    "text": "3 Fire it up from terminal\nFor convenience, no matter in which directory of your computer you are in, this section cover how to fire it up the script that we have created. For starters, if you are running macOS or Linux most likely you will be using bash, zsh or fish shells. These shells, as it comes to not surprise, can launch scripts with user-defined strings defined.\nFor my liking I have decided to proceed with:\n\n\nTerminal\n\nmkblog [type_file] [\"name_post\"]\n\nTo enable this, you have to cd to your .bash_profile or .zshrc in my case. Then we will define the alias mkblog and will do the following:\n\ncd to the path where we have stored our python script. That is _extensions\nExecute the script using python\nOpen my workspace in Visual Studio\nOpen the index.* we just created i.e. in the dir \"2023-08-21_how-to-rock\"\nMake quarto render that file. Voila start coding!\n\n\n\n.zshrc\n\n# Creates script to open blog post\nmkblog() {\n  cd ../path/more_path/_extensions/mkpost\n  python create_dir.py $1 $2\n  cd ..\n  cd ..\n  code website.code-workspace\n  BACKUPDIR=$(ls -td blog/*/ | head -1)\n  cd $BACKUPDIR\n  FILENAME=$(ls | sort -f)\n  code $FILENAME\n  quarto preview $FILENAME \n}"
  },
  {
    "objectID": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#bonus---create-an-script-to-split-views",
    "href": "blog/2023-08-18_how-to-automate-the-creation-of-blog-posts/index.html#bonus---create-an-script-to-split-views",
    "title": "Automating the creation of Blog posts",
    "section": "Bonus - Create an script to split views",
    "text": "Bonus - Create an script to split views\n\nWhen working with large screen I would like to place in the left side the file I created and in the right side my browser to preview the changes I made while editing my file. If you are familiar with AppleScript then you will be at home with the following lines.\n\n\nsplitWin.app\n\nactivate application \"Visual Studio Code\"\nrepeat 1 times\n    tell application \"System Events\" to key code 123 using {command down}\nend repeat\n\nactivate application \"Microsoft Edge\"\nrepeat 1 times\n    tell application \"System Events\" to key code 124 using {command down}\nend repeat\n\nLong story short, the script will send the keystrokes cmd + ←, cmd + → to each application and thus split the windows to left and right. How to accomplished this behavior? Glad you asked, I invite your to read this post where I go through how I made my life easier using Lua Shortcuts."
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html",
    "href": "blog/2023-12-03_large-language-models/index.html",
    "title": "Large language models",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 3, 2023"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#title",
    "href": "blog/2023-12-03_large-language-models/index.html#title",
    "title": "Large language models",
    "section": "1 Title",
    "text": "1 Title\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#outline.",
    "href": "blog/2023-12-03_large-language-models/index.html#outline.",
    "title": "Large language models",
    "section": "2 Outline.",
    "text": "2 Outline.\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#large-language-models",
    "href": "blog/2023-12-03_large-language-models/index.html#large-language-models",
    "title": "Large language models",
    "section": "3 Large language models",
    "text": "3 Large language models\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#why-is-this-useful",
    "href": "blog/2023-12-03_large-language-models/index.html#why-is-this-useful",
    "title": "Large language models",
    "section": "4 Why is this useful?",
    "text": "4 Why is this useful?\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#what-can-we-expect-this-model-to-capture",
    "href": "blog/2023-12-03_large-language-models/index.html#what-can-we-expect-this-model-to-capture",
    "title": "Large language models",
    "section": "5 What can we expect this model to capture?",
    "text": "5 What can we expect this model to capture?\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#elmo-embeddings-from-language-models",
    "href": "blog/2023-12-03_large-language-models/index.html#elmo-embeddings-from-language-models",
    "title": "Large language models",
    "section": "6 ELMo: Embeddings from Language Models",
    "text": "6 ELMo: Embeddings from Language Models\nDatare at al 9N12 Maan rnntoyvtalizad winrdn ranrocaontatinne\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#the-elmo-model",
    "href": "blog/2023-12-03_large-language-models/index.html#the-elmo-model",
    "title": "Large language models",
    "section": "7 The ELMo model",
    "text": "7 The ELMo model\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#the-contributions-of-elmo",
    "href": "blog/2023-12-03_large-language-models/index.html#the-contributions-of-elmo",
    "title": "Large language models",
    "section": "8 The contributions of ELMo",
    "text": "8 The contributions of ELMo\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#the-rise-of-the-transformer",
    "href": "blog/2023-12-03_large-language-models/index.html#the-rise-of-the-transformer",
    "title": "Large language models",
    "section": "9 The rise of the Transformer",
    "text": "9 The rise of the Transformer\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#bert-architecture",
    "href": "blog/2023-12-03_large-language-models/index.html#bert-architecture",
    "title": "Large language models",
    "section": "10 BERT: Architecture",
    "text": "10 BERT: Architecture\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#bert-input-representations",
    "href": "blog/2023-12-03_large-language-models/index.html#bert-input-representations",
    "title": "Large language models",
    "section": "11 BERT: Input representations",
    "text": "11 BERT: Input representations\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#bert-pretraining-tasks",
    "href": "blog/2023-12-03_large-language-models/index.html#bert-pretraining-tasks",
    "title": "Large language models",
    "section": "12 BERT: Pretraining tasks",
    "text": "12 BERT: Pretraining tasks\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#bert-pretraining-tasks-1",
    "href": "blog/2023-12-03_large-language-models/index.html#bert-pretraining-tasks-1",
    "title": "Large language models",
    "section": "13 BERT: Pretraining tasks",
    "text": "13 BERT: Pretraining tasks\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#bert-pretraining",
    "href": "blog/2023-12-03_large-language-models/index.html#bert-pretraining",
    "title": "Large language models",
    "section": "14 BERT: pretraining",
    "text": "14 BERT: pretraining\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#bert-fine-tuning",
    "href": "blog/2023-12-03_large-language-models/index.html#bert-fine-tuning",
    "title": "Large language models",
    "section": "15 BERT: fine-tuning",
    "text": "15 BERT: fine-tuning\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#the-contributions-of-bert",
    "href": "blog/2023-12-03_large-language-models/index.html#the-contributions-of-bert",
    "title": "Large language models",
    "section": "16 The contributions of BERT",
    "text": "16 The contributions of BERT\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#outline.-1",
    "href": "blog/2023-12-03_large-language-models/index.html#outline.-1",
    "title": "Large language models",
    "section": "17 Outline.",
    "text": "17 Outline.\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#generative-language-models-the-gpt-family",
    "href": "blog/2023-12-03_large-language-models/index.html#generative-language-models-the-gpt-family",
    "title": "Large language models",
    "section": "18 Generative language models: The GPT family",
    "text": "18 Generative language models: The GPT family\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#more-than-a-language-model",
    "href": "blog/2023-12-03_large-language-models/index.html#more-than-a-language-model",
    "title": "Large language models",
    "section": "19 More than a language model?",
    "text": "19 More than a language model?\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#instructgpt-and-chatgpt",
    "href": "blog/2023-12-03_large-language-models/index.html#instructgpt-and-chatgpt",
    "title": "Large language models",
    "section": "20 InstructGPT and ChatGPT",
    "text": "20 InstructGPT and ChatGPT\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#an-example-from-chatgpt",
    "href": "blog/2023-12-03_large-language-models/index.html#an-example-from-chatgpt",
    "title": "Large language models",
    "section": "21 An example from ChatGPT",
    "text": "21 An example from ChatGPT\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#reinforcement-learning-from-human-feedback",
    "href": "blog/2023-12-03_large-language-models/index.html#reinforcement-learning-from-human-feedback",
    "title": "Large language models",
    "section": "22 Reinforcement learning from human feedback",
    "text": "22 Reinforcement learning from human feedback\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#reinforcement-learning-from-human-feedback-1",
    "href": "blog/2023-12-03_large-language-models/index.html#reinforcement-learning-from-human-feedback-1",
    "title": "Large language models",
    "section": "23 Reinforcement learning from human feedback",
    "text": "23 Reinforcement learning from human feedback\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#training-a-reward-model",
    "href": "blog/2023-12-03_large-language-models/index.html#training-a-reward-model",
    "title": "Large language models",
    "section": "24 Training a reward model",
    "text": "24 Training a reward model\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#training-a-reward-model-1",
    "href": "blog/2023-12-03_large-language-models/index.html#training-a-reward-model-1",
    "title": "Large language models",
    "section": "25 Training a reward model",
    "text": "25 Training a reward model\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#fine-tuning-with-reinforcement-learning",
    "href": "blog/2023-12-03_large-language-models/index.html#fine-tuning-with-reinforcement-learning",
    "title": "Large language models",
    "section": "26 Fine-tuning with reinforcement learning",
    "text": "26 Fine-tuning with reinforcement learning\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#outline.-2",
    "href": "blog/2023-12-03_large-language-models/index.html#outline.-2",
    "title": "Large language models",
    "section": "27 Outline.",
    "text": "27 Outline.\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#instruction-tuned-llms-and-multi-task-learning",
    "href": "blog/2023-12-03_large-language-models/index.html#instruction-tuned-llms-and-multi-task-learning",
    "title": "Large language models",
    "section": "28 Instruction-tuned LLMs and multi-task learning",
    "text": "28 Instruction-tuned LLMs and multi-task learning\nSanh etal 2022 Multitasck Promoted Training Enables Zero-Shot Task\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#multilingual-llms",
    "href": "blog/2023-12-03_large-language-models/index.html#multilingual-llms",
    "title": "Large language models",
    "section": "29 Multilingual LLMs",
    "text": "29 Multilingual LLMs\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#multilingual-llms-intuition",
    "href": "blog/2023-12-03_large-language-models/index.html#multilingual-llms-intuition",
    "title": "Large language models",
    "section": "30 Multilingual LLMs: Intuition",
    "text": "30 Multilingual LLMs: Intuition\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#multilingual-llms-models",
    "href": "blog/2023-12-03_large-language-models/index.html#multilingual-llms-models",
    "title": "Large language models",
    "section": "31 Multilingual LLMs: Models",
    "text": "31 Multilingual LLMs: Models\nTLlRAtn4d4an lARAIIRARR ARKRRRARBA yathin nnn masAazl\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#multilingual-llms-application",
    "href": "blog/2023-12-03_large-language-models/index.html#multilingual-llms-application",
    "title": "Large language models",
    "section": "32 Multilingual LLMs: Application",
    "text": "32 Multilingual LLMs: Application\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#can-llms-solve-nlp",
    "href": "blog/2023-12-03_large-language-models/index.html#can-llms-solve-nlp",
    "title": "Large language models",
    "section": "33 Can LLMs solve NLP?",
    "text": "33 Can LLMs solve NLP?\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#can-llms-solve-nlp-1",
    "href": "blog/2023-12-03_large-language-models/index.html#can-llms-solve-nlp-1",
    "title": "Large language models",
    "section": "34 Can LLMs solve NLP?",
    "text": "34 Can LLMs solve NLP?\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-12-03_large-language-models/index.html#outstanding-challenges-and-future-directions",
    "href": "blog/2023-12-03_large-language-models/index.html#outstanding-challenges-and-future-directions",
    "title": "Large language models",
    "section": "35 Outstanding challenges and future directions",
    "text": "35 Outstanding challenges and future directions\n\n\n\n\nSlide"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html",
    "href": "blog/2023-10-04_logistic-regression/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Lecture Notes UvA on 25-9-2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 4, 2023\nThe downside with the Perceptron is that there is no probabilistic interpretation. We want to link the linear model to class conditional probabilities\ni.e we would like to say there is 50% that this belongs to class K \\[\n\\begin{align}\np(C|x)\n\\end{align}\n\\]\nNow, lets talk about Logistic Regression"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#plotting-pc_kx-2nd-plot",
    "href": "blog/2023-10-04_logistic-regression/index.html#plotting-pc_kx-2nd-plot",
    "title": "Logistic Regression",
    "section": "1 Plotting \\(p(C_k|x)\\) (2nd plot)",
    "text": "1 Plotting \\(p(C_k|x)\\) (2nd plot)\n\n\nCode\n# Generate a grid of x and y coordinates\nx_range = np.linspace(-10, 10, 400)\ny_range = np.linspace(-10, 10, 400)\nxx, yy = np.meshgrid(x_range, y_range)\n\n# Calculate p(C_A | x) and p(C_B | x) for each point on the grid\n# Note: This is a simplified approach and doesn't use the actual Naive Bayes model\np_C_A_given_x = norm.pdf(np.linalg.norm([xx - np.cos(theta) * (2 * theta + np.pi), yy - np.sin(theta) * (2 * theta + np.pi)], axis=0))\np_C_B_given_x = norm.pdf(np.linalg.norm([xx - np.cos(theta) * (-2 * theta - np.pi), yy - np.sin(theta) * (-2 * theta - np.pi)], axis=0))\n\n# Plot the contour plots\nplt.figure(figsize=(7, 3))\nplt.subplot(121)\nplt.contourf(xx, yy, p_C_A_given_x, cmap='Blues', levels=20)\nplt.title('p(C_A | x)')\nplt.colorbar()\n\nplt.subplot(122)\nplt.contourf(xx, yy, p_C_B_given_x, cmap='Oranges', levels=20)\nplt.title('p(C_B | x)')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nThe contour plots show the probability values over the grid of x and y coordinates, with color indicating probability levels. The higher the probability, the darker the color.\nSince the data is not generated according to a real statistical model, the visualized distributions are more conceptual than precise.\nIn a real Naive Bayes model, you would estimate p(C_k | x) using the actual data and the independence assumption. The visualization would depend on the data, features, and model parameters."
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#concentric-circles-distribution",
    "href": "blog/2023-10-04_logistic-regression/index.html#concentric-circles-distribution",
    "title": "Logistic Regression",
    "section": "2 Concentric circles Distribution",
    "text": "2 Concentric circles Distribution\n\n\nCode\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=1000, random_state=123, \n                    noise=0.1, factor=0.2)\n\nplt.figure(figsize=(8,6))\n\nplt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\nplt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\nplt.title('Concentric circles')\nplt.ylabel('y coordinate')\nplt.xlabel('x coordinate')\nplt.show()"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#deriving-logistic-regression",
    "href": "blog/2023-10-04_logistic-regression/index.html#deriving-logistic-regression",
    "title": "Logistic Regression",
    "section": "3 Deriving Logistic Regression",
    "text": "3 Deriving Logistic Regression\n\nNote: We would be talking about Binary Classification\n\nHere the want to link linear model to probabilities to get Logistic Regression\n\nWhen we set up Bayes Classifiers, We look at the ratio of probabilities.\n\n\n\n\nHere we are saying that this ratio would not work because i.e if we would predict a negative value for this ratio that would mean that either the denominator or denominator would be negative. But negative probables cannot happen.\nThe solution, make compute the \\(\\ln\\)\n\n\n\nNote that at the decision boundary meaning when \\(p(C_1|x)=p(C_2|x)\\) we would have that the ratio is zero.\nNow, rearranging the expression above:\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sigmoid"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#logistic-regression-making-predictions",
    "href": "blog/2023-10-04_logistic-regression/index.html#logistic-regression-making-predictions",
    "title": "Logistic Regression",
    "section": "4 Logistic Regression: Making Predictions",
    "text": "4 Logistic Regression: Making Predictions"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#counting-parameters",
    "href": "blog/2023-10-04_logistic-regression/index.html#counting-parameters",
    "title": "Logistic Regression",
    "section": "5 Counting Parameters",
    "text": "5 Counting Parameters\nHow much more parameter efficient is logistic regression vs a Gaussian classifier? (assume the feature vector has D dimensions)\nLogistic regression\n\nNum of parameters: \\(D+1\\) one weight for each feature value and then you have the bias weight.\n\nLinear Discriminant Analysis\n\nHere we defined the Gaussian likelihoods\n\n\nNum of parameters: \\((2D)+[D(D - 1)2 + D]\\)\n\n\nHow many parameters do the means of these gaussian likelihood have?\n\\(2D\\): because D dimensions per each mean and we have 2 likelihoods because we are doing binary classifier\nHow many parameters the covariance matrix have? Assume we are doing LDA, so the covariance matrix are shared.\n\\(D^2\\) Naively but in this case, the covariance matrix has this symmetry structure so \\(D*D - D/2\\) is the off diagonal matrix and we add \\(D\\) more for the diagonal\nThe last term, we have means, we have covariances. We need the bias because maybe we can have unbalance data so we also want this to balance the data.\nIn a bayes classifier we fixed our unbalance data with the prior. So we have one parameter for the prior\n\nTo understand more: link\nThat means for the LDA we need around \\(D^2\\) as compared to Logistic Regression \\(D+1\\) to get a classification example."
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#learning-via-log-likelihood",
    "href": "blog/2023-10-04_logistic-regression/index.html#learning-via-log-likelihood",
    "title": "Logistic Regression",
    "section": "6 Learning via Log-Likelihood",
    "text": "6 Learning via Log-Likelihood\nWe talked about our Logistic Regression model, now lets see how do we fit into our data."
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#learning-via-log-likelihood-1",
    "href": "blog/2023-10-04_logistic-regression/index.html#learning-via-log-likelihood-1",
    "title": "Logistic Regression",
    "section": "7 Learning via Log-likelihood",
    "text": "7 Learning via Log-likelihood\nWe assume our data are ID.\nNow that we have our labels which are binary we could use a Bernouli distribution to model this.\n\n\n\nWhere, \\(\\pi\\) represents the probability of sucess and \\(t\\) represents our possible outcomes. In this case our outcomes would be to belong to class \\(0\\) if \\(C_0\\) or \\(1\\) if \\(C_1\\)\n\n\n\nWhere, \\(\\sigma(\\textbf{w}^T\\textbf{x}_n)^{t_n}\\) is equal to \\(p(C1|x)\\). See the graph of the sigmoid Figure 1\nNow lets take the log of this Likelihood\n\n\n\nTo compute the Error we have:\n\n\n\nThis is equation is also called cross-entropy loss\nif the label \\(t_n=1\\) then second term will go away. Because we want to have the error to go to zero, then we need to make the log to go to zero. This happens when \\(log(1)\\) thus the \\(\\sigma(\\textbf{w}^T\\textbf{x}_n)\\) needs to equal one. This will only happen when the value of \\(\\textbf{w}^T\\textbf{x}_n\\) is large enough so that the \\(\\sigma()\\) computes it to one, meaning that \\(p(C_1|x)=1\\) which means we have predicted the correct label. Recall \\(t_n=1\\).\nThe other way around, the truth label is \\(t_n=1\\) but the output of \\(\\sigma(\\textbf{w}^T\\textbf{x}_n)~0\\) so close to zero which means the \\(\\ln(0.00001)=-11\\) would be negative which then recall we have a \\(-\\) minus sign which means we would be making a big error of \\(11\\).\n\\(E(w)\\) is convex in \\(\\textbf{w}\\) but unlike linear regression there is no analytical solution [1]. Which means if we take the derivative and solve for \\(w\\) we would get stuck.\n\nWhat is convex decision boundaries?\n\n\n\n\nif you take any two points within the boundary, the line segment connecting those points lies entirely within the boundary as well"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#class-entropy-loss",
    "href": "blog/2023-10-04_logistic-regression/index.html#class-entropy-loss",
    "title": "Logistic Regression",
    "section": "8 Class-Entropy Loss",
    "text": "8 Class-Entropy Loss\n\n\n\n\n8.1 Comparing Logistic Regression and Square Regression\n\n\n\nLogistic Regression is on the green line, Least Square Regression the purple.\n\nIf you have outliers, LEast Square changes its boundary so its sensible to outliers\nLogistic Regression is not, it kept the original boundary"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#gradient-descent-variants",
    "href": "blog/2023-10-04_logistic-regression/index.html#gradient-descent-variants",
    "title": "Logistic Regression",
    "section": "9 Gradient Descent: Variants",
    "text": "9 Gradient Descent: Variants\n\nNote, because we saw that the Error of the Log-likelihood of Logistic Regression cannot be solved analytically then we turn into gradient descent\n\n\n\n\n\nBecause the above summation is expensive then we got variants.\n\nBatch: Use all N data points\nStochastic: Use one data point to approximate sum\nHere we pick a random point \\(i\\) and we multiply by \\(N\\) to replicate the magnitude of learning rate. This is to keep same scale as the full gradient descent formula we see above. In other words this is just to conserve the magnitude of the \\(\\eta\\) value\n\n\n\n\n\nMini-Batch: Use B data points where 1 &lt; B &lt; N. Usually B much less than N (B &lt;&lt; N)\n\n\n\n\n\n9.1 Gradient Descent: Variants Performance"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#gradient-calculation",
    "href": "blog/2023-10-04_logistic-regression/index.html#gradient-calculation",
    "title": "Logistic Regression",
    "section": "10 Gradient Calculation",
    "text": "10 Gradient Calculation\n\n\n\n\n\n\n\n\n\n\n\n\nThe answer look identical to the one proposed for the perceptron in our prev post\n\n\n\n\nThe difference with the perceptron its that the update for the weights its softer.\nThe Gradient descent with logistic regresion will keep updating until \\(t_i-\\sigma(\\textbf{w}_t^T\\textbf{x}_i)\\) is close to zero, whereas the perceptron because of this \\(f(w^T\\phi)&gt;0\\) condition will be: \\(t_i-\\sigma(\\textbf{w}_t^T\\textbf{x}_i)\\) this is close enough (review the inequality post) to zero then we do not update the weight."
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#stochastic-gradient-descent",
    "href": "blog/2023-10-04_logistic-regression/index.html#stochastic-gradient-descent",
    "title": "Logistic Regression",
    "section": "11 Stochastic Gradient Descent",
    "text": "11 Stochastic Gradient Descent\nHere the perceptron choose only misclassified, here in Stochastic Gradient Descent we would pick any point randomly. Even if it’s well classified already.\n\n\n\nNow the question is: how do I set this parameter \\(\\eta\\) the learning rate. The answer to that is a method that help us find that. It’s called Newton-Raphson"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#setting-learning-rate",
    "href": "blog/2023-10-04_logistic-regression/index.html#setting-learning-rate",
    "title": "Logistic Regression",
    "section": "12 Setting learning rate",
    "text": "12 Setting learning rate\nIt is a difficult task:\n\ntoo small —&gt; long time to reach minimum\ntoo big —&gt; bounce around and never converge"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#idea-use-2nd-derivative",
    "href": "blog/2023-10-04_logistic-regression/index.html#idea-use-2nd-derivative",
    "title": "Logistic Regression",
    "section": "13 Idea: Use 2nd Derivative",
    "text": "13 Idea: Use 2nd Derivative\nThe second derivative corresponds to the curvature of the loss function, thus\nThis captures the notion of curvature\n\n\n\n\nSmall second derivative –&gt; large learning rate\nSo if these two purple arrows are pointing in the same direction it means there their second derivative is small because there is no much change which then means we can increase the learning rate because there is still a curvature meaning we have not reach to global minimum\nLarge second derivative –&gt; small learning rate\nIn the second case with the green arrows, this time the second derivative is large because there is a substantial change from pointing down to now pointing in almost horizontal direction. Then that means that we are getting closer to a deep and this we want to make the learning rate small\n\n\n13.1 Computing 2nd Derivative (for scalar)\n\n\n\nHere the second derivative \\(\\frac{\\partial^2}{\\partial w_t^2}\\) represents the curvature of the loss function so the answer that we will show below is just the graph of how this curvature would look like.\n\n\n\n\nThe error curvature would be maximized when the model is uncertain. The intuition is that if you have a lot of points next to the decision boundary then\nIt is small when the model is certain.\n\n\n\n\nWhere we have introduce the normalization term"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#steepest-descent-vs-newton-raphson",
    "href": "blog/2023-10-04_logistic-regression/index.html#steepest-descent-vs-newton-raphson",
    "title": "Logistic Regression",
    "section": "14 Steepest DEscent vs Newton-Raphson",
    "text": "14 Steepest DEscent vs Newton-Raphson\n\n\n\n\nThus, we see tha the Newton-Raphson takes a more direct path because it considers the curvature of the error loss.\n\n\n\n\n\n\nDerivative of vector^T with respect to vector\n\n\n\n\n\nThe notation \\(\\partial (x^T) / \\partial x\\) represents the derivative of a row vector \\(x^T\\) with respect to a column vector \\(x\\). In this context, the derivative is a Jacobian matrix.\nIf \\(x\\) is an \\(n\\)-dimensional column vector and \\(x^T\\) is the corresponding \\(n\\)-dimensional row vector, then the derivative \\(\\partial (x^T) / \\partial x\\) is a \\(n \\times n\\) Jacobian matrix.\nEach element \\((\\partial (x^T)_i / \\partial x_j)\\) of this Jacobian matrix is the partial derivative of the \\(i\\)-th element of \\(x^T\\) with respect to the \\(j\\)-th element of \\(x\\). Since each element of \\(x^T\\) is just a scalar, this derivative is straightforward to calculate:\n\\((\\partial (x^T)_i / \\partial x_j) = \\delta_{ij}\\)\nwhere \\(\\delta_{ij}\\) is the Kronecker delta, which is equal to 1 when \\(i = j\\) and 0 otherwise.\nTherefore, the Jacobian matrix \\(\\partial (x^T) / \\partial x\\) is a diagonal matrix with 1s on the diagonal and 0s off the diagonal. It’s an identity matrix of size \\(n \\times n\\)."
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#newton-raphson-iterative-optimization",
    "href": "blog/2023-10-04_logistic-regression/index.html#newton-raphson-iterative-optimization",
    "title": "Logistic Regression",
    "section": "15 Newton-Raphson Iterative Optimization",
    "text": "15 Newton-Raphson Iterative Optimization\n\n\n\n\n\n\n\n15.1 What is the H?\n\n\n\n\nThe blue highlighted section means that we have taken the second derivative over the error of the log-likelihood which has a minus hence why the blue hihglited parts vary in order."
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#newton-rpahson-iterative-optimization",
    "href": "blog/2023-10-04_logistic-regression/index.html#newton-rpahson-iterative-optimization",
    "title": "Logistic Regression",
    "section": "16 Newton-Rpahson Iterative Optimization",
    "text": "16 Newton-Rpahson Iterative Optimization"
  },
  {
    "objectID": "blog/2023-10-04_logistic-regression/index.html#faq",
    "href": "blog/2023-10-04_logistic-regression/index.html#faq",
    "title": "Logistic Regression",
    "section": "17 FAQ",
    "text": "17 FAQ\n\n\n\n\n\n\nWhat are linear features in binary classification?\n\n\n\n\n\nIn binary classification, linear features refer to features that can be effectively separated by a straight line (or a hyperplane in higher dimensions) when plotting them on a graph. These features are sometimes called linearly separable features because you can draw a line that cleanly separates the two classes, making it easy for a linear classifier like logistic regression or a linear support vector machine (SVM) to classify the data accurately.\nHere’s an example to illustrate linear features in binary classification:\nSuppose you are working on a binary classification problem to predict whether an email is spam (class 1) or not spam (class 0) based on two features: the number of words in the email and the number of times the word “free” appears in the email.\nYou collect data on various emails, and when you plot this data on a graph with the number of words on the x-axis and the frequency of the word “free” on the y-axis, you notice that spam emails tend to have fewer words and a higher frequency of the word “free,” while non-spam emails tend to have more words and a lower frequency of the word “free.”\nHere’s a simplified example:\n\nSpam Email A: 10 words, “free” appears 8 times\nSpam Email B: 12 words, “free” appears 10 times\nNon-Spam Email X: 20 words, “free” appears 2 times\nNon-Spam Email Y: 18 words, “free” appears 1 time\n\nIf you plot these data points on a graph, you might observe that you can draw a straight line that effectively separates the spam emails (class 1) from the non-spam emails (class 0). In this case, the number of words and the frequency of the word “free” are linear features, as they allow for a linear separation of the two classes.\nHere’s what the separation might look like (though in reality, the data might be more complex):\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Data points\nspam_emails = [(10, 8), (12, 10)]\nnon_spam_emails = [(20, 2), (18, 1)]\n\n# Unpack the data into separate lists\nspam_x, spam_y = zip(*spam_emails)\nnon_spam_x, non_spam_y = zip(*non_spam_emails)\n\n# Create the scatter plot\nplt.scatter(spam_x, spam_y, label='Spam Emails', marker='*')\nplt.scatter(non_spam_x, non_spam_y, label='Non-Spam Emails', marker='o')\n\n# Add labels and legend\nplt.xlabel('Number of Words')\nplt.ylabel('Frequency of \"free\"')\nplt.legend()\n\n# Add a straight line for separation (in this case, manually defined)\nplt.plot([15, 15], [0, 12], linestyle='--', color='gray')\n\n# Set plot limits and display\nplt.xlim(0, 25)\nplt.ylim(0, 12)\nplt.title('Linearly Separable Data for Binary Classification')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nIn this example, you can see that a straight line can be drawn to separate the two classes, making the features (number of words and frequency of “free”) linear features for this binary classification problem."
  },
  {
    "objectID": "blog/2022-11-05_welcome/index.html",
    "href": "blog/2022-11-05_welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome To My Blog\n        \n        \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                News\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                News\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 5, 2022\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\nThis is the first post in my blog. Welcome!"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html",
    "href": "blog/2023-12-03_discourse-processing/index.html",
    "title": "Discourse processing",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 3, 2023"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#title",
    "href": "blog/2023-12-03_discourse-processing/index.html#title",
    "title": "Discourse processing",
    "section": "1 Title",
    "text": "1 Title\n\n\n\n\nSlide 1"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#outline.",
    "href": "blog/2023-12-03_discourse-processing/index.html#outline.",
    "title": "Discourse processing",
    "section": "2 Outline.",
    "text": "2 Outline.\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#document-structure-and-discourse-structure",
    "href": "blog/2023-12-03_discourse-processing/index.html#document-structure-and-discourse-structure",
    "title": "Discourse processing",
    "section": "3 Document structure and discourse structure",
    "text": "3 Document structure and discourse structure\n\n\n\n\nSlide 3\n\n\n\n\nThere is explicit relations between sentences, these are called rhetorical relations. If in a text sentences do not have relations then it will be difficult for us to understand because they will not be coherent"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#rhetorical-relations",
    "href": "blog/2023-12-03_discourse-processing/index.html#rhetorical-relations",
    "title": "Discourse processing",
    "section": "4 Rhetorical relations",
    "text": "4 Rhetorical relations\n\n\n\n\nSlide 4\n\n\n\n\nHere there is two sentences in this discourse. Individually these sentences are not ambiguous. However, the discourse itself has some ambiguity\n\nWe are describing a sequence of events\n\nThe cue phrases, are the relationship between the two sentences. However, these relations are implicit, which means to interpret the whole text we need to be able somehow and derive those relationships between the sentences by analyzing the language of the document"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#rhetorical-relations-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#rhetorical-relations-1",
    "title": "Discourse processing",
    "section": "5 Rhetorical relations",
    "text": "5 Rhetorical relations\n\n\n\n\nSlide 5\n\n\n\n\nThey are typically not symmetric, which means there would be a main clause akak nucleus or subsidiary caluse aka the satelite. So here not all sentences are equally important in the document. There would be a main phrase and the satellite phrases would be contributing to these. This is important for analyzing text for instance to do a summary of the text you will want to know which sentences are the main ones. If we then want to interpret the text in a more fine grained we need to understand these relations.\nOn the other hands some rethorical relations have equal weight for instance when we do Narration. This is when we describe a sequence of events that took place so none of them is more important than the other"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#rhetorical-relations-2",
    "href": "blog/2023-12-03_discourse-processing/index.html#rhetorical-relations-2",
    "title": "Discourse processing",
    "section": "6 Rhetorical relations",
    "text": "6 Rhetorical relations\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#coherence",
    "href": "blog/2023-12-03_discourse-processing/index.html#coherence",
    "title": "Discourse processing",
    "section": "7 Coherence",
    "text": "7 Coherence\n\n\n\n\nSlide 7"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#coherence-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#coherence-1",
    "title": "Discourse processing",
    "section": "8 Coherence",
    "text": "8 Coherence\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#coherence-in-interpretation",
    "href": "blog/2023-12-03_discourse-processing/index.html#coherence-in-interpretation",
    "title": "Discourse processing",
    "section": "9 Coherence in interpretation",
    "text": "9 Coherence in interpretation\n\n\n\n\nSlide 9\n\n\n\n\n\nExplanation here we are explaining that because he=Bill got to Jhon an expensive gift, now Jhon likes Bill\nJustification here we are saying that Jhon likes Bill and because of that he=Jhon is buying it an expensive gift. The latter is the evidence that support the fact that Jhon likes Bill and thus the evidence of his act"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#factors-influencing-discourse-interpretation",
    "href": "blog/2023-12-03_discourse-processing/index.html#factors-influencing-discourse-interpretation",
    "title": "Discourse processing",
    "section": "10 Factors influencing discourse interpretation",
    "text": "10 Factors influencing discourse interpretation\n\n\n\n\nSlide 10\n\n\n\n\nHere the bracket is like an explanations, if is a comma it is likelly to be narration. These are explicit signals, of rethorical relations"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#discourse-parsing",
    "href": "blog/2023-12-03_discourse-processing/index.html#discourse-parsing",
    "title": "Discourse processing",
    "section": "11 Discourse parsing",
    "text": "11 Discourse parsing\n\n\n\n\nSlide 11\n\n\n\n\nThe task of automatically identifying the retorical relations is called discourse parsing.\nTo conduct discourse parsing it requires to analyse the full text. This is because we need to identify teh relations of all the sentences and derive a tree like structure for the discourse where the main phrases and the subsidiary phrases and so on, so how they connect to each other. If we do that then this would give us a picture of the discourse. But this is difficult to do.\nThere has been many approaches to that, in early research they use hand crafted rules based on the signals like punctuation , cue ..They treated it as a supervised classification problem where the input is the two sentences and the ouput which rectorial relation is."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#outline.-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#outline.-1",
    "title": "Discourse processing",
    "section": "12 Outline.",
    "text": "12 Outline.\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#document-representations",
    "href": "blog/2023-12-03_discourse-processing/index.html#document-representations",
    "title": "Discourse processing",
    "section": "13 Document representations",
    "text": "13 Document representations\n\n\n\n\nSlide 13\n\n\n\n\nAll these are training for specific task. The features that they capture is task dependent. More general methods we can find:\n\nSkip-grams word embeddings. These are trained with unlabeled data"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#learning-document-representations",
    "href": "blog/2023-12-03_discourse-processing/index.html#learning-document-representations",
    "title": "Discourse processing",
    "section": "14 Learning document representations",
    "text": "14 Learning document representations\n\n\n\n\nSlide 14\n\n\n\n\nHere this is a RNN, we go through the sentence word by word. And per time step"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#bidirectional-lstm",
    "href": "blog/2023-12-03_discourse-processing/index.html#bidirectional-lstm",
    "title": "Discourse processing",
    "section": "15 Bidirectional LSTM",
    "text": "15 Bidirectional LSTM\n\n\n\n\nSlide 15\n\n\n\n\nUni directional LSTM, is one that we traverse the sentence from beginning to end in one direction. The problem with this is that at each time step we only record representations of the sentence that we have seen so far. To derive a representation for every token we dont consider the context that comes after that token. The problem with that is that the subsequent context is also informative of what this particular token at time t can mean. Ideally we want to incorporate conext in both directions. Thus we use a bidirectional LSTM.\nAt each token we have two representations the context representation from the beginning of the sentence to the token and the representation of that token after the end of the sentence."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#what-is-the-sentence-representation",
    "href": "blog/2023-12-03_discourse-processing/index.html#what-is-the-sentence-representation",
    "title": "Discourse processing",
    "section": "16 What is the sentence representation?",
    "text": "16 What is the sentence representation?\n\n\n\n\nSlide 16\n\n\n\n\nHow do we actually obtain the representation of a whole sentence because BiLSTM ouputs representations for individual token. So how do we combine token representations into sentence representations. We have few options here:\n\nWe can apply LSTM, then once we have traverse the whole sentence so the last representation at the last token is going to be the sentence representation, as it incorporates the whole sentence. The problem with this is that because LSTM are sequential models, the more recent input(so the last token) is always going to be represented better than early inputs. The cell state tries to mitigiate this however for realistically long sentences we still have the problem that more recent context is better represented by earlier context.\nIn this way information from all tokens is incorporated equally so we get a representation of a sentence\nHere as well we take our hidden representations at all steps and then for each vector component, for each dimensions, we pick the higuest values across all token representations in the sentence and then we put it into the vector sentence.\nAn attention mechanism basically computes a weighted sum of all hidden representations of all tokens."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#attention-mechanism",
    "href": "blog/2023-12-03_discourse-processing/index.html#attention-mechanism",
    "title": "Discourse processing",
    "section": "17 Attention mechanism",
    "text": "17 Attention mechanism\n\n\n\n\n\nSlide 17\n\n\n\n\nIt is a good idea because in fact not all words are equally importnat in the document. For instance if you think of sentiment analysis taks then there is going to be positive, negative and neutral words. So here neutrals are not that relevant thus we need some way how we can put more weight into pos and neg words. For the latter we use the attention mechanism\nHumans pay more attention to some words than others. So how do we compute attention weights, we can compute them from learnable parametes in the model, so the vector \\(W_\\alpha\\) and so we learn it FFNN (feed forward neural network) is the hidden representation at time t.\nThere is different models of attention, sometimes you take the hidden representation and you pass it through a Feed NN, and the use that to compute the attention weights.\nSometimes people take the hidden state and compute the dot product of that with the attention vector. Here if we use a weighted sum then we need to care that our vector add up to 1, so normalize this alpha. And the this give us our final attention weights \\(a_t\\) and finally the sentence representation is going to be computed as a weighted sum of hiddent states of individual tokes \\(h_t\\) using the attention weights\nThe attention weights will not be transferable because we are learning for a task so its really what are the toeksn that provide good features for that task and that is not transferable across tasks"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#building-a-document-representation",
    "href": "blog/2023-12-03_discourse-processing/index.html#building-a-document-representation",
    "title": "Discourse processing",
    "section": "18 Building a document representation",
    "text": "18 Building a document representation\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#building-a-document-representation-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#building-a-document-representation-1",
    "title": "Discourse processing",
    "section": "19 Building a document representation",
    "text": "19 Building a document representation\n\n\n\n\nSlide 19\n\n\n\n\nThe things that we saw above was to compute sentence representations, now we want document representations.\n\nThis is not a good idea because LSTM suffer from forgetting things from the previous previous inputs.\n\nHowever attention helps to mittigate this because you can pick important components of the different parts of the document but still the document is pretty long and just picking up separate words here and there and forgetting a lot of the other information is not going to give us a very good model. So this is not a good idea, m\n\nA much better way is to build a hierarchical model\n\n\nFirst compute sentence representations using a sentence encoder and then we want to take those sentence representations and then\nWe want to combine them into a document representation in some way for example a document encoder, or also could be done using attention or perhaps using both. You will then train your model with a document objective with your task objective"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#hierarchical-attention-networks",
    "href": "blog/2023-12-03_discourse-processing/index.html#hierarchical-attention-networks",
    "title": "Discourse processing",
    "section": "20 Hierarchical attention networks",
    "text": "20 Hierarchical attention networks\n\n\n\n\nSlide 20\n\n\n\n\nThis is a model that came before the transformer.\nThe idea is that this model has two separate encoders, 1 a sentence encoder, take word representations as inputs, these sentence representations are passed to another encoder which then will represent my document. So we learn specifically to combine sentences into document representations.\nThis is a good idea because these two encoders can focus on learning different kind of information, so the sentence encoder will learn to compose the meaning of words into the meaning of sentences and your document encoder is going to learn to compose sentences into the document, here is wher potntial rethorical relations can happen and other properties of the document. Then we train our model with our task specific\nThe larger you make the model the more parameters you need to learn, the larger the dataset you need. If you have a large dataset then you may be better off with more parameters more complex models, if you have a small dataset then you may be better off with a model with fewer paramters that you can tune a simple architecture\nHere they use pretrained word embeddings and that is a very good idea because your dataset is not very alrge because then you already starting with an inform position because the word embeddings already capture some general semantic knowledge, so you know in a sense what the words mean and you just need to learn the relations between those words that are relevant to solve your task.\nIf you have a really long document and you just have one label for it, very often you can predict the label by analysing part of the document and what people do is you want to decide what is a reasonable size of the input to predict from and then just cut that input at some point."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#hierarchical-attention-network",
    "href": "blog/2023-12-03_discourse-processing/index.html#hierarchical-attention-network",
    "title": "Discourse processing",
    "section": "21 Hierarchical attention network",
    "text": "21 Hierarchical attention network\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#han-output-with-attention-visualised",
    "href": "blog/2023-12-03_discourse-processing/index.html#han-output-with-attention-visualised",
    "title": "Discourse processing",
    "section": "22 HAN output with attention visualised",
    "text": "22 HAN output with attention visualised\nHere we can visualize this attention weights\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#han-output-with-attention-visualised-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#han-output-with-attention-visualised-1",
    "title": "Discourse processing",
    "section": "23 HAN output with attention visualised",
    "text": "23 HAN output with attention visualised\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#outline.-2",
    "href": "blog/2023-12-03_discourse-processing/index.html#outline.-2",
    "title": "Discourse processing",
    "section": "24 Outline.",
    "text": "24 Outline.\n\n\n\n\nSlide 24\n\n\n\n\nWe will now perform another discourse processing task called reference resolution. So besides rhetorical relations and topics and sections and so on.\nTo perform full task opinion mining we need to retrieve relevant documents, then we need to take this documents and then we need to identify all the mentions of a particular person or person in this company. And we need to link then together to better understand how a particular person is described so for instance: if we want to identify opinions about Joe Biden then we need to identify all the references to Joe biden in the text. For example something that says president Biden, he or him and so on.\nSo co-reference resolutions is identifying bits of text that refer to the same identity through the discourse."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#co-reference-and-referring-expressions",
    "href": "blog/2023-12-03_discourse-processing/index.html#co-reference-and-referring-expressions",
    "title": "Discourse processing",
    "section": "25 Co-reference and referring expressions",
    "text": "25 Co-reference and referring expressions\n\n\n\n\nSlide 25\n\n\n\n\nHere there is two people. Niall and Stephen.\n\nThe referent is a real world entity that some piece of text or speech refers to. For instance here we are refering to the prof Ferguson\nThe refering expressions, these are the expresions that are used to refer these words\nThe antecedent is the text that initially introduces the referent. This is the first time that somebody is mentioned.\nAnaphora this is when we refers to something that is previously mention.\nCataphora is rare and we use it when we refer to something that is only introduced something later in the text. For instance: She loves her dog, Kim bought her some gifts. So here we mention “She” first and then we ontroduce the name “Kim”.\n\nAnaphora resolution is linking to the correct antecedent so finding what is the real word entity that they are refering to.\nWe have a lot of noun phrases like: a snappy dresser where it purely plays a descriptive role so they do not refer to anything. To complicate things even more it is not every noun phrase that actually has to refer to something. So these are non-referential phrases because they do no refer to anything. They are also called predicative use.\nSo in conclusion we have that some noun phrases are not going to refer to anything. But most of them they do"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#pronoun-resolution",
    "href": "blog/2023-12-03_discourse-processing/index.html#pronoun-resolution",
    "title": "Discourse processing",
    "section": "26 Pronoun resolution",
    "text": "26 Pronoun resolution\n\n\n\n\nSlide 26\n\n\n\n\nHere we want to identify the antecedents of pronouns. So if a sentence contains a pronoun then we want to know which pronoun (he, she and so on) we are referring to.\nHere we wat to find the referent so which is the real word entity we are refering to by using the pronoun i.e she, he, we and so on.\nSo here him refers to Niall Fergusson and he refers to Stephen Moss. So the task here is to be able to link the pronoun to the correct antecedent"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#pronoun-resolution-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#pronoun-resolution-1",
    "title": "Discourse processing",
    "section": "27 Pronoun resolution",
    "text": "27 Pronoun resolution\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#outline.-3",
    "href": "blog/2023-12-03_discourse-processing/index.html#outline.-3",
    "title": "Discourse processing",
    "section": "28 Outline.",
    "text": "28 Outline.\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#coreference-resolution-as-supervised-classification",
    "href": "blog/2023-12-03_discourse-processing/index.html#coreference-resolution-as-supervised-classification",
    "title": "Discourse processing",
    "section": "29 Coreference resolution as supervised classification",
    "text": "29 Coreference resolution as supervised classification\n\n\n\n\nSlide 29\n\n\n\n\nHere our training instances are going to be pairs of refering expressions, for instance pronouns and the possiblle additions, and for each pair we have to automatically ouput if is a correct or incorrect link. So if there is a correct co-reference or not. That is the classification task (binary in this case).\n\nOur training data you use data where you have correct pairs where you have correct pairs between pronouns and noun phrases and then you want to learn some features from the text that signal that these pronouns co-refer to this noun phrase.\n\nTipically the way the dataset is constructed for classification is that we would take our sentence that contains a pronoun then we take all the noun phrases in this sentence, we can extract for instance using a parser\nWe also are going to extract the noun phrases in the preceding five sentences because they also be candidates as well. Why 5 sentences because wea re limited to some extend by human cognitive capacity so actually humans cannot keep track of the pronouns for long texts. We can oly resolve the pronoun if the noun phrase has been mentioned ideally in the same sentence or in the previous sentence. 5 is even larger for us to remember\nSo in our example we would have all kind of pairs such as “He and Niall Ferguson”, “He and snappy dresser”, “He and Stephen Moss” and so on from the sentence also in the preceding 5 sentences.\n\n\n\n\n\n\nRemember what is noun phrase and examples\n\n\n\nA noun phrase is a group of words centered around a noun that functions as a single unit within a sentence. It typically consists of the noun and its modifiers. Here are some examples of noun phrases:\n\nThe big red ball\n\nNoun: ball\nModifiers: the, big, red\n\nMy favorite book\n\nNoun: book\nModifiers: my, favorite\n\nAn old, dusty suitcase\n\nNoun: suitcase\nModifiers: an, old, dusty\n\nThe talented and charismatic actor\n\nNoun: actor\nModifiers: the, talented, charismatic"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#hard-constraints-pronoun-agreement",
    "href": "blog/2023-12-03_discourse-processing/index.html#hard-constraints-pronoun-agreement",
    "title": "Discourse processing",
    "section": "30 Hard constraints: Pronoun agreement",
    "text": "30 Hard constraints: Pronoun agreement\n\n\n\n\nSlide 30\n\n\n\n\nSo then we asked to ourselves. How can we build such a system? Answer: the earlier models use hand crafted features, and those were based on how humans understand language. So there is some rules that needs to be observed and those already signal if is a correct or incorrect pairing. So for instance:\n\nPronoun agreement: here we agree with the noun phrase with the number and gender.\nHere in the first example it would be odd to say: “see what he wants” so then when we are referring that needs to be an agreement we mean in gender and also number\nIn “Kim and Sandy” here the coordinate conjunction makes the pronoun to be “they”"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#hard-constraints-reflexives",
    "href": "blog/2023-12-03_discourse-processing/index.html#hard-constraints-reflexives",
    "title": "Discourse processing",
    "section": "31 Hard constraints: Reflexives",
    "text": "31 Hard constraints: Reflexives\n\n\n\n\nSlide 31\n\n\n\n\nAnother hard constraint is “himself, or herself” or so on. These are called reflexive pronouns"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#hard-constraints-pleonastic-pronouns",
    "href": "blog/2023-12-03_discourse-processing/index.html#hard-constraints-pleonastic-pronouns",
    "title": "Discourse processing",
    "section": "32 Hard constraints: Pleonastic pronouns",
    "text": "32 Hard constraints: Pleonastic pronouns\n\n\n\n\nSlide 32\n\n\n\n\nThey are empty and do not refer to anything. In other contest “it” can actually refer to a “dog” i.e “it is hungry”. So this is a complication because we need to determine pleonastic pronouns whether they refer to something or no."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#soft-preferences-salience",
    "href": "blog/2023-12-03_discourse-processing/index.html#soft-preferences-salience",
    "title": "Discourse processing",
    "section": "33 Soft preferences: Salience",
    "text": "33 Soft preferences: Salience\n\n\n\n\nSlide 33\n\n\n\n\nThe soft preferences are not constraints but they are some factors that indicate the right linking\n\nRecency More recent antecedents ( is the text that initially introduces the referent. This is the first time that somebody is mentioned.) are preferred\nGrammatical role we are more likely to introduce an antecedent as the subject as opposed to an object. So here subjects are preferred\nRepeated mention entities that have been mentioned a lot in the text are also preferred"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#soft-preferences-salience-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#soft-preferences-salience-1",
    "title": "Discourse processing",
    "section": "34 Soft preferences: Salience",
    "text": "34 Soft preferences: Salience\n\n\n\n\nSlide 34\n\n\n\n\n\nPrallelism this means that the pronoun is in the same grammatical position as the noun phrase. So here in the same order, Freud is mention at the end and when we use the pronoun “him” at the end, then we are referring to “Freud”\nCoherence effects. Bill in this example Likes Fred, because he=Fred has a great sense of humour. So if we interpreted as having hummor is a reason to like someone then we know he=Fred. Here we are using Justitication because the we are providing evidence\n\n\n“Jhon likes Bill. He gave him an expensive gift”\n\n\nExplanation here we are explaining that because he=Bill got to Jhon an expensive gift, now Jhon likes Bill\nJustification here we are saying that Jhon likes Bill and because of that he=Jhon is buying it an expensive gift. The latter is the evidence that support the fact that Jhon likes Bill and thus the evidence of his act"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#features",
    "href": "blog/2023-12-03_discourse-processing/index.html#features",
    "title": "Discourse processing",
    "section": "35 Features",
    "text": "35 Features\n\n\n\n\nSlide 35\n\n\n\n\nFor the models that operate in handcrafted features, you can design features like the ones below:\nSlide 25: Cataphoric was defined as pronouns that appear before the referent (this is rare)\n\nSame verb: here we refer to the reflexive pronouns which must be coreferntial with a preceeding argument of the same verb\nThe distance in the text so Recency more recent antecedents are preferred\nGrammatical Role when we are more likely to introduce an antecedent as the subject as opposed to an object. So here subjects are preferred\nThe Linguistic form of the noun phrase, so is it a:\n\nProper noun: like a name or is it\nDefinite noun: such as “the scientist” or is it\nIndefinite noun: “a scientist” or is it\nPronoun itself i.e “you, she, he”"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#feature-vectors",
    "href": "blog/2023-12-03_discourse-processing/index.html#feature-vectors",
    "title": "Discourse processing",
    "section": "36 Feature vectors",
    "text": "36 Feature vectors\n\n\n\n\nSlide 36\n\n\n\n\n\nDistance: here we are referring if they are in the same sentence then dist=0 if there is one sentence before the pronoun then dist=1 and so on.\n\nWith these features we can create a feature vector and train a classifier to predict: for each pair of a pronoun and possible antecedent whether is a correct or incorrect link. This is the task. The task is to transfer the dataset using those features and then learn from them.\nSo here for instance we see construct per each pair a vector (the row in the table) and we fill in the features, (each column) according to this data pair i.e (him, Niall F.)\n\nRemember each feature would be either: “Binary” or “Discrete”, see slide above"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#problems-with-simple-classification-model",
    "href": "blog/2023-12-03_discourse-processing/index.html#problems-with-simple-classification-model",
    "title": "Discourse processing",
    "section": "37 Problems with simple classification model",
    "text": "37 Problems with simple classification model\n\n\n\n\nSlide 37\n\n\n\n\n\nYou dont take into consideration anything else, but just consider pairs in isolations. So you cannot model “repeated effect”\n\nYou have problems because you take the isolated pairs and you simply predict for them. The problem is that you cannot implement repeated mention effects, you now how frequent a particular reference expression was, but you just consider pairs in isolation from the rest of the text\n\nIn real text we dont have just co-reference pairs but instead co-reference chains\n\nYou are also going to have chains of references. So first you start talking about one person, then using another pronoun we talk about it again, then another noun phrase and so on. This is a chain co-reference. Ideally to build a fuller model of this we actually want to process those chains as well."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#neural-end-to-end-coreference-resolution",
    "href": "blog/2023-12-03_discourse-processing/index.html#neural-end-to-end-coreference-resolution",
    "title": "Discourse processing",
    "section": "38 Neural end-to-end coreference resolution",
    "text": "38 Neural end-to-end coreference resolution\n\n\n\n\nSlide 38\n\n\n\n\nHere the set up is different from the previous set-up we discuss, they did not just consider pairs but instead of a parser we want the whole thing to be end-to-end trainable. So we will take the text and the we will divide the text up into spans. So then they define their spans simply as n-grams, so sequence of different length.\nSo take the text, split it up in possible spans of different length. Then say for instace at a given moment in your classification you are at a particular span in the text. For instance lets call it A. Then what they do is only consider things backwards, so they did not consider cataphors, so the task is out of pspans that proceed that span we want to find which ones or which one it would be the correct co-reference link so the correct antecedent.\nSo this is a task of classifying still pairs in a sense of the span in question pair with all previous spans and classifying them as corrct or not. What was different from their approach as well was that they output a probability distribution over all of the previous spans in that text. So not only which pair is correct but also the probability of each of the spans to be a correct co-reference link\nAll these method they called the mention-ranking paradigm.\nOne differentiating factor is that they did not only consider pronouns i.e “he or she” but also they consider co-reference between different noun phrases which is something that you need to do in your real word.\n\nRemember: discourse is referred to the analysis of how sentences and phrases are linked together to form a coherent and meaningful conversation or written passage."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#task-definition",
    "href": "blog/2023-12-03_discourse-processing/index.html#task-definition",
    "title": "Discourse processing",
    "section": "39 Task definition",
    "text": "39 Task definition\n\n\n\n\nSlide 39\n\n\n\n\nFor each span i which is in the text, we want to assign a correct antecedent \\(y_i\\) that is selected out of the dataset\nThis dataset is going to be all the spans that preceed span ‘i’ (so only spans before ‘i’). In the set there is \\(\\epsilon\\) indicating the emtpy span, so the empty token rather, it is included for the model to be able to indicate that a particular span is non-referential. There is going to be a lot of spans that are not referential, so they do not refer anything in the preceeding set. For instance when a new entity is introduced in the text, then there is not gonna be an antecedent befora that. If is a verb for instance then it is not refering to anything else before. It is also going to be the case for predicative phrases i.e ‘snappy dresser’ not all noun phrases references are. So basically we need epsilon for the model to tell us when a span is not refering to any of the spans that we have seen before, meaning this span does not co-refer to them.\nIn order to assign an antecedent this model scores pairs of spans i and j (the candidate antecedent). we are going to assing an score that measure the strenght of co-reference of that link. To ctually compute a distribution over all the possible antecedents, we will take the scores and we pass it through a softmax function\nSo this is what the model ouputs, it ouputs a distribution over previously seen spans at a given span in the text.\nSo we take our text, we split it in spans, of different lenght (we decide how many we want). We collect and say we are pointing in to a certain span i, then we collect all possible pairs of spans that occur before in the text. All posible one word spans, all possible two words spans, all possible three words spans, all possible ten words spans. These are the spans that are before our choice span. Now the model assumes that those are all possible antecedents of our span ‘i’ and that is our dataset Y. Remember then, we take all those spans and we want to rank them by the probability of co-reference link"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#computing-the-score-s",
    "href": "blog/2023-12-03_discourse-processing/index.html#computing-the-score-s",
    "title": "Discourse processing",
    "section": "40 Computing the score s",
    "text": "40 Computing the score s\n\n\n\n\nSlide 40\n\n\n\n\n\nm(i): tell us how likely a particular span is to be a valid mention because they are taking all possible spans of certain length they are going to have some gramatical phrases they will have some verbs that do not refer but what we want is noun phrases that can actually refer to somthing. So in a sense this function m, serves as a filter. A filter because we want to learn what are the properties of a good span that could potentially be a valid mention or referring expression.\nc(i,j) stands for candidate. This indicates whether ‘j’ is the antecedent of ‘i’.\ns(i,j) here we compute the score for those mentioned scores.\n\nThe \\(s(i, \\epsilon)\\) means that if a model predicts a positive score for one of the spans then that spans is potentially co-referential, if is a negative score then it should abstain and not predict anything at all because then it just predict the empty span."
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#computing-the-scoring-functions-m-and-c",
    "href": "blog/2023-12-03_discourse-processing/index.html#computing-the-scoring-functions-m-and-c",
    "title": "Discourse processing",
    "section": "41 Computing the scoring functions m and c",
    "text": "41 Computing the scoring functions m and c\n\n\n\n\nSlide 41\n\n\n\n\nHow do we compute m and c?\nOur representations of spans comes from our LSTM model. So we have an LSTM encoder and then from that we compute our span representations. The way the spans are represented is the vector \\(g\\), so here we take the hidden state of our start token in our span, so the first word in the span, then we take the hidden state of the last word in our span, then we use an attention mechanism to compute an overall span representation\nThe phi in g_i is a hand crafted feature which is the lenght of the span. So then g_i si the represntation of the span\nour function c is a joint representation of the two spans and we want to elarn some feature interaction from these join representation to tell us something about co-reference"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#learning-span-representations",
    "href": "blog/2023-12-03_discourse-processing/index.html#learning-span-representations",
    "title": "Discourse processing",
    "section": "42 Learning span representations",
    "text": "42 Learning span representations\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#computing-the-score",
    "href": "blog/2023-12-03_discourse-processing/index.html#computing-the-score",
    "title": "Discourse processing",
    "section": "43 Computing the score",
    "text": "43 Computing the score\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#computing-the-score-1",
    "href": "blog/2023-12-03_discourse-processing/index.html#computing-the-score-1",
    "title": "Discourse processing",
    "section": "44 Computing the score",
    "text": "44 Computing the score\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#model-output-with-attention-visualised",
    "href": "blog/2023-12-03_discourse-processing/index.html#model-output-with-attention-visualised",
    "title": "Discourse processing",
    "section": "45 Model output with attention visualised",
    "text": "45 Model output with attention visualised\n\n\n\n\nSlide 45\n\n\n\n\nHere we have correct pairs in bold and attention is highlighted. For instance we have the noun phrase: “A fire in a Bangladeshi garment factory” this was identified as a whole span, then it found the co-reference the blaze. Fire=blaze, because we have word embeddings we already start with semantic information then the model was able to connect these two phrases"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#examples-of-errors",
    "href": "blog/2023-12-03_discourse-processing/index.html#examples-of-errors",
    "title": "Discourse processing",
    "section": "46 Examples of errors",
    "text": "46 Examples of errors\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-12-03_discourse-processing/index.html#acknowledgement",
    "href": "blog/2023-12-03_discourse-processing/index.html#acknowledgement",
    "title": "Discourse processing",
    "section": "47 Acknowledgement",
    "text": "47 Acknowledgement\n\n\n\n\nSlide 47"
  },
  {
    "objectID": "blog/2023-10-12_bayes-rule/index.html",
    "href": "blog/2023-10-12_bayes-rule/index.html",
    "title": "Bayes Rule Equation",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Probability\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Probability\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 12, 2023"
  },
  {
    "objectID": "blog/2023-10-12_bayes-rule/index.html#bayes-theorem",
    "href": "blog/2023-10-12_bayes-rule/index.html#bayes-theorem",
    "title": "Bayes Rule Equation",
    "section": "1 Bayes Theorem",
    "text": "1 Bayes Theorem\n\\[\n\\begin{align}\np(C|X) = \\frac{P(X|C)P(C)}{P(X)} \\\\\n\\end{align}\n\\]\nWhere:\n\n\\(P(C|X)\\) is the Posterior (Given the data what is the prob of being class C_k)\n\\(P(X|C)\\) is the Likelihood (How the data is distributed given C)\n\\(P(C)\\) is the Prior\n\\(P(X)\\) is the Evidence/ Marginal likelihood\n\nThe evidence \\(P(X)\\) can also be decomposed in:\n\\[\n\\begin{align}\nP(X) &= \\sum_{j}{}P(X,C_j)\\\\\n     &= \\sum_{j}{}P(X|C_j)P(C_j) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2023-11-25_why-using-logit-scaling-in-softmax/index.html",
    "href": "blog/2023-11-25_why-using-logit-scaling-in-softmax/index.html",
    "title": "Why using Logit Scaling in Softmax?",
    "section": "",
    "text": "Why using Logit Scaling in Softmax?\n        \n        \n                    \n                \n                    Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 25, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\nDiscussed in the context of Depp Learning in this section\nLet’s consider a hypothetical set of logits before applying the softmax function. For simplicity, let’s take a vector with three logits: \\((2, 5, 8)\\). The softmax function is defined as:\n\\[\nP_i = \\frac{e^{z_i / \\tau}}{\\sum_{j=1}^{N} e^{z_j / \\tau}}\n\\]\nwhere: - \\(z_i\\) is the logit for class \\(i\\), - \\(N\\) is the number of classes, - \\(\\tau\\) is the temperature.\nLet’s examine how different temperatures affect the resulting softmax probabilities:\n\nHigh Temperature (Smoothed Distribution):\n\nTemperature (\\(\\tau\\)) is high, let’s say \\(\\tau = 5\\).\nSoftmax Probabilities: \\(P_1, P_2, P_3\\) where \\(P_i\\) is the probability for class \\(i\\).\n\nlogits = [2, 5, 8]\ntau = 5.0\n\nsoftmax_probs = torch.nn.functional.softmax(torch.tensor(logits) / tau, dim=-1)\nprint(softmax_probs.numpy())\nThe output will be a set of probabilities where the distribution is “smoothed” due to the high temperature:\n[0.04661262, 0.23688284, 0.71650454]\nNotice that the probabilities are more evenly distributed among the classes.\nLow Temperature (Sharp Distribution):\n\nTemperature (\\(\\tau\\)) is low, let’s say \\(\\tau = 0.5\\).\nSoftmax Probabilities: \\(P_1, P_2, P_3\\)\n\ntau = 0.5\n\nsoftmax_probs = torch.nn.functional.softmax(torch.tensor(logits) / tau, dim=-1)\nprint(softmax_probs.numpy())\nThe output will be a set of probabilities where the distribution is “sharpened” due to the low temperature:\n[0.00242826, 0.04741446, 0.95015728]\nNotice that the probability for the class with the highest logit (class 3, with logit 8) is much higher compared to the others.\n # pairwise similarities between ima_feat and text_feat -&gt; softamx -&gt; scaling\n # scaling: self.logit_scale = self.clip_model.logit_scale.exp().detach() aka e^(logit_scale)\n # ^ makes softmax have a temperture term in its formula\n #   - high tau -&gt; smothing probs\n #   - low  tau -&gt; peaked probs\n\nIn these examples, adjusting the temperature influences the degree of exploration (smoothness) versus exploitation (sharpness) in the distribution of probabilities. Higher temperatures encourage a more uniform distribution, while lower temperatures emphasize the most confident predictions. This temperature parameter is often used in training as a form of regularization to control the model’s uncertainty."
  },
  {
    "objectID": "blog/2023-01-29_displaying-jupyter-notebooks/index.html",
    "href": "blog/2023-01-29_displaying-jupyter-notebooks/index.html",
    "title": "Displaying jupyter notebooks",
    "section": "",
    "text": "Example of a simple blog post using Jupyter Notebooks\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Analysis\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Testing\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Analysis\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Testing\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          January 29, 2023"
  },
  {
    "objectID": "blog/2023-01-29_displaying-jupyter-notebooks/index.html#polar-demo",
    "href": "blog/2023-01-29_displaying-jupyter-notebooks/index.html#polar-demo",
    "title": "Displaying jupyter notebooks",
    "section": "Polar Demo",
    "text": "Polar Demo\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 1 * np.pi * r\nfig, ax = plt.subplots(\n    subplot_kw = {'projection': 'polar'}\n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\nHow to transform into a plot\n\nplt.figure(figsize=(4, 2))\n_ = plt.plot([3, 4, 2, 5])\n\n\n\n\n\nThis is gonna be a normal plot. The figure above\nWhen people think about using artificial intelligence (AI) for writing, they immediately jump to 1500-word articles and lengthy social media posts. And yes, these are use cases that AI writing assistants can help with.\nBut it’s only a fraction of what this technology can do.\nA high-quality paragraph generator transforms everyone on your team into a robust and well-rounded writer. Now anyone can create informative, engaging, and persuasive content one paragraph at a time – whether responding to a lead via email or creating a thought leadership piece to go viral.\nThat’s why, in a moment, we’ll show you how to use Cop.aiI’s paragraph generator to build any text-based asset you need.\nBefore diving in, though, let’s cover the basics and answer the following: what’s a paragraph generator, how do they work, and why would you want to use one?\n\n\n\nAnother Big boy\nWhen people think about using artificial intelligence (AI) for writing, they immediately jump to 1500-word articles and lengthy social media posts. And yes, these are use cases that AI writing assistants can help with.\nBut it’s only a fraction of what this technology can do.\nA high-quality paragraph generator transforms everyone on your team into a robust and well-rounded writer. Now anyone can create informative, engaging, and persuasive content one paragraph at a time – whether responding to a lead via email or creating a thought leadership piece to go viral.\nThat’s why, in a moment, we’ll show you how to use Cop.aiI’s paragraph generator to build any text-based asset you need.\nBefore diving in, though, let’s cover the basics and answer the following: what’s a paragraph generator, how do they work, and why would you want to use one?\n\n\n2.1 My template for you"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html",
    "href": "blog/2023-10-04_neural-networks/index.html",
    "title": "Neural networks",
    "section": "",
    "text": "Lecture Notes UvA on 26-9-2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 4, 2023\nThe only difference between the normal perceptron and the logistic regression its how we compute the activation function the f(x)\nThe perceptron cannot solve the XOR problem also the logistic regression. To get around this problem, we stack perceptron like in a layer fashion model. This is called Multilayer Perceptron\nBut because people start to use the logistic regression instead of the actual hardcore perceptron output (so \\(+1\\) or \\(-1\\)) then we now call it. Multilayer Logistic Regression\nSo instead of that we are gonna have different activation functions\nWe also talked about QDA this is a non linear model classifier\nWe call the computation from going to the input to the ouput as forward propagation\nThe number of parameters for Logistic Regression is \\(D+1\\) vs Gaussian Classifier contains \\(D^2\\). all these computation.\nSo when you get scared about the number of layers then you just remember the previous layers they were just a type of basis funtions and the last layer is a linear model like so:\nThe difference between linear models with adaptive basis is that the Neural Networks learn the basis functions.\nWith Neural Networks we do not have to have heuristics for this basis functions but they learn by themselves.\nWe see that to separate our data from above we can use one hidden layer with tree units:\nRecall from the previous lecture that with our Logistic Regression we neede to have some radial functions, the difference here its that we can now gotten learnt this basis functions with our Neural Network.\nNN start with low level features and keeps increasing until it calculates the features it find relevant to take a decision.\nIt is spacial invariant because you are moving your kernel along all dimensions."
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#universal-approximators",
    "href": "blog/2023-10-04_neural-networks/index.html#universal-approximators",
    "title": "Neural networks",
    "section": "1 Universal Approximators",
    "text": "1 Universal Approximators\nWe can find a number M and for the weights \\(\\textbf{w}^(2)\\) so the weights in the second layer. Here \\(M\\) represents the units aka the circles in a layer. See the green highlighted circles below\n\n\n\n\nIf we want the error to be to zero then we want the value of \\(M\\) to be larger and larger. This is because we fixed our NN to have only 2 layers so the only thing that can grow its the widht so \\(M\\) so the number of neurons (units). So with M its hard if you compute plenty of them because then you need a lot of GPUs.\nThe summary of the Universal Approximator is that width \\(M\\) is the one that matter. It is just one hidden layer and if you can make it big enough, so if you can increase \\(M\\) enough then you can fit approximate almost anything\n\n\n\n\nHere \\(\\textbf{w}^2 \\in \\mathbb{R}^{Mx1}\\) because if we want to compute only one single value at the end of the NN we need the dimensions of the vector to align see figure above"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#deep-neural-nets-and-shallow-neural-nets",
    "href": "blog/2023-10-04_neural-networks/index.html#deep-neural-nets-and-shallow-neural-nets",
    "title": "Neural networks",
    "section": "2 Deep Neural Nets and Shallow Neural Nets",
    "text": "2 Deep Neural Nets and Shallow Neural Nets\n\n\n\nHere we are saying that if we want to conserve the error lower as much as a Deep Neural Net, so that we can have a Shallow Neural Net then we need to increase the number of \\(M\\) or number of weights. But by doing so this scales exponentially.\nTo growth complexity you have to increase the width, but instead of this you can do it in the depth. As we will see in the slide below, Depth gives you more complexity so its preferable to increase depth instead of width\n\nWidth: \\(M\\) number of weights\nIf you want the error small then increase this number. However if you go this route the number \\(M(\\varepsilon)\\) grows exponentially\nDepth: \\(l\\) number of hidden layers.\nHere it gives you more complexity, so its recommended, End result have deeper NNs.\n\n\nThe point of the image above is to show that with the same model (3 hidden units and 1 linesr ouput unit) which is the Neural Network we can approximate these 4 functions."
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#expressive-power-relu-networks",
    "href": "blog/2023-10-04_neural-networks/index.html#expressive-power-relu-networks",
    "title": "Neural networks",
    "section": "3 Expressive power ReLU networks",
    "text": "3 Expressive power ReLU networks"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#network-training-regression",
    "href": "blog/2023-10-04_neural-networks/index.html#network-training-regression",
    "title": "Neural networks",
    "section": "4 Network Training: Regression",
    "text": "4 Network Training: Regression\nRemember computing the Maximum Likelihood is the same as the minimum of the negative log likelihood, that is why there is a minus on front the \\(\\ln p(\\textbf{t}|\\textbf{W},\\textbf{w})\\)"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#network-training",
    "href": "blog/2023-10-04_neural-networks/index.html#network-training",
    "title": "Neural networks",
    "section": "5 Network Training",
    "text": "5 Network Training\n\n5.1 Regression: Network Training\n\n\n\n\n\n5.2 Binary Classification: Network Training\n\n\n\n5.3 Classification with K classes: Network Training\n\n\n\nFor further explanation in class distribution take a look at here"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#losses-overview",
    "href": "blog/2023-10-04_neural-networks/index.html#losses-overview",
    "title": "Neural networks",
    "section": "6 Losses overview",
    "text": "6 Losses overview"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#training-neural-networks",
    "href": "blog/2023-10-04_neural-networks/index.html#training-neural-networks",
    "title": "Neural networks",
    "section": "7 Training Neural Networks",
    "text": "7 Training Neural Networks\nBecause we cannot find easily the mathematically solutions for training the NN, we use Gradient Descent\n\n\n\n\nResult of forward propagation. Depends on all the NN weights —&gt; Lots of local minima! It results in lots of local minima because we are updating simultaneously all the weights\nThat means that the loss function depends on all the parameters (all the weights in the NN) thus we have a lot of parameters to update.\n\n\n\n\nHere for instance we have 2 local minima and one unique global minima"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#neural-network-optimization-surface",
    "href": "blog/2023-10-04_neural-networks/index.html#neural-network-optimization-surface",
    "title": "Neural networks",
    "section": "8 Neural Network Optimization Surface",
    "text": "8 Neural Network Optimization Surface\n\n\n\n\nDespite these crazy optimization landscapes, gradient descent works amazingly well!\nFor large networks, many of the critical points are saddle-points, not local minima.\n\nAs our dimensionality increases, the saddle points will grow with respect to the minimum, but the minimam will also grow\n\n\n\nBecause of all these variation we want to always report uncertainties on performance. This uncertainties comes from randomness in initialization and Stochastic Gradient Descend SGD.\nThe idea is to report error by averaging over all these models in the same column.\nNow, we talked about making the learning rate higher at start and then slowly decreased it at training. This what we talk in the next topic:"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#cyclic-learning-rates",
    "href": "blog/2023-10-04_neural-networks/index.html#cyclic-learning-rates",
    "title": "Neural networks",
    "section": "9 Cyclic Learning Rates",
    "text": "9 Cyclic Learning Rates\nIt may seem crazy making the learning rate cyclic, so increase it again then lower it then increase it again. But this results in exploration so that you don’t end up in a local minima.\n\n\n\n\n\n\nWith cyclic learning rate we could explore three local minima\n\nNote: We used to compute Logistic Regression with Gradient Descent, here its the same with NN. More mathematically we cannot compute a closed form of the weights hence we use GD."
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#chain-rule-through-layers",
    "href": "blog/2023-10-04_neural-networks/index.html#chain-rule-through-layers",
    "title": "Neural networks",
    "section": "10 Chain Rule through Layers",
    "text": "10 Chain Rule through Layers\n\nForward Propagation: is to evaluate the full network from inputs to ouputs\nBackward Propagation: is to send error signals back though the network\n\n\n\n\nRemember \\(h_l = \\sigma(a)\\). Meaning the ouput of the sigmoid is the new \\(h_l\\). With this in mind because we want to tune the parameters aka weights we send the error back to the inputs, this as we said before it’s called backpropagation.\nNow, because we want to send the error back so that we can reduce it, we would compute the derivative. Here is the same, we will compute the derivatives and because the NN its like a chain of multiplication due to the chain rule, here we can make use of optimization techniques. That is:\n\nBackpropagation is simply the chain rule implemented with cached values of intermediate computations.\nWe have the tradeoff between computational complexity for memory. In other words we have saved in computation complexity (carry out many derivatives) because we have cached these \\(h_{l,j}\\) which is exactly the result of computing the derivatives to reduce the error and thus carry out backpropagation.\n\nIn summary when we do backprogation due to the symmetry of the hidden layers, we can save in computing all the times the derivatives and instead trade it for memory to cache these \\(h_{l,j}\\)\nThe above is what distinguishes the chain rule from backpropagation. Backpropagation uses the notion of reusing information."
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#downside-numerical-issues",
    "href": "blog/2023-10-04_neural-networks/index.html#downside-numerical-issues",
    "title": "Neural networks",
    "section": "11 Downside: Numerical Issues",
    "text": "11 Downside: Numerical Issues\n\n\n\nTwo things can go wrong with this approach:\n\nExploding gradients:\n\nProblem: The multiplication of an intermediate derivate value could be large so then wehn you continue multiplying with the rest of the layers then it will become a huge number.\n\nSolution: can usually be handled by ‘clipping’ values or regularizing weights to be small.\n\nVanishing gradients:\n\nProblem: When the ouput of the sigmoid is close to \\(1\\) or \\(0\\) we would have that the derivative \\(h_{l,j}*(1-h_{l,j})\\) where recall that \\(\\textbf{h}_l=\\sigma(\\textbf{a})\\) meaning when sigmoid evaluates to this extremes then the derivative would become zero. In other words, if zero because \\(h_{l,j}\\) can be \\(1\\) or \\(0\\) then all derivatives to earlier layers will be zero. See figure below:\n\n\n\n\nThe figure above then means that if one derivative becomes zero then learning cannot happen.\n\nPossible Solution1: there is no proper solution because if you say you want to clip this derivative not to yield \\(1\\) or \\(0\\) then your next question its to how much close to these extremes I should set the derivative. (In practice however the sigmoid will not evaluate to perfect \\(1\\) or \\(0\\) but it will be small enough to make the multiplication of the other derivatives in the chain rule significantly deleterious to proceed training).\n\nPossible Solution2: In the neural networks we add a line that connect the current output layer \\(h_l\\) with the previous outout layer \\(h_{l-1}\\)"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#scalar-neural-network",
    "href": "blog/2023-10-04_neural-networks/index.html#scalar-neural-network",
    "title": "Neural networks",
    "section": "12 Scalar Neural Network",
    "text": "12 Scalar Neural Network\n\n\n\n\nRecall that when we do linear regression we do not need to have an activation function \\(f\\) like softmax or logistic, we just have the identity which means \\(f(w_2 \\cdot h)\\) becomes just \\(w_2 \\cdot h\\).\n\n\n\n\n\n12.1 Computing the Error loss on \\(w_1\\)\n\n\n\nIf we would compute the error but now with activation function \\(a\\) equals Relu\n\n\n\nThis could be a bad idea because if \\(w_1 \\cdot x\\) is lower than \\(0\\) then the whole error loss would be evaluated to zero and thus cannot train the network because you keep getting derivatives of zero. This is called the Dead Relu problem. In practice people initialize the input to a positive value. Also people use Leaky Relu depicted below"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#vector-neural-network",
    "href": "blog/2023-10-04_neural-networks/index.html#vector-neural-network",
    "title": "Neural networks",
    "section": "13 Vector Neural Network",
    "text": "13 Vector Neural Network\n\nWe will have a scalar input and scalar ouput but with hidden layers as vectors.\n\n\n\n\n\n\n\nNote\n\n\n\n\nDerivative is a row vector: \\[\n\\begin{align}\n\\frac{\\partial f}{\\partial \\textbf{x}} = \\left[\\frac{\\partial f}{\\partial x_1},...,\\frac{\\partial f}{\\partial x_D} \\right]\n\\end{align}\n\\]\nGradient is the transpose of the Derivative vector: \\[\n\\begin{align}\n\\nabla_{w_1}f = \\left[\\frac{\\partial f}{\\partial \\textbf{w}_1} \\right]^T\n\\end{align}\n\\] We do the transpose because we are tryng to get the same shape as our paramter \\(\\textbf{W}\\). So in this case if \\(W\\) would have been a two-row vector then with the transpose we ensure that."
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#computations-in-2d",
    "href": "blog/2023-10-04_neural-networks/index.html#computations-in-2d",
    "title": "Neural networks",
    "section": "14 Computations in 2D",
    "text": "14 Computations in 2D\n\n\n\nA rule of thumb is:\n\nOutput linear operator Rows\nThe model parameters as Columns"
  },
  {
    "objectID": "blog/2023-10-04_neural-networks/index.html#two-modes-forward-and-reverse",
    "href": "blog/2023-10-04_neural-networks/index.html#two-modes-forward-and-reverse",
    "title": "Neural networks",
    "section": "15 Two Modes: Forward and Reverse",
    "text": "15 Two Modes: Forward and Reverse\nIn Pytorch, you can compute the forward or reverse computation we saw before. The only difference is which direction you cache from.\n\nForward mode: you start with the partial derivatives at the beginning of the NN. For instance in the image below, from dh1/dw1 (the first layer) to df/dh2 (the last layer).\n\nBetter when ouput dim &gt;&gt; input dim\n\n\n\n\n\n\nReverse mode: you start from the ouput to the input.\n\nBetter when ouput dim &lt;&lt; input dim\n\n\n\n\n\n\nThis is telling us that i.e to be more efficient in an image classifier then you would prefer the Reverse mode because you have that your input would be a huge array of numbers whereas your ouput would be a few classes. So here it applies: dim ouput &lt;&lt; dim input.\nThe reason for ie in forward mode is that you would compute small derivatives because your input is small so then you delay to compute much more multiplication to the end\nAnother examples why is more efficient, say your outputs &gt;&gt; inputs, then if you start by computing the ouput derivatives then you are going to accumulate and carry all those dimensions due to the large ouput derivatives. Then you carry all these computations to the input layer that has fewer computations but you already have a hug baggage and its not efficient. Summary: you want to do the large multiplication until the end so that you do not carry unnecessary derivation."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 7, 2023"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#re-cap-graph-representation-for-us",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#re-cap-graph-representation-for-us",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "1 Re Cap Graph representation for us",
    "text": "1 Re Cap Graph representation for us\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#title",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#title",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "2 Title",
    "text": "2 Title\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#recap-sss-yyy",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#recap-sss-yyy",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "3 Recap sss yyy",
    "text": "3 Recap sss yyy\n\n\n\n\nSlide 10"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#title-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#title-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "4 Title",
    "text": "4 Title\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#todays-lecture-overview",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#todays-lecture-overview",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "5 Today’s lecture Overview",
    "text": "5 Today’s lecture Overview\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#what-is-generative-modelling",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#what-is-generative-modelling",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "6 What is generative modelling?",
    "text": "6 What is generative modelling?\n\n\n\n\nSlide 13\n\n\n\n\n\nGeneral: - p(x) is the probability distribution of the data itself.\n\nDiscriminative model: discriminate between different kinds of data instances - you predict \\(y\\) given \\(x\\), where \\(y\\) could be the label i.e what kind of dog is in the image and \\(x\\) is the data. - p(y|x) Its aim is to model the decision boundary (whether is A or B, the cat class or the dog class) or the relationship between input features and the output directly. - p(y|x) says get me a certain label conditional on some inputs. - Typically more effective when the primary goal is to perform a specific task, such as classification or regression.\nGenerative models: can generate new data instances. - They focus on estimating the probability of observing a particular set of input features - p(x) says how probable an image is to be i.e a dog - Can be used for tasks like data generation, missing data imputation, and anomaly detection. - capture the joint probability p(X, Y), or just p(X) if there are no labels.\nThe difference:\nA generative model could generate new photos of animals that look like real animals, while a discriminative model could tell a dog from a cat. GANs are just one kind of generative model.\nAbout probability dsitributions:\nA Discriminative classifier like a decision tree can label an instance without assigning a probability to that label. Such a classifier would still be a model because the distribution of all predicted labels would model the real distribution of labels in the data.\nA Generative model can model a distribution by producing convincing “fake” data that looks like it’s drawn from that distribution.\n\n\n\n\n\n\nGenerative models are Hard, they model more\n\n\n\n\n\nGenerative models tackle a more difficult task than analogous discriminative models. Generative models have to model more.\nA generative model for images might capture correlations like “things that look like boats are probably going to appear near things that look like water” and “eyes are unlikely to appear on foreheads.” These are very complicated distributions.\nIn contrast, a discriminative model might learn the difference between “sailboat” or “not sailboat” by just looking for a few tell-tale patterns. It could ignore many of the correlations that the generative model must get right.\nDiscriminative models try to draw boundaries in the data space, while generative models try to model how data is placed throughout the space. For example, the following diagram shows discriminative and generative models of handwritten digits:\n\n\n\n\nThe discriminative model tries to tell the difference between handwritten 0’s and 1’s by drawing a line in the data space. If it gets the line right, it can distinguish 0’s from 1’s without ever having to model exactly where the instances are placed in the data space on either side of the line.\nIn contrast, the generative model tries to produce convincing 1’s and 0’s by generating digits that fall close to their real counterparts in the data space. It has to model the distribution throughout the data space.\nGANs offer an effective way to train such rich models to resemble a real distribution. To understand how they work we’ll need to understand the basic structure of a GAN."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-generative-modelling",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-generative-modelling",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "7 Why generative modelling?",
    "text": "7 Why generative modelling?\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-generative-modelling-more-reasons",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-generative-modelling-more-reasons",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "8 Why generative modelling? More reasons",
    "text": "8 Why generative modelling? More reasons\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#bayes-rule-if-we-have-generative-models-we-have-it-all",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#bayes-rule-if-we-have-generative-models-we-have-it-all",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "9 Bayes rule: if we have generative models: we have it all",
    "text": "9 Bayes rule: if we have generative models: we have it all\n\n\n\n\nSlide 16\n\n\n\n\np(y|x) was all ConvNets, the transformers and all models before.\np(x) is parametrize with theta"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#a-map-of-generative-models",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#a-map-of-generative-models",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "10 A map of generative models",
    "text": "10 A map of generative models\n\n\n\n\nSlide 17\n\n\n\n\nHere with GANs you can sample images but you cannot say how likely a given image is\nTypes of Generative models\n\n\n\n\nLil’s Blog"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#title-2",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#title-2",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "11 Title",
    "text": "11 Title\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-shape",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-shape",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "12 Autoencoders: shape",
    "text": "12 Autoencoders: shape\n\n\n\n\nSlide 19\n\n\n\n\nIt is called Autoencoder because the ouput has the same size as the input\nBecause of the bottle neck it will only learn what is important to reconstruct the input back, which could be for instance noise"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-structure",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-structure",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "13 Autoencoders: structure",
    "text": "13 Autoencoders: structure\n\n\n\n\nSlide 20\n\n\n\n\nFor instance CNN because it is important for us to have in every layer the proper dimensions"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#learning-autoencoders",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#learning-autoencoders",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "14 Learning Autoencoders",
    "text": "14 Learning Autoencoders\n\n\n\n\nSlide 21\n\n\n\n\nHere is an unsupervised loss, because we do not use any labels"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-why-though",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-why-though",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "15 Autoencoders: why though?",
    "text": "15 Autoencoders: why though?\n\n\n\n\nSlide 22\n\n\n\n\nThe encoder learns to compress the input but compress it just enougth such that the decoder can reconstructed again"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-for-representation-learning-ie-use-the-encoder-afterwards",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-for-representation-learning-ie-use-the-encoder-afterwards",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "16 Autoencoders for representation learning (ie use the encoder afterwards)",
    "text": "16 Autoencoders for representation learning (ie use the encoder afterwards)\n\n\n\n\nSlide 23\n\n\n\n\nThe loss does not have any dependency on labels, no prior knowledge"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-for-representation-learning",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-for-representation-learning",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "17 Autoencoders for representation learning",
    "text": "17 Autoencoders for representation learning\n\n\n\n\nSlide 24\n\n\n\n\nCompared to PCA approach the colors are better separated, both use same data. Except that the autoencoder uses a NN"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-for-representation-learning-13years-later-bigbigan",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoders-for-representation-learning-13years-later-bigbigan",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "18 “Autoencoders” for representation learning (13years later): BigBiGAN",
    "text": "18 “Autoencoders” for representation learning (13years later): BigBiGAN\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#dimensionality-of-latent-space",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#dimensionality-of-latent-space",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "19 Dimensionality of latent space",
    "text": "19 Dimensionality of latent space\n\n\n\n\nSlide 26\n\n\n\n\nIf the dimensionality of the latent is too low then you cannot reconstruct all the details, because there is so much information that you can compress in a 2D space"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#quiz",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#quiz",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "20 Quiz:",
    "text": "20 Quiz:\n\n\n\n\nSlide 27\n\n\n\n\n\nTrue: because PCA minimizes the L2 norm so it has the same problems as Least Square fitting. So L2 looks like a quadratic function depending how far are you from the thing that you are trying to fit. So all the stuff that is further away gets quadratically exaggerated.\nNot quite True: PCA gives you linear combinations of features that could be interpretable but mostly they are not because for example you want to estimate the price of a house, then what you end up is you have 5 times the size of the house - 0.2 the location of the house, which then you get a feature with positive and negative coefficients which is not interpretable"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#pca-refresher",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#pca-refresher",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "21 PCA Refresher",
    "text": "21 PCA Refresher\n\n\n\n\nSlide 28\n\n\n\n\nContinue this explanation here"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoder-applications",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#autoencoder-applications",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "22 Autoencoder applications",
    "text": "22 Autoencoder applications\n\n\n\n\n\n\n\n\nSlide 29\n\n\n\n\nAutoencoder do not need prior knowledge abour invariances, or augmentations."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-can-they-not-generate-new-data-points",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-can-they-not-generate-new-data-points",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "23 Why can they not generate new data points?",
    "text": "23 Why can they not generate new data points?\n\n\n\n\nSlide 30\n\n\n\n\n\nThey can reconstruct the input: encoder\nThey cannot generate new data: decoder\n\nWe cannot generate new data because the latent representation can be extremely “entagled”. For instance there is no way to know if i.e the 1st dimension is ranging from,…. So we don’t know how the dimensions are related.\nThat means we do not know how we would sample a new datapoint from this latent space because the NN has been trained to reconstruct the input. It has not been given any instruction that if I take a different input in this latent space that it should still reconstruct something."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#what-we-will-arrive-at-in-this-lecture",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#what-we-will-arrive-at-in-this-lecture",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "24 What we will arrive at in this lecture",
    "text": "24 What we will arrive at in this lecture\n\n\n\n\nSlide 31\n\n\n\n\nNow we parametrized the latent z with \\(\\mu, \\sigma\\), those are the variables that will be coming from a Gaussian and with these variables we are parametrize a latent distribution over the latent p(z)\nWith this p(z) data distribution we can sample a sample ‘z’, push it trough the decoder and then we can get an ouput out.\nVAE sample from the data and get a probability for a particular image"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#recent-vae-use-cases",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#recent-vae-use-cases",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "25 Recent VAE use cases",
    "text": "25 Recent VAE use cases\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#for-vaes-we-need-to-understand-latent-variable-models",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#for-vaes-we-need-to-understand-latent-variable-models",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "26 For VAEs we need to understand latent variable models",
    "text": "26 For VAEs we need to understand latent variable models\n\n\n\n\nSlide 33\n\n\n\n\n\n\n\n\n\n\nNotes on this slide\n\n\n\n\n\nYou first sample z and that generates with a different distribution generates your observed data x\nGoal is to compute:\n\n\\(p(x)\\) likelihood\n\\(p(z|x)\\) posterior distribution\n\n\n\n\n\n\\(p_{\\theta}(z)\\): Prior, distribution for the latent variable\n\\(p_{\\theta}(x|z)\\): Likelihood, connects the latent variable wot the observation\n\\(p_{\\theta}(z|x)\\): Posterior\n\n\n\n\n\nSource at Lil’s Blog\n\n\n\n\n\n\n\n\nSource in my blog about: Latent models\n\n\n\n\n\n\n\n\n\n\nSummary of VAE\n\n\n\nVariational Autoencoders (VAEs) are a type of generative model that learn to represent data in a lower-dimensional latent space and can generate new data that resemble the input data. Here’s an overview of the formulas involved, the known and unknown distributions, what corresponds to each part of the VAE architecture, and what needs to be computed, trained, or optimized.\n\n26.1 Known Distributions:\n\nPrior Distribution of Latent Variables (p(z)):\n\nTypically assumed to be a standard normal distribution: \\(p(z) = \\mathcal{N}(z; 0, I)\\).\nThis is a design choice and is known a priori.\n\nLikelihood of Data Given Latent Variables (p(x|z)):\n\nThis is modeled by the decoder network, usually parameterized as a normal distribution \\(\\mathcal{N}(x; \\mu_{\\theta}(z), \\sigma^2_{\\theta}(z)I)\\) where \\(\\mu_{\\theta}(z)\\) and \\(\\sigma^2_{\\theta}(z)\\) are the outputs of the decoder.\nThe parameters \\(\\theta\\) of the decoder network that determine this distribution are unknown and need to be learned.\n\n\n\n\n26.2 Unknown Distributions:\n\nPosterior Distribution of Latent Variables Given Data (p(z|x)):\n\nThis is the true but intractable distribution we want to approximate.\nIt cannot be computed directly in most cases.\n\n\n\n\n26.3 Approximate Distribution:\n\nVariational Approximation to the Posterior (q(z|x)):\n\nThis is approximated by the encoder network, usually parameterized as a normal distribution \\(q(z|x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\sigma^2_{\\phi}(x)I)\\) where \\(\\mu_{\\phi}(x)\\) and \\(\\sigma^2_{\\phi}(x)\\) are the outputs of the encoder.\nThe parameters \\(\\phi\\) of the encoder network that determine this distribution are unknown and need to be learned.\n\n\n\n\n26.4 Objective Function:\n\nEvidence Lower Bound (ELBO):\n\nThe ELBO is the objective function that VAEs maximize. It is a lower bound on the logarithm of the marginal likelihood of the data \\(p(x)\\).\nThe ELBO is given by: \\(\\text{ELBO} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\text{KL}[q(z|x) || p(z)]\\)\nThe first term is the expected log likelihood of the data given the latent variables, which encourages the decoder to reconstruct the data well.\nThe second term is the Kullback-Leibler divergence between the approximate posterior and the prior, which encourages the approximate posterior to be similar to the prior.\n\n\n\n\n26.5 Example:\n\nLet’s say we have a dataset of images and we want to model the distribution of these images using a VAE.\nEncoder (Approximate Posterior q(z|x)): An image \\(x\\) is input into the encoder, which outputs parameters \\(\\mu_{\\phi}(x)\\) and \\(\\sigma^2_{\\phi}(x)\\) of a normal distribution representing the distribution of the latent variables for this image.\nDecoder (Likelihood p(x|z)): A latent variable \\(z\\) is sampled from the approximate posterior and input into the decoder, which outputs parameters of a distribution over images. For simplicity, assume this is also a normal distribution with mean \\(\\mu_{\\theta}(z)\\) and fixed variance.\n\n\n\n26.6 What We Compute and Optimize:\n\nCompute: We can compute the ELBO for given parameters \\(\\phi\\) and \\(\\theta\\).\nOptimize: We use stochastic gradient descent or a variant to optimize the parameters \\(\\phi\\) and \\(\\theta\\) to maximize the ELBO.\nTrain: We train the encoder and decoder networks by using backpropagation based on gradients computed from the ELBO.\n\n\n\n26.7 What We Cannot Compute Directly:\n\nPosterior p(z|x): We cannot compute this directly; it’s typically intractable due to the integral over all possible values of \\(z\\).\nMarginal Likelihood p(x): We also cannot compute this directly; it involves an integral over all latent variables which is typically intractable.\n\nVAEs aim to learn the parameters of the encoder and decoder networks such that the latent space effectively captures the variations in the data, and new data can be generated that resemble the input data. We optimize the ELBO because the true posterior \\(p(z|x)\\) and the marginal likelihood \\(p(x)\\) are intractable to compute directly"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#latent-variable-models",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#latent-variable-models",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "27 Latent variable models",
    "text": "27 Latent variable models\n\n\n\n\nSlide 34\n\n\n\n\n\nFirst we sample z, so from the distribution p(z)\nSecond we sample x from p(x|z)\nThird we do “statistical Inference” where is defined as the process of going from the observations to the latent variable z, so we want to know the factors that generate the data. So we want to calculate the posterior p(z|x)"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reminder-notes-from-ml1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reminder-notes-from-ml1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "28 Reminder: notes from ML1",
    "text": "28 Reminder: notes from ML1\n\n\n\n\nSlide 35"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "29 Inference",
    "text": "29 Inference\n\n\n\n\nSlide 36\n\n\n\n\nWhen we want to do inference, so compute the posterior p(z|x) we see that this involves the join (numerator) and the marginal likelihood p(x) (denominator). To compute the marginal p(x) we use the integration over the join, but this is expensive computationally.\n\n\n\n\n\n\nWhy is expensive?"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "30 Inference",
    "text": "30 Inference\nWe will talk how to solve computing p(x), which is in the denominator for calculating the posterior aka doing inference. Inference is going from the generated observation to the latent variable z\n\n\n\n\nSlide 37\n\n\n\n\nIf we can generate data using the latent variable so from z -&gt; x. We can do the inverse and that would be inference\nSo we can have our data where x is generated from p(x) and then given this distribution (so given p(x)) we can derive p(z|x). That is expressed in the last line with formulas. That means that we arrive to p(x,z) the join which is a step to calculate the inference step aka posterior.\nThis we can do because we have have a dataset x which has been sample from the world and now we want to get a z out of the observations. The join distribution that we will be modelling is always the same because of bayes rule"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-shall-we-do-inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#why-shall-we-do-inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "31 Why shall we do inference?",
    "text": "31 Why shall we do inference?\n\n\n\n\nSlide 38\n\n\n\n\n\n\n\n\nSlide 36\n\n\n\n\nWe want to do inference, so computing the posterioir p(z|x), because then we can explain the observation. See the bulletpoints"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-via-maximum-likelihood",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-via-maximum-likelihood",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "32 Inference via maximum likelihood",
    "text": "32 Inference via maximum likelihood\n\n\n\n\nSlide 39\n\n\n\n\nMLE is all about finding the \\(\\theta\\) that maximizes modelling the distribution that represents the data \\(p_{\\theta}(x)\\)\nFor laten variable models there is no closed-form solution. It is not like you can derive with regards to theta (set it to zero and solve for theta) no you cannot.\nHere we are also assuming that all datapoints are independent see next slide\n\n\n\n\n\n\nWhy are we talking about p(x)?\n\n\n\n\n\nWe do it because it is in the denominator when we want to do inference. Remember inference was the posterior\n\n\n\n\nAnd recall we want to do inference because it was one of the 3 steps. Also recall that doing Inference is like going back because inference is defined as the process of going from the observations to the latent variable z"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reminder-why-the-sum-of-logarithms",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reminder-why-the-sum-of-logarithms",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "33 Reminder: Why the sum of logarithms?",
    "text": "33 Reminder: Why the sum of logarithms?\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#the-gradient-of-max-likelihood",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#the-gradient-of-max-likelihood",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "34 The gradient of max likelihood",
    "text": "34 The gradient of max likelihood\n\n\n\n\nSlide 41\n\n\n\n\n\n\n\n\nBecause computing the \\(\\theta\\) that maximizes \\(p_{\\theta}(x)\\) has not closed form solution, we can do SGD. Thus why in this slide we explain how to calculate the derivative of \\(log p_{\\theta}(x)\\).\n\nFirst line, second equal p(x) we just make it into a joint distribution, where we add z and integrate over the whole z.\nSecond line, we can change the integral and the gradient. This is because the gradient does not depend on thetha but dz (so the integral depends on z). Here we also use the same identiy but with p(x,z)\nThird line, we see that to compute the gradient of the marginal we need to compute the posterior \\(p(z|x)\\). Which as we saw before is very expensive"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#but-exact-inference-is-hard",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#but-exact-inference-is-hard",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "35 But: Exact inference is hard",
    "text": "35 But: Exact inference is hard\n\n\n\n\nSlide 42\n\n\n\n\nSo as we see in the last slide from third line and second as well, computing the prior is very expensive we see an example that if we have an image with 20 dimensions then we will end up summing over 1M latents (because of integral or summation). Thus is very expensive.\nSo this approach of modelling the probabilities, so the latent distribution p(z) is not feseable because we would have to do inference in this way and we cannot solve this integral even not with brute force"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#lets-take-a-breath.-where-are-we-me",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#lets-take-a-breath.-where-are-we-me",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "36 Let’s take a breath. Where are we? me",
    "text": "36 Let’s take a breath. Where are we? me\n\n\n\n\nSlide 43\n\n\n\n\nHere in the last bullet point by normalize we mean we want to sum/integram across all data but this is intractable"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "37 Variational Inference",
    "text": "37 Variational Inference\n\n\n\n\nSlide 44\n\n\n\n\nSo because we this is intractable we will aproximate the integrals, which means we go from statistical inference to optimization.\nSo basically have a NN for the inference process for us"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#approximate-inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#approximate-inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "38 Approximate inference",
    "text": "38 Approximate inference\n\n\n\n\nSlide 45\n\n\n\n\nWith Variational inference, we say we do not know the posterior p(z|x) but we can model it, we just say is a Gaussian distribution which makes everything super easy. So instead of computing the full posterior we want to approximate it for example as we said by saying is a Gaussian distribution and then we want a NN to then learn the parameters wrt to the distribution and thus now the untractable problem becomes an optimization problem\nThis method however, will make it into a single forward pass which means we cannot trade computation for accuracy. Whereas in markov chain monte carlo we just know we keep running it for long and then we will get more and more accurate but this is not an option for NNs"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#revisit-kullbackleibler-divergence",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#revisit-kullbackleibler-divergence",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "39 Revisit: Kullback—Leibler divergence",
    "text": "39 Revisit: Kullback—Leibler divergence\n\n\n\n\nSlide 46\n\n\n\n\nKL is not symetric.\nThat means if you have q fix and you optimize p. This will give you a different result if you write it like this instead of the other way. This is because you will have the mode seeking behavior of this process. That means in one case it will learn q and you observing data p(x), it will try to fit both of these peaks. While if you use the reverse KL that will just collapse into one of the modes and model a peak instead of both"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#tool-1-jensens-inequality",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#tool-1-jensens-inequality",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "40 Tool 1: Jensen’s inequality",
    "text": "40 Tool 1: Jensen’s inequality\n\n\n\n\nSlide 47\n\n\n\n\nIt says that the logarithm is monotonic. Monotonic refers tgat:\n\na&lt;b -&gt; log(a) &lt; log(b). So it conserves the order\n\nSo here the Jensens is defined for log and it just says that a line connecting two points will always be below the function"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#tool-2-monte-carlo-methods",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#tool-2-monte-carlo-methods",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "41 Tool 2: Monte Carlo methods",
    "text": "41 Tool 2: Monte Carlo methods\n\n\n\n\nSlide 48\n\n\n\n\nHere we would be doing one-sample montecarlo"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#tool-2-monte-carlo-methods-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#tool-2-monte-carlo-methods-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "42 Tool 2: Monte Carlo methods",
    "text": "42 Tool 2: Monte Carlo methods\n\n\n\n\nSlide 49\n\n\n\n\n\np(x) is a probability distribution\n\nThis integral, you cannot just find a close form solution. Because we cannot do this we will use solve it numerically and we use Montecarlo then we do an estimation fo sampling points ‘x’ from this distribution p(x) and then just plug in it, and summing then up. That is how you get an estimate of this integral So Montecarlo here just samples from p(x). The longer you do this sampling the more the samples will have been approximated by probabilities distributions and the more accurate of the integral would be.\nIf n goes to infinity you will have the exact same solution"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-inference-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-inference-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "43 Variational inference",
    "text": "43 Variational inference\n\n\n\n\nSlide 50\n\n\n\n\nThis changes the task of finding the posterior distribution into an optimization problem. So now we approximate:\n\nThe posterior with a variational posterior\n\nThe parameters phi they do not need to be NNs later on they will be NN parameters, but in general variational inference is not the case"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-inference-2",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-inference-2",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "44 Variational inference",
    "text": "44 Variational inference\n\n\n\n\nSlide 51"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#training-with-variational-inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#training-with-variational-inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "45 Training with variational inference",
    "text": "45 Training with variational inference\n\n\n\n\nSlide 52\n\n\n\n\n\n\n\n\nWe want to go up for the log likelihood p(x), if we learn a lower bound that means if we keep increasing the low bound we will actually be maximizing the log likelihood p(x).\nHere we will be training the model with Variational inference by maximizing the variational lower bound"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#important-how-to-arrive-at-the-variational-lower-bound",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#important-how-to-arrive-at-the-variational-lower-bound",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "46 Important: How to arrive at the variational lower bound",
    "text": "46 Important: How to arrive at the variational lower bound\n\n\n\n\nSlide 53\n\n\n\n\nNow we can express it as an expectation over the sample of q. We multiply everything with q, so we may as well sampling it instead. This is the montecarlo estimate of this difficult integral.\nThis expectation is in theory infinite samples but we just take a few of this"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-lower-bound",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-lower-bound",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "47 Variational lower bound",
    "text": "47 Variational lower bound\n\n\n\n\nSlide 54\n\n\n\n\nNow we can have a loss function because we got the variational lower bound.\nWe can now maximize this L instead of the untractable log p(x)"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-lower-bound-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-lower-bound-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "48 Variational lower bound",
    "text": "48 Variational lower bound\nHere note we are designing a variational posterior q(z|x) not q(z) as we have before. And we do it as simple as possible i.e Normal dist.\n\n\n\n\nSlide 55\n\n\n\n\nThis function is called the Evidence Lower Bound. It is called evidence (another term for describing the data we see)"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#derive-elbo-in-a-different-way",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#derive-elbo-in-a-different-way",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "49 Derive ELBO in a different way",
    "text": "49 Derive ELBO in a different way\n\n\n\n\nSlide 56"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#entropy-regularization",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#entropy-regularization",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "50 Entropy regularization",
    "text": "50 Entropy regularization\n\n\n\n\nSlide 57\n\n\n\n\nWe want high entropy which means we carry more information"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-gap",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-gap",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "51 Variational gap",
    "text": "51 Variational gap\n\n\n\n\nSlide 58\n\n\n\n\nThere is a gap between the lower bound, So between ELBO =second line and p(x). It turns out the gap is the second term of the third line\nSo this is the difference between the real likelihood and the lowe bound ELBO"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-gap-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-gap-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "52 Variational gap",
    "text": "52 Variational gap\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#fitting-the-variational-posterior",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#fitting-the-variational-posterior",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "53 Fitting the variational posterior",
    "text": "53 Fitting the variational posterior\n\n\n\n\nSlide 60\n\n\n\n\nThe error loss is on the RHS of the red equation, we CAN optimize. These two things are untractable:\n\np(x) expensive to compute the integral\np(z|x) we dont know\n\nSee the picture:\n\n\n\n\nBut two things that are untractable can be tractable"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#training-the-model",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#training-the-model",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "54 Training the model",
    "text": "54 Training the model\n\n\n\n\n\n\n\nSlide 61\n\n\n\n\nFirst we want to optimize the model parameters:\n\nWe want to update the model parameters thetha to increase the ELBO. This belongs to the model parameters\nBy optimizing the thetha:\n\nincrease log p(x) prob\ndecrease the varational gap\n\n\nFor this we should use the most expressive psoterior that we can such that we are as closs as possible to modelling the actual distribution. Now the question is how do we choose the variational posterior q"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#where-are-we",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#where-are-we",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "55 Where are we?",
    "text": "55 Where are we?\n\n\n\n\nSlide 62\n\n\n\n\nThe last point refers to how to choose variational posterior q that is expresive enough so that is close to the actual distribution"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#choosing-the-form-of-the-variational-posterior",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#choosing-the-form-of-the-variational-posterior",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "56 Choosing the form of the variational posterior",
    "text": "56 Choosing the form of the variational posterior\n\n\n\n\nSlide 63\n\n\n\n\nThe simples way is to fully factorize the distribution, where we say the full distribution of z is the multiplication of all different z_i distributions. That means they are very simply related all of these points are identically distributed so idd\nIn clasic Variational inference, the idd is also know as mean field approximation.\nHere we model the high-level latent factors that can cature independent factors such as camera, angle, lighthening, etc such that forgetting the main latent representation you just multiply all these together\nThe treadoff: in practice for VAE the mean field approximation is used. Where we have gaussian distributions, so every z so every latent dimension is independent of each other"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#amortized-variational-inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#amortized-variational-inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "57 Amortized variational inference",
    "text": "57 Amortized variational inference\n\n\n\n\nSlide 64\n\n\n\n\nHere you have your observation x and now you have a complicated procedure back and forth to gradually arrive at your variational parameters phi. Now because we say q would be Gaussians then \\(\\phi\\) represents \\(\\mu\\) and \\(\\sigma\\) which are the parameters of a Gaussian distribution\nNow because p(z|x) is different for each observation x, we would do something more efficient called amortized inference, where amortized just means we will reuse things and therefore we do not have to do this iterative for each data sample"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#amortized-variational-inference-the-deep-learners-inference",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#amortized-variational-inference-the-deep-learners-inference",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "58 Amortized variational inference: the Deep Learner’s inference",
    "text": "58 Amortized variational inference: the Deep Learner’s inference\n\n\n\n\nSlide 65\n\n\n\n\nInstead of optimizing a set of free paramaters, we use a NN that accepts an observation an input and ouputs these \\(\\mu\\) and \\(\\sigma\\) directly.\nSo instead of having this blackbox, for every single sample that gradually learns \\(\\mu\\) and \\(\\sigma\\) which is done tipically in statistics, we just have a NN to approximate this behavior\nIt is amortized because across all the samples in the dataset we are using the same NN to do this, thus we are amortizing this process"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#amortized-variational-inference-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#amortized-variational-inference-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "59 Amortized variational inference",
    "text": "59 Amortized variational inference\n\n\n\n\nSlide 66\n\n\n\n\nSo because now \\(\\phi\\) represents the weights of our NN, and we can train jointly with the model using the ELBO loss"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#benefits-of-using-amortization",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#benefits-of-using-amortization",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "60 Benefits of using amortization",
    "text": "60 Benefits of using amortization\n\n\n\n\n\n\n\nSlide 67\n\n\n\n\nIt is fast, for every new observation all we need to do is one single forward pass and then we have all the approximate posterior \\(q_{\\phi}(z|x)\\) distribution parameters \\(\\mu\\) and \\(\\sigma\\). With the paramaters then we have the approximate posterior distribution as well because we are saying is a Gaussian distribution"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#maximizing-the-elbo",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#maximizing-the-elbo",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "61 Maximizing the ELBO",
    "text": "61 Maximizing the ELBO\n\n\n\n\nSlide 68\n\n\n\n\nBecause to maximize the ELBO is a non-convex optimization we need to estimate the gradients. We will now estimating the gradient because you still have this expectations over this distribution, we will be estimating the gradients using Montecarlo sampling"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#gradient-w.r.t.-the-model-parameters",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#gradient-w.r.t.-the-model-parameters",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "62 Gradient w.r.t. the model parameters",
    "text": "62 Gradient w.r.t. the model parameters\n\n\n\n\nSlide 70\n\n\n\n\nInstead of taking computing the whole expectation (which the formula for expectations involves an integral) we just take \\(k\\) samples to approximate it. This is a Montecarlo approach. Thus by taking k samples we get a rough estimate of the gradient. So instead of sampling the whole thing we do the simplest montecarlo estimate that is possible"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#gradient-w.r.t.-variational-parameters",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#gradient-w.r.t.-variational-parameters",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "63 Gradient w.r.t. variational parameters",
    "text": "63 Gradient w.r.t. variational parameters\n\n\n\n\nSlide 71\n\n\n\n\nIt is not obvious how to take the derivative of a distribution from where we are sampling"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#gradients-of-expectations",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#gradients-of-expectations",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "64 Gradients of expectations",
    "text": "64 Gradients of expectations\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reparameterization-trick",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reparameterization-trick",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "65 Reparameterization trick",
    "text": "65 Reparameterization trick\n\n\n\n\nSlide 73\n\n\n\n\nFirst we sample \\(z\\) form the approximated posterior distribution \\(q(\\textbf{z}_n | \\textbf{x}_n)\\)\nThen express the random variable \\(z\\) as a deterministic variable \\(z=g(\\epsilon, \\phi)\\), where \\(\\epsilon\\) is an auxiliary independent random variable, and the transformation function \\(g(.)\\) parameterized by \\(\\phi\\) converts \\(\\epsilon\\) to \\(z\\).\nThe problem that we have before is that the get the gradient of the expectation over \\(q(\\textbf{z}_n | \\textbf{x}_n)\\) was not feasible, as this requires us to sample infinietly, so now instead we do reparametrization which instead of sampling \\(z\\) we will be sampling from the this parametrized function \\(g(\\epsilon, \\phi)\\)\nThis \\(\\epsilon\\) will form a distribution which in practice in a Normal dsitr.\nSo the crucial bit is that with reparametrization we shift in the expectation from which distribution to sample in this case from \\(p(\\epsilon)\\). So now we can take the gradient and now the gradient can go inside because the sampling is not done by anithing that depends on the parameters \\(\\phi\\). So then we end up with the last line of equation above by taking the Chain Rule."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reparameterization-trick-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reparameterization-trick-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "66 Reparameterization trick",
    "text": "66 Reparameterization trick\n\n\n\n\nSlide 74\n\n\n\n\n\n\n\n\nWe want to learn \\(\\mu\\) and \\(\\sigma\\) so the last line represent the reparametrization where now we can sample from \\(\\epsilon\\).\nEven though we add stotastichs because we sample from epsilon, this make it differentiable which was a problem before by just taking the gradient of the expectation"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reparameterization-trick-visualised",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#reparameterization-trick-visualised",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "67 Reparameterization trick visualised",
    "text": "67 Reparameterization trick visualised\n\n\n\n\nSlide 75\n\n\n\n\nIf sample as in the left you cannot take gradients because there is no way how you could do that. However, if you add the reparametrization you get a noisy sample and then you multiply by \\(\\sigma\\) and add \\(\\mu\\) and that is differentiable.\nHere in the most right we have a vector because we have a mean for every latent dimension i.e this could be 120 dimensions and then we have a standard deviation for every latent dimension that is being ouputed. So now you have our parameters \\(\\sigma\\) and \\(\\mu\\) so then we can get p(z|x). You also get your \\(z =\\mu + \\sigma circ \\epsilon\\) because we did the reparametrization trick. Now you put this sample \\(z\\) into the decoder network to deconstruct it."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#where-are-we-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#where-are-we-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "68 Where are we?:",
    "text": "68 Where are we?:\n\n\n\n\nSlide 76\n\n\n\n\nSo we can choose any simple approximation for our posterior q but when we want to optimizie this then we get into the trouble of not knowing how to get the gradient of a expectation which we need to samply from this distribution. This gradient of the expectation involves the variational parameters that we wish to take take the derivative from.\nSo instead we reparametrized this to have have a generic stochastic epsilon that we could map us back to our z whihc before was difficult to compute.\nA sampling procedure is like you have an image and then you are taking the crop, so then you cannot take the gradients with regards to the cropping procedure. You can fake it by doing the reparametrization because you are saying okay you take the sample but then you shift it and you scale it by exaclty the same thing. Note, for images is different because we do not have just a gaussian distribution.\nWhat we say is that all latent dimensions are idenpendent so basically your covariance matrix your sigma has only entries on the diagonal matrix, technically this does not need to be the case. It will just mean that this network will ouput a lot more standard deviations. But genreally Gaussians are simple, they stay gaussian even if you take the derivative"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-autoencoders-summary",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-autoencoders-summary",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "69 Variational autoencoders summary",
    "text": "69 Variational autoencoders summary\n\n\n\n\nSlide 77\n\n\n\n\n\n\n\n\n\nVAE are generative models with continous latent variables. We do not chunk them into zeros or ones\nThe likelihood p(x) and the variational posterioir \\(q_\\theta\\) are NNs\nThe last point refers that the noise comes from the epsilon"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-autoencoders-elbo",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#variational-autoencoders-elbo",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "70 Variational autoencoders ELBO",
    "text": "70 Variational autoencoders ELBO\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#encoder-decoder",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#encoder-decoder",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "71 Encoder & Decoder",
    "text": "71 Encoder & Decoder\n\n\n\n\nSlide 79\n\n\n\n\nWith your input data you get your \\(\\mu\\) and \\(\\sigma\\) and then you get \\(z\\). And then given this \\(z\\) you can reconstruct the input. In between these steeps there is a reparametrization step when you do the training"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#generating-data",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#generating-data",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "72 Generating Data",
    "text": "72 Generating Data\n\n\n\n\nSlide 80\n\n\n\n\nWhen you generate new data, you can now simply sample \\(z\\) from this Gaussian distrivution with mean zero and std \\(I\\). And then for example if your Gaussian distribution is 2D then you can sample a point from the top left all the way to the bottom right and each time you sample \\(z\\) through the decoder network you can vizualise it based on where it cames from for instance if it comes from the left corner or top corner or so on.\n\nWe sample first from from p(z) to then obtain the values of \\(\\mu\\) and \\(\\sigma\\)\nWith those now we sample from \\(p(x|z)\\) that comes from this new Gaussian distribution with the parameters we found in the prev step."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#dimensionality-of-latent-space-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#dimensionality-of-latent-space-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "73 Dimensionality of latent space",
    "text": "73 Dimensionality of latent space\n\n\n\n\nSlide 81\n\n\n\n\nOnce we have more dimensions we get more noisy data. This is because it start to fit the noise which is not particualrly usefull\nIf you go to higuer dimensionalities, then you will start to have optimizations issues."
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#face-generation",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#face-generation",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "74 Face generation",
    "text": "74 Face generation\n\n\n\n\nSlide 82\n\n\n\n\nIt is not blurry is noisy because now we have more dimensions"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-suboptimality",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-suboptimality",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "75 Inference suboptimality",
    "text": "75 Inference suboptimality\n\n\n\n\nSlide 83\n\n\n\n\n\nVariational posterior: \\(q_{\\phi}(\\textbf{z}_n | \\textbf{x}_n)\\) match to the true posterior \\(p_{\\theta}(\\textbf{z}_n)\\)\n\nIn terms ob observations: - The divergence from the true posterioir is often imperfect mostly due to the amortized inference network rather that the limmiting capacity or complexity of the approximating distribution"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-gaps",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#inference-gaps",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "76 Inference Gaps",
    "text": "76 Inference Gaps\n\n\n\n\nSlide 84\n\n\n\n\nHere the approximation gap is the same as Variational gap\n\nVariational gap: if this is small then you can make it samll by having an expressive variational distribution\nAmortization Gap: you can make this small by having a stronger bigger NN"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#vae-variants",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#vae-variants",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "77 VAE variants",
    "text": "77 VAE variants"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#conditional-vaes",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#conditional-vaes",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "78 Conditional VAEs",
    "text": "78 Conditional VAEs\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#conditional-vaes-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#conditional-vaes-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "79 Conditional VAEs",
    "text": "79 Conditional VAEs\n\n\n\n\nSlide 87"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#conditional-vaes-2",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#conditional-vaes-2",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "80 Conditional VAEs",
    "text": "80 Conditional VAEs\n\n\n\n\nSlide 88"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#beta-vae",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#beta-vae",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "81 Beta-VAE",
    "text": "81 Beta-VAE\n\n\n\n\nSlide 89"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#beta-vae-1",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#beta-vae-1",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "82 Beta-VAE",
    "text": "82 Beta-VAE\n\n\n\n\nSlide 90"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#vq-vae-and-vq-vae2",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#vq-vae-and-vq-vae2",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "83 VQ-VAE and VQ-VAE2",
    "text": "83 VQ-VAE and VQ-VAE2\n\n\n\n\nSlide 91"
  },
  {
    "objectID": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#further-reading",
    "href": "blog/2023-12-07_generative-modelling-and-deep-variational-inference/index.html#further-reading",
    "title": "Generative modelling and Deep Variational Inference",
    "section": "84 Further reading",
    "text": "84 Further reading\n\n\n\n\nSlide 92"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html",
    "title": "Interpretability of NLP models",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 3, 2023"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#title",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#title",
    "title": "Interpretability of NLP models",
    "section": "1 Title",
    "text": "1 Title\n\n\n\n\nSlide 1"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#he-lwoann",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#he-lwoann",
    "title": "Interpretability of NLP models",
    "section": "2 hE lwoann",
    "text": "2 hE lwoann\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#he-lwoann-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#he-lwoann-1",
    "title": "Interpretability of NLP models",
    "section": "3 hE lwoann",
    "text": "3 hE lwoann\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#he-lwoann-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#he-lwoann-2",
    "title": "Interpretability of NLP models",
    "section": "4 hE lwoann",
    "text": "4 hE lwoann\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#plan-for-today",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#plan-for-today",
    "title": "Interpretability of NLP models",
    "section": "5 Plan for today",
    "text": "5 Plan for today\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability",
    "title": "Interpretability of NLP models",
    "section": "6 Why do we need interpretability?",
    "text": "6 Why do we need interpretability?\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-1",
    "title": "Interpretability of NLP models",
    "section": "7 Why do we need interpretability? |",
    "text": "7 Why do we need interpretability? |\n\n\n\n\nSlide 7"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-2",
    "title": "Interpretability of NLP models",
    "section": "8 Why do we need interpretability?",
    "text": "8 Why do we need interpretability?\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-3",
    "title": "Interpretability of NLP models",
    "section": "9 Why do we need interpretability?",
    "text": "9 Why do we need interpretability?\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-4",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-4",
    "title": "Interpretability of NLP models",
    "section": "10 Why do we need interpretability?",
    "text": "10 Why do we need interpretability?\n\n\n\n\nSlide 10"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-5",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-5",
    "title": "Interpretability of NLP models",
    "section": "11 Why do we need interpretability?",
    "text": "11 Why do we need interpretability?\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-6",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-6",
    "title": "Interpretability of NLP models",
    "section": "12 Why do we need interpretability?",
    "text": "12 Why do we need interpretability?\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-7",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-7",
    "title": "Interpretability of NLP models",
    "section": "13 Why do we need interpretability?",
    "text": "13 Why do we need interpretability?\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-8",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-8",
    "title": "Interpretability of NLP models",
    "section": "14 Why do we need interpretability?",
    "text": "14 Why do we need interpretability?\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-9",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-9",
    "title": "Interpretability of NLP models",
    "section": "15 Why do we need interpretability?",
    "text": "15 Why do we need interpretability?\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#nos-nieuws.-sport.-live-programmas-2-q",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#nos-nieuws.-sport.-live-programmas-2-q",
    "title": "Interpretability of NLP models",
    "section": "16 NOS Nieuws. Sport. Live Programma’s 2 Q @",
    "text": "16 NOS Nieuws. Sport. Live Programma’s 2 Q @\nclathodieose oe Genweg\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#title-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#title-1",
    "title": "Interpretability of NLP models",
    "section": "17 Title",
    "text": "17 Title\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#x-i-bog-q",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#x-i-bog-q",
    "title": "Interpretability of NLP models",
    "section": "18 x I BoG =Q",
    "text": "18 x I BoG =Q\nFr Nn dows Need ay ey\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#title-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#title-2",
    "title": "Interpretability of NLP models",
    "section": "19 Title",
    "text": "19 Title\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-10",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-10",
    "title": "Interpretability of NLP models",
    "section": "20 Why do we need interpretability?",
    "text": "20 Why do we need interpretability?\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-11",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-11",
    "title": "Interpretability of NLP models",
    "section": "21 Why do we need interpretability?",
    "text": "21 Why do we need interpretability?\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#egg",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#egg",
    "title": "Interpretability of NLP models",
    "section": "22 egg",
    "text": "22 egg\nx Can we ever truly understand a large-scale Al model’s internal reasoning? vy | Wh\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-12",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-do-we-need-interpretability-12",
    "title": "Interpretability of NLP models",
    "section": "23 Why do we need interpretability?",
    "text": "23 Why do we need interpretability?\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model",
    "title": "Interpretability of NLP models",
    "section": "24 How do we explain a model?",
    "text": "24 How do we explain a model?\n\n\n\n\nSlide 24"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-1",
    "title": "Interpretability of NLP models",
    "section": "25 How do we explain a model?",
    "text": "25 How do we explain a model?\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-2",
    "title": "Interpretability of NLP models",
    "section": "26 How do we explain a model?",
    "text": "26 How do we explain a model?\n\n\n\n\nSlide 26"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-3",
    "title": "Interpretability of NLP models",
    "section": "27 How do we explain a model?",
    "text": "27 How do we explain a model?\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-4",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-4",
    "title": "Interpretability of NLP models",
    "section": "28 How do we explain a model?",
    "text": "28 How do we explain a model?\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-5",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-5",
    "title": "Interpretability of NLP models",
    "section": "29 How do we explain a model?",
    "text": "29 How do we explain a model?\n\n\n\n\nSlide 29"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-6",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#how-do-we-explain-a-model-6",
    "title": "Interpretability of NLP models",
    "section": "30 How do we explain a model?",
    "text": "30 How do we explain a model?\n\n\n\n\nSlide 30"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-faithfulness",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-faithfulness",
    "title": "Interpretability of NLP models",
    "section": "31 Explanation Faithfulness",
    "text": "31 Explanation Faithfulness\n\n\n\n\nSlide 31"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-faithfulness-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-faithfulness-1",
    "title": "Interpretability of NLP models",
    "section": "32 Explanation Faithfulness",
    "text": "32 Explanation Faithfulness\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-faithfulness-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-faithfulness-2",
    "title": "Interpretability of NLP models",
    "section": "33 Explanation Faithfulness",
    "text": "33 Explanation Faithfulness\n\n\n\n\nSlide 33"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods",
    "title": "Interpretability of NLP models",
    "section": "34 Explanation Methods",
    "text": "34 Explanation Methods\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods-1",
    "title": "Interpretability of NLP models",
    "section": "35 Explanation Methods",
    "text": "35 Explanation Methods\n\n\n\n\nSlide 35"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods-2",
    "title": "Interpretability of NLP models",
    "section": "36 Explanation Methods",
    "text": "36 Explanation Methods\n\n\n\n\nSlide 36"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#explanation-methods-3",
    "title": "Interpretability of NLP models",
    "section": "37 Explanation Methods",
    "text": "37 Explanation Methods\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-interpretability",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-interpretability",
    "title": "Interpretability of NLP models",
    "section": "38 Behavioural Interpretability",
    "text": "38 Behavioural Interpretability\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-interpretability-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-interpretability-1",
    "title": "Interpretability of NLP models",
    "section": "39 Behavioural Interpretability",
    "text": "39 Behavioural Interpretability\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp",
    "title": "Interpretability of NLP models",
    "section": "40 BLIMP",
    "text": "40 BLIMP\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-1",
    "title": "Interpretability of NLP models",
    "section": "41 BLIMP",
    "text": "41 BLIMP\n\n\n\n\nSlide 41"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-2",
    "title": "Interpretability of NLP models",
    "section": "42 BLIMP",
    "text": "42 BLIMP\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-3",
    "title": "Interpretability of NLP models",
    "section": "43 BLIMP",
    "text": "43 BLIMP\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-4",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-4",
    "title": "Interpretability of NLP models",
    "section": "44 BLIMP",
    "text": "44 BLIMP\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-5",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-5",
    "title": "Interpretability of NLP models",
    "section": "45 BLIMP",
    "text": "45 BLIMP\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-6",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-6",
    "title": "Interpretability of NLP models",
    "section": "46 BLIMP",
    "text": "46 BLIMP\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-7",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#blimp-7",
    "title": "Interpretability of NLP models",
    "section": "47 BLIMP",
    "text": "47 BLIMP\n\n\n\n\nSlide 47"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-tests-for-uncovering-biases",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-tests-for-uncovering-biases",
    "title": "Interpretability of NLP models",
    "section": "48 Behavioural Tests for Uncovering Biases",
    "text": "48 Behavioural Tests for Uncovering Biases\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-tests-for-uncovering-biases-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#behavioural-tests-for-uncovering-biases-1",
    "title": "Interpretability of NLP models",
    "section": "49 Behavioural Tests for Uncovering Biases",
    "text": "49 Behavioural Tests for Uncovering Biases\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#limitations-of-behavioural-tests",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#limitations-of-behavioural-tests",
    "title": "Interpretability of NLP models",
    "section": "50 Limitations of Behavioural Tests",
    "text": "50 Limitations of Behavioural Tests\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#limitations-of-behavioural-tests-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#limitations-of-behavioural-tests-1",
    "title": "Interpretability of NLP models",
    "section": "51 Limitations of Behavioural Tests",
    "text": "51 Limitations of Behavioural Tests\n\n\n\n\nSlide 51"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-attribution-methods",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-attribution-methods",
    "title": "Interpretability of NLP models",
    "section": "52 Feature Attribution Methods",
    "text": "52 Feature Attribution Methods\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution",
    "title": "Interpretability of NLP models",
    "section": "53 Pronoun Resolution",
    "text": "53 Pronoun Resolution\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-1",
    "title": "Interpretability of NLP models",
    "section": "54 Pronoun Resolution",
    "text": "54 Pronoun Resolution\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-2",
    "title": "Interpretability of NLP models",
    "section": "55 Pronoun Resolution",
    "text": "55 Pronoun Resolution\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-3",
    "title": "Interpretability of NLP models",
    "section": "56 Pronoun Resolution",
    "text": "56 Pronoun Resolution\n\n\n\n\nSlide 56"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-4",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-4",
    "title": "Interpretability of NLP models",
    "section": "57 Pronoun Resolution",
    "text": "57 Pronoun Resolution\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-5",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#pronoun-resolution-5",
    "title": "Interpretability of NLP models",
    "section": "58 Pronoun Resolution",
    "text": "58 Pronoun Resolution\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions",
    "title": "Interpretability of NLP models",
    "section": "59 Averaae contributions",
    "text": "59 Averaae contributions\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions-1",
    "title": "Interpretability of NLP models",
    "section": "60 Averaae contributions",
    "text": "60 Averaae contributions\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions-2",
    "title": "Interpretability of NLP models",
    "section": "61 Averaae contributions",
    "text": "61 Averaae contributions\n\n\n\n\nSlide 61"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#averaae-contributions-3",
    "title": "Interpretability of NLP models",
    "section": "62 Averaae contributions",
    "text": "62 Averaae contributions\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#default-reasoning",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#default-reasoning",
    "title": "Interpretability of NLP models",
    "section": "63 Default Reasoning?",
    "text": "63 Default Reasoning?\n\n\n\n\nSlide 63"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-attribution-methods-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-attribution-methods-1",
    "title": "Interpretability of NLP models",
    "section": "64 Feature Attribution Methods",
    "text": "64 Feature Attribution Methods\n\n\n\n\nSlide 64"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-attribution-methods-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-attribution-methods-2",
    "title": "Interpretability of NLP models",
    "section": "65 Feature Attribution Methods",
    "text": "65 Feature Attribution Methods\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#attribution-dimensions",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#attribution-dimensions",
    "title": "Interpretability of NLP models",
    "section": "66 Attribution Dimensions",
    "text": "66 Attribution Dimensions\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal",
    "title": "Interpretability of NLP models",
    "section": "67 Feature Removal",
    "text": "67 Feature Removal\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-1",
    "title": "Interpretability of NLP models",
    "section": "68 Feature Removal",
    "text": "68 Feature Removal\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-2",
    "title": "Interpretability of NLP models",
    "section": "69 Feature Removal",
    "text": "69 Feature Removal\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-3",
    "title": "Interpretability of NLP models",
    "section": "70 Feature Removal",
    "text": "70 Feature Removal\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-4",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-4",
    "title": "Interpretability of NLP models",
    "section": "71 Feature Removal",
    "text": "71 Feature Removal\n\n\n\n\nSlide 71"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-5",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-5",
    "title": "Interpretability of NLP models",
    "section": "72 Feature Removal",
    "text": "72 Feature Removal\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-6",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-removal-6",
    "title": "Interpretability of NLP models",
    "section": "73 Feature Removal",
    "text": "73 Feature Removal\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#featu-re-removal-conditioned-on-present-features",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#featu-re-removal-conditioned-on-present-features",
    "title": "Interpretability of NLP models",
    "section": "74 Featu re Removal Conditioned on present features |",
    "text": "74 Featu re Removal Conditioned on present features |\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#featu-re-removal-conditioned-on-present-features-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#featu-re-removal-conditioned-on-present-features-1",
    "title": "Interpretability of NLP models",
    "section": "75 Featu re Removal Conditioned on present features |",
    "text": "75 Featu re Removal Conditioned on present features |\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence",
    "title": "Interpretability of NLP models",
    "section": "76 Feature Influence",
    "text": "76 Feature Influence\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence-1",
    "title": "Interpretability of NLP models",
    "section": "77 Feature Influence",
    "text": "77 Feature Influence\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values",
    "title": "Interpretability of NLP models",
    "section": "78 Shapley Values",
    "text": "78 Shapley Values\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values-1",
    "title": "Interpretability of NLP models",
    "section": "79 Shapley Values",
    "text": "79 Shapley Values\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values-2",
    "title": "Interpretability of NLP models",
    "section": "80 Shapley Values",
    "text": "80 Shapley Values\n\n\n\n\nSlide 80"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#shapley-values-3",
    "title": "Interpretability of NLP models",
    "section": "81 Shapley Values",
    "text": "81 Shapley Values\n\n\n\n\nSlide 81"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence-2",
    "title": "Interpretability of NLP models",
    "section": "82 Feature Influence",
    "text": "82 Feature Influence\n\n\n\n\nSlide 82"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence-3",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#feature-influence-3",
    "title": "Interpretability of NLP models",
    "section": "83 Feature Influence",
    "text": "83 Feature Influence\n\n\n\n\nSlide 83"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#highlighting-via-input-gradients",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#highlighting-via-input-gradients",
    "title": "Interpretability of NLP models",
    "section": "84 Highlighting via Input Gradients",
    "text": "84 Highlighting via Input Gradients\ne Estimate importance of a feature using derivative of output w.rt that feature\n\n\n\n\nSlide 84"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#example-of-highlighting-image-classification",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#example-of-highlighting-image-classification",
    "title": "Interpretability of NLP models",
    "section": "85 Example of highlighting: Image classification",
    "text": "85 Example of highlighting: Image classification\n\n\n\n\nSlide 85"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#gradient-based-highlightings-for-nlp",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#gradient-based-highlightings-for-nlp",
    "title": "Interpretability of NLP models",
    "section": "86 Gradient-based Highlightings for NLP",
    "text": "86 Gradient-based Highlightings for NLP\nFor NLP, derivative of output w.r.t a feature\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#gradient-based-highlightings-for-nlp-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#gradient-based-highlightings-for-nlp-1",
    "title": "Interpretability of NLP models",
    "section": "87 Gradient-based Highlightings for NLP",
    "text": "87 Gradient-based Highlightings for NLP\nFor NLP, derivative of output w.r.t a feature\n\n\n\n\nSlide 87"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#problems-with-using-gradient-for-highlighting",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#problems-with-using-gradient-for-highlighting",
    "title": "Interpretability of NLP models",
    "section": "88 Problems with Using Gradient for Highlighting",
    "text": "88 Problems with Using Gradient for Highlighting\ne 100 “local” and thus sensitive to slight perturbations\n\n\n\n\nSlide 88"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#problems-with-using-gradient-for-highlighting-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#problems-with-using-gradient-for-highlighting-1",
    "title": "Interpretability of NLP models",
    "section": "89 Problems with Using Gradient for Highlighting",
    "text": "89 Problems with Using Gradient for Highlighting\n\n\n\n\nSlide 89"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#problems-with-using-gradient-for-highlighting-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#problems-with-using-gradient-for-highlighting-2",
    "title": "Interpretability of NLP models",
    "section": "90 Problems with Using Gradient for Highlighting",
    "text": "90 Problems with Using Gradient for Highlighting\n\n\n\n\nSlide 90"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#extensions-of-vanilla-gradient",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#extensions-of-vanilla-gradient",
    "title": "Interpretability of NLP models",
    "section": "91 Extensions of Vanilla Gradient",
    "text": "91 Extensions of Vanilla Gradient\ne too “local” and thus sensitive to slight perturbations\n\n\n\n\nSlide 91"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#extensions-of-vanilla-gradient-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#extensions-of-vanilla-gradient-1",
    "title": "Interpretability of NLP models",
    "section": "92 Extensions of Vanilla Gradient",
    "text": "92 Extensions of Vanilla Gradient\nSmoothGrad: add gaussian noise to input and average the gradient\n\n\n\n\nSlide 92"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#extensions-of-vanilla-gradient-2",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#extensions-of-vanilla-gradient-2",
    "title": "Interpretability of NLP models",
    "section": "93 Extensions of Vanilla Gradient",
    "text": "93 Extensions of Vanilla Gradient\nIntegrated Gradients: average gradients along path from zero to input\n\n\n\n\nSlide 93"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#summary-of-gradient-based-highlighting",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#summary-of-gradient-based-highlighting",
    "title": "Interpretability of NLP models",
    "section": "94 Summary of Gradient-based Highlighting",
    "text": "94 Summary of Gradient-based Highlighting\nPositives:\n\n\n\n\nSlide 94"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#summary-of-gradient-based-highlighting-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#summary-of-gradient-based-highlighting-1",
    "title": "Interpretability of NLP models",
    "section": "95 Summary of Gradient-based Highlighting",
    "text": "95 Summary of Gradient-based Highlighting\n\n\n\n\nSlide 95"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing",
    "title": "Interpretability of NLP models",
    "section": "96 Probing",
    "text": "96 Probing\n\n\n\n\nSlide 96"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing-1",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing-1",
    "title": "Interpretability of NLP models",
    "section": "97 Probing",
    "text": "97 Probing\n\n\n\n\nSlide 97"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing-linauistic",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing-linauistic",
    "title": "Interpretability of NLP models",
    "section": "98 Probing | Linauistic",
    "text": "98 Probing | Linauistic\n\n\n\n\nSlide 98"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing-os-tase-ner-etc.",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#probing-os-tase-ner-etc.",
    "title": "Interpretability of NLP models",
    "section": "99 Probing | os-tase NER etc. |",
    "text": "99 Probing | os-tase NER etc. |\n\n\n\n\nSlide 99"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#representations",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#representations",
    "title": "Interpretability of NLP models",
    "section": "100 Representations",
    "text": "100 Representations\n\n\n\n\nSlide 100"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#what-does-probed-info-imply",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#what-does-probed-info-imply",
    "title": "Interpretability of NLP models",
    "section": "101 What does probed info imply?",
    "text": "101 What does probed info imply?\n\n\n\n\nSlide 101"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-linear",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#why-linear",
    "title": "Interpretability of NLP models",
    "section": "102 Why linear?",
    "text": "102 Why linear?\n\n\n\n\nSlide 102"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#ka-1.60-ks-0.19",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#ka-1.60-ks-0.19",
    "title": "Interpretability of NLP models",
    "section": "103 K(A) = 1.60 K(s) = 0.19",
    "text": "103 K(A) = 1.60 K(s) = 0.19\nProbing | POS-tags | S| 0] k@ets7 K(s) = 0.83\n\n\n\n\nSlide 103"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#x",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#x",
    "title": "Interpretability of NLP models",
    "section": "104 x",
    "text": "104 x\nx] | Recap\n\n\n\n\nSlide 104"
  },
  {
    "objectID": "blog/2023-12-03_interpretability-of-nlp-models/index.html#references",
    "href": "blog/2023-12-03_interpretability-of-nlp-models/index.html#references",
    "title": "Interpretability of NLP models",
    "section": "105 References",
    "text": "105 References\n\n\n\n\nSlide 105"
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html",
    "href": "blog/2023-09-05_equation-for-ml1/index.html",
    "title": "Probability Theory in Machine Learning",
    "section": "",
    "text": "Formulas for the course at UvA: Machine Learning 1\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Probability Theory\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Probability Theory\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 5, 2023\nThis section focus on two weeks of the course. For Classification continue to this post."
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html#rules-of-probability-theory",
    "href": "blog/2023-09-05_equation-for-ml1/index.html#rules-of-probability-theory",
    "title": "Probability Theory in Machine Learning",
    "section": "1 Rules of Probability Theory",
    "text": "1 Rules of Probability Theory\nSum Rule used in Marginalization \\[\n\\begin{align}\np(x) &= \\sum_{y \\in Y }^{} p(x,y) \\\\\n     &= \\sum_{y \\in Y }^{} p(x|y)p(y) \\nonumber\n\\end{align}\n\\]\nProduct Rule used in the Join Probability \\[\n\\begin{align}\np(x,y) = p(x|y)p(y)\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html#expectation-rules",
    "href": "blog/2023-09-05_equation-for-ml1/index.html#expectation-rules",
    "title": "Probability Theory in Machine Learning",
    "section": "2 Expectation Rules",
    "text": "2 Expectation Rules\n\\[\n\\begin{align}\n    \\mathbb{E}[f(x)+g(x)] &= \\mathbb{E}[f(x)] + \\mathbb{E}[g(x)]\\\\\n    \\mathbb{E}[cf(x)] &= c\\mathbb{E}[f(x)]\\\\\n    \\mathbb{E}[c] &= c\\\\\n    \\mathbb{E}[\\mathbb{E}[f(x)]] &= \\mathbb{E}[f(x)]^2\\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html#probability-theory-equations",
    "href": "blog/2023-09-05_equation-for-ml1/index.html#probability-theory-equations",
    "title": "Probability Theory in Machine Learning",
    "section": "3 Probability Theory Equations",
    "text": "3 Probability Theory Equations\nExpectancies \\[\n\\begin{align}\n    \\mathbb{E}[x] &= \\int_{x}x \\, p(x) \\, dx\\\\\n    var[x] &= \\mathbb{E}[(x-\\mathbb{E}[x])^2] \\nonumber\\\\\n    &=\\mathbb{E}[f(x)^2] - \\mathbb{E}[f(x)]^2\\\\\n\\end{align}\n\\]\nWhen cov for two scalar variables \\[\n\\begin{align}\ncov[x,y] &= \\mathbb{E}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]   \n\\end{align}\n\\]\nCovariance Matrix: When \\(\\textbf{x}\\), \\(\\textbf{y}\\) are vectors of random variables \\[\n\\begin{align}\ncov[\\textbf{x},\\textbf{x}] &= \\mathbb{E}[(\\textbf{x}-\\mathbb{E}[x])(\\textbf{x}-\\mathbb{E}[x])^T]\\\\  \ncov[\\textbf{z},\\textbf{z}] &= \\mathbb{E}[\\textbf{z}\\textbf{z}^T]-\\mathbb{E}[\\textbf{z}]\\mathbb{E}[\\textbf{z}]\\\\\n\\end{align}\n\\]\nGaussians: scalar and for a matrix \\[\n\\begin{align}\n\\mathcal{N}(x|\\mu , \\sigma^2) &= \\frac{1}{\\sqrt{2 \\pi  \\sigma^2}} e^{\\left(-\\frac{1}{2 \\sigma^2}(x-\\mu)^2\\right)}\\\\\n    \\mathcal{N}(x|\\mu , \\Sigma ) &= \\frac{1}{(2\\pi)^{D/2}|\\Sigma|^{1/2}}  e^{\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^-1 (x-\\mu)\\right)}\\\\\n\\end{align}\n\\]\nThis is our model. This is what we estimate: \\[\n\\begin{align}\ny(\\underline{x})\n\\end{align}\n\\]\nThe observations can be sampled from (signal + noise): \\[\n\\begin{align}\nt = sin(\\underline{x}) + \\varepsilon\n\\end{align}\n\\]\nData Pair: For a given \\(x\\), I can get a target value \\(t\\) (the observation) \\[\n\\begin{align}\n(t, x)  \n\\end{align}\n\\]\nFor instance, we have a sampled data D, and for each \\(x\\) we have seen \\(t\\) and that is our excel file we can work with"
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html#bayessian-linear-regression",
    "href": "blog/2023-09-05_equation-for-ml1/index.html#bayessian-linear-regression",
    "title": "Probability Theory in Machine Learning",
    "section": "4 Bayessian Linear Regression",
    "text": "4 Bayessian Linear Regression\nWe do not want to average over models but this time we want to find the best parameters over only one Data set (without splitting it) and change only our parameters.\nFor this we would consider the posterior. So what is the probability that this parameters \\(w\\) represent the actual data.\nGoal recover the probability distribution that may have generated this data (the posterior)\n\n4.1 Dimensions\n\\[\n\\begin{align}\n\\underline{t} &\\in \\mathbb{R}^{Nx1}, \\text{ $N$ amount of linear regressions}\\\\\n\\underline{w} &\\in \\mathbb{R}^{Mx1}, \\text{ $M$ amount of parameters}\\\\\nX &\\in \\mathbb{R}^{NxD}, \\text{ $N$ amount of observ, $D$ amount of models}\\\\\n% \\underline{x_i} &\\in \\mathbb{R}^{Dx1}, \\text{ $i$ the $i_{th}$ experiment} \\\\\n&= [\\underline{x_{1}}, \\underline{x_{2}}, ... ,\\underline{x_{N}}] \\nonumber \\\\\n\\underline{x_{1}} &\\in \\mathbb{R}^{Nx1} = [x_1, x_2,...,x_N]\\\\\n\\underline{\\phi}(\\underline{x}) &\\in \\mathbb{R}^{Dx1} \\to \\mathbb{R}^{Mx1}\\\\\n\\phi_{1}(\\underline{x}) &\\in \\mathbb{R}^{Nx1} \\to \\mathbb{R}^{1x1}\\\\\n\\Phi &\\in \\mathbb{R}^{NxM}\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html#sequential-bayesian-learning",
    "href": "blog/2023-09-05_equation-for-ml1/index.html#sequential-bayesian-learning",
    "title": "Probability Theory in Machine Learning",
    "section": "5 Sequential Bayesian Learning",
    "text": "5 Sequential Bayesian Learning\nHere our goal is to find the parameters of w. So we want to find the values w that discribe best the distribution of our data points meaning we want to find the posterior described as:\n\\(p(w|x_1,t_1, \\alpha, \\beta)\\)\n\nWe have a prior, we assume initial values\nWe sample one data point and apply Gaussian distribution to obtain a likelihood\nWith the prior and likelihood we get the posterior (what we want)"
  },
  {
    "objectID": "blog/2023-09-05_equation-for-ml1/index.html#predictive-distribution",
    "href": "blog/2023-09-05_equation-for-ml1/index.html#predictive-distribution",
    "title": "Probability Theory in Machine Learning",
    "section": "6 Predictive Distribution",
    "text": "6 Predictive Distribution\nIf we are given a new input x’, then we want to be able to compute its new distribution meaning we want to compute the new likelihood that looks as follows:\n\\(p(t'|x', X, \\underline{t}, \\alpha, \\beta) = \\int p(t'|x', \\underline{w}, \\beta) p(\\underline{w}|X, \\underline{t}, \\alpha, \\beta)dw\\)\nThe above equation uses Marginalization over w. That does not mean that it depends on w. It’s just a dummy variable it can be another variable thus the predictive distribution does not depend on w.\nThe second term is the posterior.\n\nHere we are given all data points and we get the parameters for w. With that we can get a prior. Those are the assumptions of how the weigths should be.\nThe predictive probability does not depend on w anymore. It depends on the data, so on the experience gained so far. If a new data point comes in then we would update our predictive distribution\nFor each new point x’ we want to fins the new distribution probability for t’"
  },
  {
    "objectID": "blog/2023-10-03_the-perceptron/index.html",
    "href": "blog/2023-10-03_the-perceptron/index.html",
    "title": "The Perceptron",
    "section": "",
    "text": "Lecture Notes UvA on 19-9-2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                AI\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                AI\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 3, 2023"
  },
  {
    "objectID": "blog/2023-10-03_the-perceptron/index.html#the-perceptron-model",
    "href": "blog/2023-10-03_the-perceptron/index.html#the-perceptron-model",
    "title": "The Perceptron",
    "section": "1 The Perceptron Model",
    "text": "1 The Perceptron Model\nit just a linear model where:\n\n\n\nHere \\(\\phi(x)=x\\) for instance.\n\n1.1 Error function\n\n\n\n\n\n1.2 The Perceptron: Learning\n\n\n\nHere \\(&lt;1\\) we can also have \\(\\gamma\\)\n\n\n\n\n\n1.3 Perceptron Learning as Gradient Descent\n\n\n\n\n\n1.4 Pros with the Perceptron\n\nThe algorithm guarantees to converge if the data is linear separable\n\n\n\n1.5 Problems with the Perceptron\n\nPerceptron only works for 2 classes\nCycling theorem: many solutions if data is not linearly separable\nBased on linear combination of fixed basis functions."
  },
  {
    "objectID": "blog/2023-09-26_matrix-calculus/index.html",
    "href": "blog/2023-09-26_matrix-calculus/index.html",
    "title": "Matrix Calculus & Derivatives Ax",
    "section": "",
    "text": "Derivatives of vectors, matrix and other utils\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Vector Calculus\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Vector Calculus\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 26, 2023"
  },
  {
    "objectID": "blog/2023-09-26_matrix-calculus/index.html#transpose-properties",
    "href": "blog/2023-09-26_matrix-calculus/index.html#transpose-properties",
    "title": "Matrix Calculus & Derivatives Ax",
    "section": "Transpose Properties",
    "text": "Transpose Properties\nIf \\(\\Sigma\\) is symmetric:\n\\[\n\\begin{align}\n\\Sigma = \\Sigma^T\\\\\n\\end{align}\n\\]\nThe inverse of a symmetric matrix is also symmetric\n\\[\n\\begin{align}\n(\\Sigma^{-1})^T = (\\Sigma^T)^{-1} = \\Sigma^{-1} \\label{cov_trans_inv} \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nProof Eq. \\(\\ref{cov_trans_inv}\\)\n\n\n\n\n\n\n\nCode\nimport numpy as np\n\n# Create a symmetric matrix\nA = np.array([[4, 1, 2],\n              [1, 5, 3],\n              [2, 3, 6]])\n\n# Check if A is symmetric\nis_symmetric = np.allclose(A, A.T)\n\nif is_symmetric:\n    # Calculate the inverse of the symmetric matrix\n    A_inv = np.linalg.inv(A)\n\n    print(\"Original symmetric matrix A:\")\n    print(A)\n\n    print(\"\\nInverse of A:\")\n    print(A_inv)\nelse:\n    print(\"The matrix A is not symmetric.\")\n\n\nOriginal symmetric matrix A:\n[[4 1 2]\n [1 5 3]\n [2 3 6]]\n\nInverse of A:\n[[ 0.3         0.         -0.1       ]\n [ 0.          0.28571429 -0.14285714]\n [-0.1        -0.14285714  0.27142857]]\n\n\nHere we have shown that the inverse of a symmetric matrix its also a symmetric matrix. It is not the same because doing the inverse you do other calculations but is symmetric around the diagonal"
  },
  {
    "objectID": "blog/2023-09-26_matrix-calculus/index.html#partial-derivatives",
    "href": "blog/2023-09-26_matrix-calculus/index.html#partial-derivatives",
    "title": "Matrix Calculus & Derivatives Ax",
    "section": "Partial Derivatives",
    "text": "Partial Derivatives\nVectors:\n\\[\n\\begin{align}\n\\frac{\\partial (\\textbf{v}^T \\textbf{v})}{\\partial\\textbf{v}} &= 2\\textbf{v}^T\\\\\n\\frac{\\partial (\\textbf{v} \\textbf{v}^T)}{\\partial\\textbf{v}} &= 2\\textbf{v}^T\\\\\n\\frac{\\partial (\\textbf{x}^T \\textbf{w})}{\\partial\\textbf{x}} &= \\textbf{w}^T\\\\\n\\frac{\\partial (\\textbf{w}^T \\textbf{x})}{\\partial\\textbf{x}} &= \\textbf{w}^T\\\\\n\\end{align}\n\\]\nMatrix: \\[\n\\begin{align}\n\\frac{\\partial (\\textbf{A} \\textbf{x})}{\\partial\\textbf{x}} &= \\textbf{A}\\\\\n\\frac{\\partial (\\textbf{x}^T \\textbf{A})}{\\partial\\textbf{x}} &= \\textbf{A}^T\\\\\n\\end{align}\n\\]\nWrt Vector: \\[\n\\begin{align}\n\\frac{\\partial(\\textbf{x}^T\\textbf{A}\\textbf{x})}{\\partial\\textbf{x}} =& \\textbf{x}^T(\\textbf{A}+\\textbf{A}^T)\\\\\n\\frac{\\partial(\\textbf{w}^T\\textbf{X}^T\\textbf{y})}{\\partial\\textbf{w}} =& \\textbf{y}^T\\textbf{X}\\\\\n\\frac{\\partial(\\textbf{y}^T\\textbf{X}\\textbf{w})}{\\partial\\textbf{w}} =& \\textbf{y}^T\\textbf{X}\\\\\n\\end{align}\n\\]\nWrt Matrix: \\[\n\\begin{align}\n\\frac{\\partial(\\textbf{x}^T\\textbf{A}\\textbf{x})}{\\partial\\textbf{A}} =& \\textbf{x}\\textbf{x}^T\\\\\n\\frac{\\partial(\\textbf{a}^T\\textbf{A}\\textbf{b})}{\\partial\\textbf{A}} =& \\textbf{a}\\textbf{b}^T\\\\\n\\frac{\\partial(\\textbf{a}^T\\textbf{A}^T\\textbf{b})}{\\partial\\textbf{A}} =& \\textbf{b}\\textbf{a}^T\\\\\n\\end{align}\n\\]\nSpecial: \\[\n\\begin{align}\n\\frac{\\partial \\textbf{a}^T\\textbf{X}\\textbf{b}}{\\partial\\textbf{X}} &= \\textbf{a}\\textbf{b}^T\\\\\n\\frac{\\partial \\textbf{a}^T\\textbf{X}^{-1}\\textbf{b}}{\\partial\\textbf{X}} &= -(\\textbf{X}^{-1})^T\\textbf{a}\\textbf{b}^T(\\textbf{X}^{-1})^T\\\\\n& = - (\\textbf{X}^{-1})\\textbf{a}\\textbf{b}^T(\\textbf{X}^{-1})\\quad &\\text{If $\\textbf{X}$ is symmetric} \\nonumber \\\\\n\\frac{\\partial (\\textbf{x}-\\textbf{A}\\textbf{s})^T\\textbf{W}(\\textbf{x}-\\textbf{A}\\textbf{s})}{\\partial\\textbf{A}} &= -2\\textbf{W}(\\textbf{x}-\\textbf{A}\\textbf{s})\\textbf{s}^T \\quad &\\text{If $\\textbf{W}$ is symmetric} \\\\\n\\frac{\\partial (\\textbf{x}-\\textbf{A}\\textbf{s})^T\\textbf{W}(\\textbf{x}-\\textbf{A}\\textbf{s})}{\\partial\\textbf{s}} &= -2(\\textbf{x}-\\textbf{A}\\textbf{s})^T\\textbf{W}\\textbf{A} \\quad &\\text{If $\\textbf{W}$ is symmetric} \\\\\n\\frac{\\partial (\\textbf{x}-\\textbf{s})^T\\textbf{W}(\\textbf{x}-\\textbf{s})}{\\partial\\textbf{s}} &= -2(\\textbf{x}-\\textbf{s})\\textbf{W}^T \\quad &\\text{If $\\textbf{W}$ is symmetric}\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2023-06-09_on-the-topic/index.html",
    "href": "blog/2023-06-09_on-the-topic/index.html",
    "title": "On the topic of Optimization",
    "section": "",
    "text": "Below the apps I use in a daily basis to make my life easier\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Workflow\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Lua\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Workflow\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Lua\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          June 9, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                     Microsoft Edge\n                        \n                                     Hammerspoon Code"
  },
  {
    "objectID": "blog/2023-06-09_on-the-topic/index.html#microsoft-edge",
    "href": "blog/2023-06-09_on-the-topic/index.html#microsoft-edge",
    "title": "On the topic of Optimization",
    "section": "1 Microsoft Edge",
    "text": "1 Microsoft Edge\n\nPrefer this over Safari because of add-ons. Chrome is the same\n\nI used it to install apps like YouTube or Google Translator so that when I hit a shortcut command, it opens that app. Here it’s an example."
  },
  {
    "objectID": "blog/2023-06-09_on-the-topic/index.html#hammerspoon",
    "href": "blog/2023-06-09_on-the-topic/index.html#hammerspoon",
    "title": "On the topic of Optimization",
    "section": "2 Hammerspoon",
    "text": "2 Hammerspoon\n\nI use this application to create global shortcuts. Donwload App\n\nUses Lua programming language to assign keybindings to certain actions. Here a sneak peek of what can you do.\n--- Open App\nfunction open(name)\n    return function()\n        hs.application.launchOrFocus(name)\n        if name == 'Finder' then\n            hs.appfinder.appFromName(name):activate()\n        end\n    end\nend\nFor complete implementation visit this repo.\nThe above function can be called to open an app: Google Maps by an i.e. the shortcut: ⌘ + M\n--- Binding Keys\nhs.hotkey.bind({ \"cmd\" }, \"M\", open(\"Google Maps\"))\n\n2.1 Windows Approach\nFor Windows alternative see the code below:\n\n\nCode\n\n#MaxHotkeysPerInterval 200\n!WheelUp::Volume_Up\n!WheelDown::Volume_Down\n!MButton::Volume_Mute\n&lt;#!e::\nRun, %WINDIR%\\explorer.exe\nreturn\n!q::Send !{F4}\nreturn\n&lt;#!x::\nRun, msedge.exe\nreturn\n!y::\nRun, C:\\Users\\datoapnta\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\YouTube.lnk\nreturn"
  },
  {
    "objectID": "blog/2023-06-09_on-the-topic/index.html#honourable-mentions",
    "href": "blog/2023-06-09_on-the-topic/index.html#honourable-mentions",
    "title": "On the topic of Optimization",
    "section": "3 Honourable Mentions",
    "text": "3 Honourable Mentions\n\nThe apps above can do the same, but I like the UX interface.\n\n\n\n\nApp\nDescription\n\n\n\n\nBetterTouchTool\nEnables trackpad new gestures shortcuts\n\n\nAutoHotkey\nSame as Hammerspoon but for Windows\n\n\nTiles\nWindows manager (allows keybindings)"
  },
  {
    "objectID": "blog/2023-12-07_deep-learning-&-the-natural-sciences/index.html",
    "href": "blog/2023-12-07_deep-learning-&-the-natural-sciences/index.html",
    "title": "Deep Learning & The Natural Sciences",
    "section": "",
    "text": "Deep Learning & The Natural Sciences\n        \n        \n                    \n                \n                    Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 7, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n\n\n\n\nSlide 1\n\n\n\n\n\n\n\n\nSlide 2\n\n\n\n\n\n\n\n\nSlide 3\n\n\n\n\n\n\n\n\nSlide 4\n\n\n\n\n\n\n\n\nSlide 5\n\n\n\n\n\n\n\n\nSlide 6\n\n\n\n\n\n\n\n\nSlide 7\n\n\n\n\n\n\n\n\nSlide 8\n\n\n\n\n\n\n\n\nSlide 9\n\n\n\n\n\n\n\n\nSlide 10\n\n\n\n\n\n\n\n\nSlide 11\n\n\n\n\n\n\n\n\nSlide 12\n\n\n\n\n\n\n\n\nSlide 13\n\n\n\n\n\n\n\n\nSlide 14\n\n\n\n\n\n\n\n\nSlide 15\n\n\n\n\n\n\n\n\nSlide 16\n\n\n\n\n\n\n\n\nSlide 17\n\n\n\n\n\n\n\n\nSlide 18\n\n\n\n\n\n\n\n\nSlide 19\n\n\n\n\n\n\n\n\nSlide 20\n\n\n\n\n\n\n\n\nSlide 21\n\n\n\n\n\n\n\n\nSlide 22\n\n\n\n\n\n\n\n\nSlide 23\n\n\n\n\n\n\n\n\nSlide 24\n\n\n\n\n\n\n\n\nSlide 25\n\n\n\n\n\n\n\n\nSlide 26\n\n\n\n\n\n\n\n\nSlide 27\n\n\n\n\n\n\n\n\nSlide 28\n\n\n\n\n\n\n\n\nSlide 29\n\n\n\n\n\n\n\n\nSlide 30\n\n\n\n\n\n\n\n\nSlide 31\n\n\n\n\n\n\n\n\nSlide 32\n\n\n\n\n\n\n\n\nSlide 33\n\n\n\n\n\n\n\n\nSlide 34\n\n\n\n\n\n\n\n\nSlide 35\n\n\n\n\n\n\n\n\nSlide 36\n\n\n\n\n\n\n\n\nSlide 37\n\n\n\n\n\n\n\n\nSlide 38\n\n\n\n\n\n\n\n\nSlide 39\n\n\n\n\n\n\n\n\nSlide 40\n\n\n\n\n\n\n\n\nSlide 41\n\n\n\n\n\n\n\n\nSlide 42\n\n\n\n\n\n\n\n\nSlide 43\n\n\n\n\n\n\n\n\nSlide 44\n\n\n\n\n\n\n\n\nSlide 45\n\n\n\n\n\n\n\n\nSlide 46\n\n\n\n\n\n\n\n\nSlide 47\n\n\n\n\n\n\n\n\nSlide 48\n\n\n\n\n\n\n\n\nSlide 49\n\n\n\n\n\n\n\n\nSlide 50\n\n\n\n\n\n\n\n\nSlide 51\n\n\n\n\n\n\n\n\nSlide 52\n\n\n\n\n\n\n\n\nSlide 53\n\n\n\n\n\n\n\n\nSlide 54\n\n\n\n\n\n\n\n\nSlide 55\n\n\n\n\n\n\n\n\nSlide 56\n\n\n\n\n\n\n\n\nSlide 57\n\n\n\n\ntional gap\n\n\n\n\nSlide 58\n\n\n\n\ntional gap\n\n\n\n\nSlide 59\n\n\n\n\nng the variational posterior\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html",
    "href": "blog/2023-10-11_principal-component-analysis/index.html",
    "title": "Principal Component Analysis (PCA)",
    "section": "",
    "text": "Lecture Notes UvA on 9-10-2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Linear models\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Linear models\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 11, 2023"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#continuous-latent-space",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#continuous-latent-space",
    "title": "Principal Component Analysis (PCA)",
    "section": "1 Continuous latent space",
    "text": "1 Continuous latent space\nThe idea is of latent variables is that the data is described in the low dimensional latent space and somehow it can map it to this hight dimensional space."
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#principal-component-analysis-pca",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#principal-component-analysis-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "2 Principal Component Analysis (PCA)",
    "text": "2 Principal Component Analysis (PCA)\nImagine I have the bottom right plot in multiple dimensions so not only 2D dimensions. I can compute a mean and a covariance matrix to fit this data.\nNow I pick only line and I will fit the data in 1D-dimension i.e using Gaussian with the mean and convariance from the original plot. This mapping depends on the direction on the line for the PCA, imagine now the green line below then the gaussian will look different\n\n\n\n\n\nSo we want to find the direction in which it maximizes the variance data. This is important because if you have a low variance then all the points map to this compressed graph and may look like one point when in reality there are plenty."
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#recall-orthonomal-projections",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#recall-orthonomal-projections",
    "title": "Principal Component Analysis (PCA)",
    "section": "3 Recall orthonomal projections",
    "text": "3 Recall orthonomal projections\n\nThe span of a vector."
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#d-projection",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#d-projection",
    "title": "Principal Component Analysis (PCA)",
    "section": "4 1D Projection",
    "text": "4 1D Projection\nRemarks:\n\nHere the \\(z_1\\) is the projected point into the 1D-dimensional space if we would like more than one dimension then this z_1 would be now a vector \\(\\textbf{z}_1\\) with dimensions \\(M\\)"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#maximizing-the-variance-of-1-component",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#maximizing-the-variance-of-1-component",
    "title": "Principal Component Analysis (PCA)",
    "section": "5 Maximizing the variance of 1 component",
    "text": "5 Maximizing the variance of 1 component\nThe principal components are the \\(u_1\\) the lines (the directions)\n\n\n\n\n\n5.1 PCA via maximum variance\n\n\n\n\n\n\n5.2 Reminder: eigen de-composition\n\n\n\n\nRemarks:\n\nThe total variance of our datapoints in the new dimensional space can be calculated by summing up the eigenvalues\nWe should think of \\(U\\) as a change of basis from the D-dimensional space to the new dimensional space. This new dimensional space its determined by how many eigevector at the end we choose\nBecause the eigenvectors are orthonormal that means when we apply do the change of basis with the A_weird we are decorrelating our data.\n\n\n\n5.3 How to choose M?\n\n\n\n\n\n\n5.4 Feature Decorrelation\n\n\n\n\n\n\n5.5 Applications: Whitening"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#probabilistic-pca",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#probabilistic-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "6 Probabilistic PCA",
    "text": "6 Probabilistic PCA\n\n\n\n\n\n\nRecall difference in Discrete and Continuous latent variable\n\n\n\n\nDiscrete latent models: k-means, Gaussian Mixture models\nContinuous latent models: probabilistic PCA, unsupervised regression\n\nIf you compare k-means as latent clarifier then this can be consider as an unsupervised regression\nIf you compare Gaussian-mixture model then this can be though as continuous PPCA model\n\n\nProbabilistic view of PCA:\n\nLearn it via maximum likelihood\n(Third) alternative view of PCA\nBoth latent and observed variables are Gaussian\n\n\n6.1 Continuous latent variable model\n\n\n\n\n\nWe assume that there is a latent space z from which we can sample a point z.\nThen I have this conditional p(x|z) given z what is the probability that is lands\n\n\n\n6.2 PPCA modelling assumptions\n\n\n\n\n\nRecall z was the latent variable, the hidden variable the one its making something that we have x. Remember toughs –&gt; words\n\n\nWe assume that x is formed by a linear combinations with the latent variable z, W, \\(\\mu\\) and \\(\\epsilon\\)\n\nHere we are saying that there is a linear relation between z and x.\n\n\nHere, \\(W\\), \\(\\mu\\) and \\(\\epsilon\\) are the parameters that we want to recover\nBecause z is Gaussian and noise is also modelled by Gaussian, then \\(x\\) will also be Gaussian\n\n\n\n6.3 It follows\n\n\n\n\n\n\n\n\nIn the covariance part we take out W because its not a random variable\n\n\n\n6.4 Probabilistic PCA in a picture\n\n\n\n\nFrom now on we can find the parameters by doing MLE\n\n\n6.5 The log-likelihood\n\n\n\n\n\n\n6.6 PPCA has closed-form solutions\n\n\n\n\n\n\n6.7 PPCA Summary\n\n\n\n\nThree views:\n\nMax variance, min reconstruction error, probabilistic\n\nApplications\n\nDimensionality reduction\n2D/3D visualization\nCompression\nWhitening (de-correlating features)\n(not mentioned) De-noising: discard the smallest variance features = the noise components (hopefully!)\n\nLimitations:\n\nOnly linear transformations\n\n\n\n6.8 Comparing PCA & PPCA\nThe PCA can be expressed as the maximum likelihood solution probabilistic PCA.\nAdvantages of the probabilistic PCA over the conventional PCA:\n\nWe can associate a likelihood function to the probabilistic PCA which allows a direct comparison with other probabilistic density models\nProbabilistic PCA can be used to model class-conditional densities and can thus be used in classification problems\nWe can run the model generatively to provide samples from the modeled distribution."
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#non-linear-pca",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#non-linear-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "7 Non Linear PCA",
    "text": "7 Non Linear PCA\nFind the subspace that maximizes the variance of the projection or minimizes the reconstruction error\nBy linear we mean all the data is cluster around one contour line like the green line. The question is can we do non-linear PCA where we have not the reed line which is clearly non-linear?"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#pca-using-basis-functions",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#pca-using-basis-functions",
    "title": "Principal Component Analysis (PCA)",
    "section": "8 PCA using basis functions",
    "text": "8 PCA using basis functions\nThe question is how do we map these non-linear data to a linear line such that we use basis function. We can do this in two ways:\n\n\n\n\n\nUsing Neural Networks\nUsing kernel methods: we can do the mapping from original space to the new space without explicit modeling the basis functions\n\n\n\n\n\n\n\nHow to craft Basis functions?\n\n\n\n\nWe can use Neural Networks. For instance \\(\\mathbf{\\phi(\\textbf{x})} = NN(\\textbf{x})\\), where \\(\\textbf{x}\\) is the input vector\nWe can use Kernel methods where we do not make explicit use of basis functions\n\n\n\nWe will first sketch the structure of an autoencoder"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#auto-encoders-auto-associative-neural-nets",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#auto-encoders-auto-associative-neural-nets",
    "title": "Principal Component Analysis (PCA)",
    "section": "9 Auto-encoders (auto-associative neural nets)",
    "text": "9 Auto-encoders (auto-associative neural nets)\n\n\n\n\nIf \\(f(x)=w^Tx\\) is a linear projection that goes from D to M then this resembles to PCA where we have \\(z=\\mu_M^T(x-\\bar x)\\). So if the function \\(f(x)\\) is of form like in the PCA case then we have a linear projection that can transform our points to a lower dimensional space"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#autoencoder-objective",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#autoencoder-objective",
    "title": "Principal Component Analysis (PCA)",
    "section": "10 Autoencoder objective",
    "text": "10 Autoencoder objective\n\n\n\n\n\nThe mapping from input through the encoder, we call it the latent \\(z_n\\). The later is like a compression.\nBefore we will project the data into our principal components but now we let the NN learn what the latent mapping should be\nWhen we carry out this encoder and decoder without using activation functions we end up in the PCA structure\nIf we use however activation function and we use more layers than 2, then we have a non-linear model which has no closed solutions and therefore we can solve it by SGD because our error is non-convex"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#autoencoder-as-generator",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#autoencoder-as-generator",
    "title": "Principal Component Analysis (PCA)",
    "section": "11 Autoencoder as generator",
    "text": "11 Autoencoder as generator"
  },
  {
    "objectID": "blog/2023-10-11_principal-component-analysis/index.html#kernel-pca",
    "href": "blog/2023-10-11_principal-component-analysis/index.html#kernel-pca",
    "title": "Principal Component Analysis (PCA)",
    "section": "12 Kernel PCA",
    "text": "12 Kernel PCA\nWe are use to express things in terms of features vectors, i.e the latent is obtained by taking the features vectors and projecting it onto a lower dimensional space\nNow we are going to report our results in terms of the kernel \\(k(\\textbf{x}\\textbf{x}_n)\\). The result is that the projection would be purely in terms of the other data points via the kernel but not by my other parameters\nThis is another way how to do PCA for non-linear"
  },
  {
    "objectID": "blog/2023-01-12_2022-into-2023/index.html",
    "href": "blog/2023-01-12_2022-into-2023/index.html",
    "title": "2022 into 2023",
    "section": "",
    "text": "2022 into 2023\n        \n        \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                News\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                News\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          January 12, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\nHappy New Year! Time to write another year in review. This will be the first time I’ve done this. Here are all the previous ones:\n\n2022 into 2023\n\nI have really high hopes for 2023! It’s only a few days in and so far so good. I’ve really had a chance to relax, rest, and reset."
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html",
    "title": "Kernel methods & SVM",
    "section": "",
    "text": "Lecture Notes UvA on 9-10-2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 15, 2023"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernel-methods",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernel-methods",
    "title": "Kernel methods & SVM",
    "section": "1 Kernel Methods",
    "text": "1 Kernel Methods\nIn Unsupervised Learning we assume a latent target. In supervised we have a target variable in Unsupervised we assume there is some relation between a latent variable \\(z\\) and our datapoints.\nSupervised Learning:\n\nRegression: is continuous, the target \\(\\in \\mathbb{R}\\)\n\nProbabilistic modelling:\n\nIn regression we want to have a continuous target variable\n\nFind model parameters via ML, MAP (or use fully Bayesian)\nIf we want to avoid overfilling we include a prior\n\n\n\n\n\n\nClassification: is discrete, the target \\(\\in \\{C1,...C_k\\}\\) finite set of options\n\nProbabilistic modelling:\n\nWe also predict distributions. Here we want to predict a probability per each class ie. generalized bernoulli distribution\nFind model parameters via ML, MAP (or use fully Bayesian)\n\nIf we want to avoid overfilling we include a prior\n\n\n\n\n\n\n\nUnsupervised Learning:\n\nClustering: is discrete. Here there is some latent classes \\(z\\)\n\nHere we can think of unsupervised classification i.e. K-Means Clustering\n\nDimensionality Reduction: is continuous. PCA assumes a continuous latent variable\n\nHere we think about an unsupervised regression\n\n\nProbabilistic methods:\n\nDefine (predictive) distributions\nFind model parameters via ML, MAP (or use fully Bayesian)\n\nDiscriminative methods:\n\nHere we no think about probabilities we just want to make decisions\nIt is more algorithmic in nature\n\nSimilarities between methods:\nDiscriminative methods (left) & Probabilistic methods (Right)\n\nLeast Square Regression &lt;-&gt; MLE Gaussian predictive distribution\nRidge Regression &lt;-&gt; MAP\nK-Means (hard assignment) &lt;-&gt; Gaussian Mixture Models (soft assigment dot point of color green but also red bluish)\n\n\nNew: we will add parametric and non-parametric models\n\nParametric models:\n\nLinear models \\(y(x)=Wx+b\\)\nGeneralized Linear models \\(y(x)=\\sigma(Wx+b)\\)\nNNs i.e our weights W and bias b\nDistribution classes (Gaussian, Bernoulli,..) i.e parametrized by mean, cov, etc\nBasis functions (this is like a hyperparameter)\n\nNon-parametric models:\n\nKernel methods: does not have parameters but still does predictions\nSVM\n\n\nTool in Machine learning is optimization\n\nOptimization:\n\nConvex (one solution: we can solve it analytically) vs non-convex (we used numerical aka brute force solutions like SGD)\nFind stationary points (solve derivative =0)\nAnalytic solutions vs numerical (SGD)\nMethod of Lagrange multipliers (used because sometimes our optimization needs to obey some constraint)\n\nEquality constraint optimization\nInequality constraint optimization"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#so-far-parametric-models",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#so-far-parametric-models",
    "title": "Kernel methods & SVM",
    "section": "2 So Far: Parametric Models",
    "text": "2 So Far: Parametric Models\n\n\n\n\n\nWe teak the parameters highlighted in green color"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#parametric-vs-non-parametric-models",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#parametric-vs-non-parametric-models",
    "title": "Kernel methods & SVM",
    "section": "3 Parametric vs Non-Parametric Models",
    "text": "3 Parametric vs Non-Parametric Models\n\n\n\n\n\nIn parametric you train then discard data and use weights W or other parameters, in Non-parametric you do predictions but you always carry the data to kae such predictions.\n\nBecause carrying big data is not efficient then you may use SVM\n\nHere we define infinitely number of functions spaces \\(M=\\infty\\)\nThis means we have a continuos representation of our space. We can sample as finely as we want, so we can discretized and the index would be one function, because \\(M=\\infty\\) then we adjust this index to the sampling rate to make it more or less smother"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#non-parametric-kernel-methods",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#non-parametric-kernel-methods",
    "title": "Kernel methods & SVM",
    "section": "4 Non-Parametric Kernel Methods",
    "text": "4 Non-Parametric Kernel Methods\n\n\n\n\nRidge regression can be defined in kernelize form without using explicit using basis functions.\n\nPrimal for the parametric case\nDual for the non-parametric representation of the model\n\nFor linear models:\n\nThe kernel it just computing the similarity between \\(x\\) and \\(x'\\)"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernelized-ridge-regression",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernelized-ridge-regression",
    "title": "Kernel methods & SVM",
    "section": "5 Kernelized Ridge Regression",
    "text": "5 Kernelized Ridge Regression"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#primal-vs-dualkernel-approach",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#primal-vs-dualkernel-approach",
    "title": "Kernel methods & SVM",
    "section": "6 Primal vs Dual/Kernel Approach",
    "text": "6 Primal vs Dual/Kernel Approach"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernel-trickkernel-substitution",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernel-trickkernel-substitution",
    "title": "Kernel methods & SVM",
    "section": "7 Kernel Trick/Kernel Substitution",
    "text": "7 Kernel Trick/Kernel Substitution\n\n\n\n\n\n\nHow it works?\n\n\n\nThe kernel trick its like calculating similarities with the input datapoint, the prediction would be heavily influenced based on point pairs that are similar\nThe kernel trick is whenever I see the form \\(\\textbf{x}_n^T \\textbf{x}_n\\) (instead of using basis functions) we will replace it with the kernel\n\\[\n\\begin{align}\n\\textbf{k}(\\textbf{x}_n^T, \\textbf{x}_m) = \\textbf{K}_{nm}\n\\end{align}\n\\]\nWe do not know what kernel will be but we can prove that if the kernel is symetric positive semi definite that there are always corresponding basis function \\(\\mathbb{\\phi}(\\textbf{x}_n)^T\\mathbb{\\phi}(\\textbf{x}_n)\\)\nSo basically we find \\(\\mathbb{\\phi}(\\textbf{x}_n)^T\\mathbb{\\phi}(\\textbf{x}_n) = \\textbf{k}(\\textbf{x}_n^T, \\textbf{x}_m) = \\textbf{K}_{nm}\\)\n\n\n\n\n\n\nIf a choice any basis function it induces a kernel simply by computing inner product in the new feature space"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#deriving-the-corresponding-feature-vector",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#deriving-the-corresponding-feature-vector",
    "title": "Kernel methods & SVM",
    "section": "8 Deriving the corresponding feature vector",
    "text": "8 Deriving the corresponding feature vector\n\n\n\n\nFor every positive definite kernel there exist a set of basis from \\(R^d\\) to \\(R^M\\). Meaning there would be a set of basis which means instead of solving for the basis we can just get a valid kernel and not learn the basis\n\n\n\n\n\n\nWhy do I want to go from basis (finite) to valid kernels (can be infinitely?\n\n\n\n\nThis is important because now I do not need to limit myself to find a a dimensional feature descriptors i.e basis in \\(R^d\\) (i.e basis like polynomials basis, or gaussians basis) now I can have a valid kernel that can basically be infinitely in basis functions \\(M = \\infty\\)\n\nSidenote: there is more chance to overfilling\n\n\n\n8.1 Note on infinite dimensional feature space of Gaussian kernels"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernel-trick-kernel-substitution",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#kernel-trick-kernel-substitution",
    "title": "Kernel methods & SVM",
    "section": "9 Kernel Trick/ Kernel substitution",
    "text": "9 Kernel Trick/ Kernel substitution"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#example-polynomial-kernel",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#example-polynomial-kernel",
    "title": "Kernel methods & SVM",
    "section": "10 Example: polynomial kernel",
    "text": "10 Example: polynomial kernel\n\n\n\n\nThis shows that if we have this polynomial kernel then that we can decompose it into \\(\\mathbb{\\phi}(\\textbf{x}_n)^T\\mathbb{\\phi}(\\textbf{x}_n)\\)\nWhere: \\(\\mathbb{\\phi}(\\textbf{x}) \\in \\mathbb{R}^{6}\\)"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#examples-of-valid-kernels",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#examples-of-valid-kernels",
    "title": "Kernel methods & SVM",
    "section": "11 Examples of valid Kernels",
    "text": "11 Examples of valid Kernels\n\n\n\n\n\nThe Gaussian kernel produces infinitely \\(M = \\infty\\) many basis functions"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#support-vector-machines",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#support-vector-machines",
    "title": "Kernel methods & SVM",
    "section": "12 Support vector machines",
    "text": "12 Support vector machines\nSVM are kernel methods but with sparse solutions. MEaning some \\(a_n = 0\\) so that we do not compute all datapoints but only relevant. The ones that are relevant are called suport vectors\n\n\n\n\n\nWe are guaranteed to find a solution because it is a Convex optimization problem"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#linearly-separable-dataset",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#linearly-separable-dataset",
    "title": "Kernel methods & SVM",
    "section": "13 Linearly Separable dataset",
    "text": "13 Linearly Separable dataset\n\n\n\n\n\nOne way to tell whether a classifier is better than other is to look at how far us the margin from a closes point to the decision boundary. If the margin is large then I have a stable classifier.\n\n\nGoal: we want to maximize the margin to have a stable classifier. To classify the size of the margin we can use some linear projections to the boundary we see this in the next topic\n\n\n\n\n\nThe graph above is taking into account that we classify the data correctly in two parts so the blue points in one side the red in another."
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#maximum-margin-classifier",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#maximum-margin-classifier",
    "title": "Kernel methods & SVM",
    "section": "14 Maximum Margin Classifier",
    "text": "14 Maximum Margin Classifier\nNow that we have a expression for the margin our objective is to maximize this margin\n\n\n\n\n\nIn this case we have \\(r_n\\) so we could tune \\(w\\) to obtain the maximum margin to a point. How do we do this? we follow:\n\n\n\n\n\n\nIdentify the closest point. The \\(\\kappa\\) (kappa) its introduced to say we can amplify our distance we can i.e have the distance in kiloliters or miles etc.\nTo resolve the ambiguity of measuring in kilometers or in miles or so for, we set our \\(||\\kappa \\textbf{w}|| = 1\\). This is essentially setting the unit which you want to compute the distance. You are saying then the closest point should have unit 1 this is a constraint\n\nFrom this step it follows that all the points would be my prediction times label \\(t_n \\, y_n\\) will be greater or equal to \\(1\\)\n\nWe maximize the size of the margin given by 1/|W| with the inequality constraint\n\n\n\n\n\n\n\n\n\n\n\nMaximum Margin Classifier: Goal\n\n\n\n\nMaximize \\(\\frac{1}{\\textbf{w}}\\) means minimize \\(\\frac{1}{2}||\\textbf{w}||^2\\)\nWe have \\(N\\) constraints because we need to go over each datapoint\nIt is a convex quadratic optimization problem with a quadratic loss and linear constraint. We solve this by Lagrange Multipliers"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#constrained-optimization-inequality-constraint",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#constrained-optimization-inequality-constraint",
    "title": "Kernel methods & SVM",
    "section": "15 Constrained optimization (inequality constraint)",
    "text": "15 Constrained optimization (inequality constraint)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow the solution should lie within the yellow region\n\nThere is two cases: 1. The objective could lie inside the yellow region. Then I am doing maximization without any constriant 2. If the solution lies outside then the solution would lie on the perpendicular to the yellow boundary. See point \\(x_A\\) where there is an arrow pointing to \\(x\\) (the optimum solution)\nBefore\n\nWe solve the lagrange multiplier, derive with respect to parameter ie. \\(w\\) then derive wrt to \\(\\lambda\\) and then replace this solution into \\(w\\).\n\nNow (we need additional requirements):\n\nDefine Primal Lagrangian function: \\(L = f(x) - constraint\\)\nCompute the dual lagrangian\nWith solvers solve the dual problem\n\n\nSidenote: dual optimizer (is convex) gives you optimal solutions, it is a way of convexifiyng a problem\n\nYou need to think that the dual gives you an upper bound, and you are lowering this by setting \\(\\mu\\) as much as possible"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#svm-maximum-margin-classifier",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#svm-maximum-margin-classifier",
    "title": "Kernel methods & SVM",
    "section": "16 SVM: Maximum Margin Classifier",
    "text": "16 SVM: Maximum Margin Classifier\n\nFor a maximization problem we \\(+\\) (sum) our constraint\nFor a minimization problem we \\(-\\) (subtract) our constraint\nThen you approach the limit from below\n\n\n16.1 Calculating the Lagrange\n\n\n\n\n\n\n\n\n\n\nSteps to compute the lagrange\n\n\n\n\nGoal: we want to obtain optimal values for \\(\\textbf{w}^*\\) and \\(b^*\\)\n\n\nWrite down:\n\nPrimal Lagrange function: \\(L = f(x) - constraints\\)\nWrite the KKT conditions\n\nDerive the Dual Lagrangian by setting \\(\\frac{\\partial L}{\\partial \\textbf{w}}=0\\), \\(\\frac{\\partial L}{\\partial b}=0\\)\n\nLet the machine compute what is the argmax \\(a\\) so that we obtain our parameters \\(\\textbf{w}^*\\) and \\(b^*\\)\n\n\n\n\nFor the case of SVM we start by step 1.\n\n\n\n\n\nWe write the primal Lagrange with the constriant. Also, we introduce the dual variables (lagrange multipliers) \\(\\textbf{a}_n\\) meaning for every each datapoint we want it to lie in the proper side of the boundary. This condition that \\(\\textbf{a}_n\\) imposes comes from the KKT conditions\n\nWe solve for \\(\\frac{\\partial L}{\\partial \\textbf{w}}=0\\), \\(\\frac{\\partial L}{\\partial b}=0\\) and rewrite the Primal to the Dual Lagrangian\n\n\n\n\n\nOnce we have those parameters we plug in again in the Lagrange in step 1. Then the function will be dependent on \\(a_n\\) which is then called the Dual Lagrangian\n\n\n\n\n\nRemember Dual Lagrangian would be convex so we know there would be unique solution\n\nNow that we have derived the dual we can now apply the kernel trick as follows:"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#sparse-solutions-due-to-kkt",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#sparse-solutions-due-to-kkt",
    "title": "Kernel methods & SVM",
    "section": "17 Sparse solutions due to KKT",
    "text": "17 Sparse solutions due to KKT\n\n\n\n\n\n\nKernel trick uses fewer points rather than \\(x_n\\) these are called: Support vectors\n\n\n\nDue to kernel trick we now would have used all the datapoints in our dataset but this is no longer the case because of our complimentary slackness which says that points liying on \\(y=1\\) or \\(y=-1\\) or better said when this \\(t_ny(\\textbf{x}_n)\\) is true then their lagrange multipliers: \\(a_n\\) would be zero.\nWhich means we would a couple of datapoints instead of the whole batch \\(x_n\\). These selected points are called the support vectors"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#svm-solution-for-bias-b",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#svm-solution-for-bias-b",
    "title": "Kernel methods & SVM",
    "section": "18 SVM: Solution for bias b",
    "text": "18 SVM: Solution for bias b\nSo far we have solve \\(y(\\textbf{x})\\) but we still have \\(\\textbf{b}\\). So now we solve for this latter variable:\nRecall we have the constraint that we derive deriving the primal wrt to \\(b\\):\n\n\n\n\nThat means I can pick whathever \\(\\textbf{b}\\) I want as long the KKT constraints are satisfied\nNo we use the final constraint that \\(t_ny_n(x)=1\\)\n\n\n\n\n\nIn practice when we found a solution for b its better to average them"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#maximum-margin-classifier-contours",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#maximum-margin-classifier-contours",
    "title": "Kernel methods & SVM",
    "section": "19 Maximum Margin Classifier: Contours",
    "text": "19 Maximum Margin Classifier: Contours\n\n\n\n\n\nIf we use a gaussian kernel then our Maximal Margin Classifier we will always be able to classify the points i.e we may end up with an island to classify only one point.\nThe reason why the contours are smooth is because of the sigma parameter from the gaussian. The small the sigma the more wably (unpredictable), the larger the smoother.\nNote on the sigma value: sigma very small then the similarity decays. That means the the decision boundaries would be very curvy aka irregular decision boundary. If I make sigma very large then the countours become smoother"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#note-on-outliers",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#note-on-outliers",
    "title": "Kernel methods & SVM",
    "section": "20 Note on outliers",
    "text": "20 Note on outliers\n\n\n\n\nDue to the capability of the Maximum Margin Classifier all outliers would be calssified well which sometimes may not be desirable, to avoid this irregular boundaries we introduce slackness variables. This notion leads to SVM with Soft Margins"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#soft-margin-classifiers",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#soft-margin-classifiers",
    "title": "Kernel methods & SVM",
    "section": "21 Soft Margin Classifiers",
    "text": "21 Soft Margin Classifiers\n\n\n\n\n\nWe do not move the decision boundary, we allow outliers but we add a penalty to them\nPenalty is proportional to the boundary\n\n\n\n\n\nSo now because we introduce this slack variable we have a new constraint optimization problem."
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#maximum-margin-classifiers-soft-margins",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#maximum-margin-classifiers-soft-margins",
    "title": "Kernel methods & SVM",
    "section": "22 Maximum Margin Classifiers: Soft Margins",
    "text": "22 Maximum Margin Classifiers: Soft Margins\n\n\n\n\n\nThe \\(C\\) variable would be the one that decides which points get penalize. That is, \\(C\\) prevents that every point get a slack and, our goal now is to avoid as many points to be in the wrong side of the decision boundaries.\nThus we are now minimizing three parameters: \\(\\textbf{w}\\), \\(\\textbf{b}\\) and \\(\\xi_n\\), where the slack variable should always be positive variable. This induces the following constraints\n\nSolving steps 1 -&gt; 2:\n\n\n\n\n\nPrimal variables: \\(\\textbf{w}\\), \\(\\textbf{b}\\) and \\(\\xi_n\\)\nDual variables: \\(a_n\\) and \\(\\mu_n\\)\n\nSolving steps 3 -&gt; 4:\n\nSolving the derivatives for the primal variables \\(\\textbf{w}\\), \\(\\textbf{b}\\) and \\(\\xi_n\\)\n\n\n\n\n\nSolving mid-step: Box constraints:\n\n\n\n\n\nThe box constraint comes from the green highlighted equations"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#wrapping-up-maximum-classifiers-w-soft-margins",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#wrapping-up-maximum-classifiers-w-soft-margins",
    "title": "Kernel methods & SVM",
    "section": "23 Wrapping up: Maximum Classifiers w/ Soft Margins",
    "text": "23 Wrapping up: Maximum Classifiers w/ Soft Margins\n\nIf there is no constraint \\(C -&gt; 0\\): then you allow every point in the boundary to have slack because there is essentially no penalty. That means:\n\nYou are more flexible to outliers\nYour margin becomes infinitely wide\nEvery datapoint becomes a support vector\nThere is no sparcity to the kernel matrix, because all the points would be a support vector\n\n\n\n\nIf my \\(C -&gt; \\infty\\) then I do not allow slack so that means I am very strick because I am penalizing very hugely to my slack that means I will not allow points that corss the margin. Which brings you to the case of hard margin\n\nThis means if I have an outlier I will have an smaller decision boundary because that outlier will be considered as a support vector, therefore it will lower the margin size which means I will have an unstable model when a random datapoint appears\nBefore & After"
  },
  {
    "objectID": "blog/2023-10-15_kernel-methods-&-svm/index.html#clarification-of-problems",
    "href": "blog/2023-10-15_kernel-methods-&-svm/index.html#clarification-of-problems",
    "title": "Kernel methods & SVM",
    "section": "24 Clarification of problems",
    "text": "24 Clarification of problems\nConvex optimization problems\n\n\nLinear Regression\n\nLinear regression with L2 regularization (ridge regression) and L1 regularization (lasso regression) are convex optimization problems. Ridge regression minimizes the sum of squared errors plus the L2 norm of the coefficients, while lasso minimizes the sum of squared errors plus the L1 norm of the coefficients\n\n\n\nLogistic Regression with L1 or L2 Regularization\n\nSimilar to linear regression, logistic regression can incorporate L1 or L2 regularization terms, making it a convex optimization problem\n\n\n\nSVM\n\nWe can use the Dual Lagrangian together with the kernel trick so that we find a convex solution. The dual isolates the \\(a_n\\) and also \\(x_n x_n^T\\) so then in the end we use the kernel trick and because we can express it with gaussian which where infinitely then we know we will find a solution\n\n\n\nQuadratic Discriminant Analysis (QDA)\n\nQDA is a supervised learning algorithm with a quadratic decision boundary and can be formulated as a convex optimization problem\n\n\n\nPerceptron\n\n\nNon-Convex optimization problems:\n\n\nK-Means Clustering\n\n\n\n\nGMM\n\nWe use the EM algorithm because the maths get heavy so there we run into possibility of getting stuck in local minima\n\n\n\nPCA\n\n\n\n\nNeural Networks\n\nbecause of the non-linearity introduced by the activation functions"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html",
    "title": "Convolutional Neural Networks",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 20, 2023"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#optimizing-neural-networks",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#optimizing-neural-networks",
    "title": "Convolutional Neural Networks",
    "section": "1 Optimizing neural networks",
    "text": "1 Optimizing neural networks\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#multi-layer-perceptrons-recap",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#multi-layer-perceptrons-recap",
    "title": "Convolutional Neural Networks",
    "section": "2 Multi-layer perceptrons (Recap)",
    "text": "2 Multi-layer perceptrons (Recap)\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#multi-layer-perceptrons-recap-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#multi-layer-perceptrons-recap-1",
    "title": "Convolutional Neural Networks",
    "section": "3 Multi-layer perceptrons (Recap)",
    "text": "3 Multi-layer perceptrons (Recap)\n\n\n\n\nSlide 4\n\n\n\n\nPrior knowledge, is something that we know about the data, we want to bring this into the design of the NNs"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#consider-an-image",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#consider-an-image",
    "title": "Convolutional Neural Networks",
    "section": "4 Consider an image",
    "text": "4 Consider an image\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#hubel-and-wiesel-nobel-prize-for-physiology-or-medicine-in-1981",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#hubel-and-wiesel-nobel-prize-for-physiology-or-medicine-in-1981",
    "title": "Convolutional Neural Networks",
    "section": "5 Hubel and Wiesel: Nobel Prize for Physiology or Medicine in 1981",
    "text": "5 Hubel and Wiesel: Nobel Prize for Physiology or Medicine in 1981\n\n\n\n\nSlide 6\n\n\n\n\nHere when we see edges, there is some electricity"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#filters-yes.-how-about-learnable-filters",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#filters-yes.-how-about-learnable-filters",
    "title": "Convolutional Neural Networks",
    "section": "6 Filters, yes. How about learnable filters",
    "text": "6 Filters, yes. How about learnable filters\n\n\n\n\nSlide 7\n\n\n\n\nCanny and GAbor filters they all try to find edges, then it can be used for recognition purposes."
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#filters-yes.-how-about-learnable-filters-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#filters-yes.-how-about-learnable-filters-1",
    "title": "Convolutional Neural Networks",
    "section": "7 Filters, yes. How about learnable filters",
    "text": "7 Filters, yes. How about learnable filters\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-convolution-operation",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-convolution-operation",
    "title": "Convolutional Neural Networks",
    "section": "8 The convolution operation",
    "text": "8 The convolution operation\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-convolution-operation-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-convolution-operation-1",
    "title": "Convolutional Neural Networks",
    "section": "9 The convolution operation",
    "text": "9 The convolution operation\n\n\n\n\nSlide 10\n\n\n\n\n\n\n\n\n\n\n\n\nHere f*g is the convolution in red line. In the 1D case\nHere g is the kernel"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-for-2d-images",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-for-2d-images",
    "title": "Convolutional Neural Networks",
    "section": "10 Convolution for 2D images",
    "text": "10 Convolution for 2D images\n\n\n\n\nSlide 11\n\n\n\n\nNow our kernel is 2D"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-for-2d-images-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-for-2d-images-1",
    "title": "Convolutional Neural Networks",
    "section": "11 Convolution for 2D images",
    "text": "11 Convolution for 2D images\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#examples",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#examples",
    "title": "Convolutional Neural Networks",
    "section": "12 Examples",
    "text": "12 Examples\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#examples-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#examples-1",
    "title": "Convolutional Neural Networks",
    "section": "13 Examples",
    "text": "13 Examples\n\n\n\n\nSlide 14\n\n\n\n\nSobel fires for vertical edges"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#quiz",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#quiz",
    "title": "Convolutional Neural Networks",
    "section": "14 Quiz",
    "text": "14 Quiz\n\n\n\n\nSlide 15\n\n\n\n\n\nIt will emphasize edges\n\nIf you take a CNN and you have weights uniform then you would not have this edge detectors and the NN would not train well\nThis is like adding prior knowledge because we know edges are supper important to detect whether is a cat or a dog"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "15 The motivation of convolutions",
    "text": "15 The motivation of convolutions\n\n\n\n\nSlide 16\n\n\n\n\nLocal connectivity, for ie if you want to detect edges, you dont need to look at the whole image and because you share the parameters, the weights are tied and you are more efficient."
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-1",
    "title": "Convolutional Neural Networks",
    "section": "16 The motivation of convolutions",
    "text": "16 The motivation of convolutions\n\n\n\n\nSlide 17\n\n\n\n\nThis saves quite a bit of neurons connected, so less parameters, this is the same as analysing an img of 16x16, but now instead we use filters so that we can detect edges and only with these edges we have now building blocks which are less than computing the whole image.\nHere the kernel would be of width 3"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-2",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-2",
    "title": "Convolutional Neural Networks",
    "section": "17 The motivation of convolutions",
    "text": "17 The motivation of convolutions\n\n\n\n\nSlide 18\n\n\n\n\nSo here in the left the NN has a receptive field of size 3, because this is how much a neuron can look up, so it is the kernel size. But per layer the receptive field gradually grows which allow you to have a hierarchical structure\n\nFor instance the neurons at layer 50 now they can see at the whole image and can put image into context. This is how you go from local to global"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-3",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-3",
    "title": "Convolutional Neural Networks",
    "section": "18 The motivation of convolutions",
    "text": "18 The motivation of convolutions\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-4",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-motivation-of-convolutions-4",
    "title": "Convolutional Neural Networks",
    "section": "19 The motivation of convolutions",
    "text": "19 The motivation of convolutions\n\n\n\n\nSlide 20\n\n\n\n\nIf the input shift then the outputs does the same, this is not the case for a fully connected NN"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#a-simple-convolution-saves-space",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#a-simple-convolution-saves-space",
    "title": "Convolutional Neural Networks",
    "section": "20 A simple convolution: saves space!",
    "text": "20 A simple convolution: saves space!\n\n\n\n\nSlide 21\n\n\n\n\nThe bigger the filter the more zeros we will have"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-vs-pooling-in-2d",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-vs-pooling-in-2d",
    "title": "Convolutional Neural Networks",
    "section": "21 Convolution vs Pooling in 2D",
    "text": "21 Convolution vs Pooling in 2D"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-pooling-operations",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-pooling-operations",
    "title": "Convolutional Neural Networks",
    "section": "22 The pooling operations",
    "text": "22 The pooling operations\n\n\n\n\nSlide 22\n\n\n\n\nPooling functions are another way to incorporate prior knowledge. It aggregates the activations. This can be local or global\nYou can max pool, or average pool the activations in some rectangular neighborhood. It reduces the space size and improves the efficiency and it also increases robustness\nIt also incorporates invariance to translations, because it will not matter whether the 6 would be in that corner or so on so on\nAt the last step you could od average global pooling, and just have one vector out, and in this vector will be trained to represent the whole image. Here you could apply a fully connected layer if you care classification\nMin, max all are differentiable. If you instead would have and argmax then it will not be differentiable\nSo pooling operations like the global ones, also allow you to be independent in which input image you feed into your NN"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#lenet-5",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#lenet-5",
    "title": "Convolutional Neural Networks",
    "section": "23 LeNet-5",
    "text": "23 LeNet-5\n\n\n\n\nSlide 23\n\n\n\n\nHere you do not have global pooling so an img of 29x29 would not have worked\nThe hidden dimensionalities are called channels, and the pooling is applied to all channels. So pooling operations do not change dimensionality but change the spatial extent.\nSo each layer would have channels those are all the squares in a layer, pooling it is apploed to every channel and per channel it reduces its square matrix to a lower width and lower height\n\n23.1 More\nLeNet-5, a convolutional neural network architecture proposed by Yann LeCun and his collaborators in 1998, does not use global average pooling in its original design. LeNet-5 primarily relies on subsampling layers (pooling layers) and fully connected layers.\nThe typical structure of LeNet-5 consists of alternating convolutional layers with subsampling (pooling) layers, followed by fully connected layers. The pooling layers in LeNet-5 perform down-sampling through operations like max pooling. Global average pooling was not a commonly used technique at the time LeNet-5 was introduced.\nGlobal average pooling became more prominent in later CNN architectures, such as Google’s Inception models and the popular ResNet architectures\n\n\n23.2 Global Pooling\nGlobal pooling (or global average pooling) is a technique used in convolutional neural networks (CNNs) to reduce the spatial dimensions of a feature map to a single value or a vector. It involves taking the average (or maximum) value across all spatial locations of each feature map, resulting in a global representation.\nHere’s an example of global average pooling with Python using NumPy:\nimport numpy as np\n\n# Assume you have a 3x3 feature map with 2 channels\nfeature_map = np.array([\n    [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n    [[10, 11, 12], [13, 14, 15], [16, 17, 18]]\n])\n\n# Apply global average pooling\nglobal_avg_pooled = np.mean(feature_map, axis=(0, 1))\n\n# Print the original feature map and the result after global average pooling\nprint(\"Original Feature Map:\")\nprint(feature_map)\nprint(\"\\nGlobal Average Pooled Result:\")\nprint(global_avg_pooled)\nIn this example, feature_map is a 3x3 feature map with 2 channels. The np.mean function is used to compute the average along the spatial dimensions (axis 0 and 1). The resulting global_avg_pooled is a vector representing the global average-pooled values for each channel.\nThe output should look like this:\nOriginal Feature Map:\n[[[ 1  2  3]\n  [ 4  5  6]\n  [ 7  8  9]]\n\n [[10 11 12]\n  [13 14 15]\n  [16 17 18]]]\n\nGlobal Average Pooled Result:\n[8.5 9.5 10.5]\nIn this case, the global average pooling operation has computed the average value for each channel across all spatial locations, resulting in a global representation for each channel. This global representation is often used as a compact and informative input to subsequent layers or for making predictions in the network."
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#alexnet-similar-principles-but-some-extra-engineering.",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#alexnet-similar-principles-but-some-extra-engineering.",
    "title": "Convolutional Neural Networks",
    "section": "24 AlexNet: similar principles, but some extra engineering.",
    "text": "24 AlexNet: similar principles, but some extra engineering.\n\n\n\n\nSlide 24\n\n\n\n\n\n\n\n\n\nSlide 25\n\n\n\n\nWeight sharing in convolutional neural networks (CNNs) refers to the practice of using the same set of learnable parameters (weights and biases) for multiple units or neurons in a layer. In other words, the weights are shared across different spatial locations in the input.\nThe key idea behind weight sharing is to enforce translation invariance in the features learned by the convolutional layers. In an image, certain features (e.g., edges, textures) are meaningful regardless of their specific location. By using shared weights, the network can learn to detect these features at different spatial positions, leading to a more robust and generalizable representation.\nHere’s a brief explanation of weight sharing in CNNs:\n\nConvolutional Operation:\n\nIn a convolutional layer, a set of filters (also known as kernels) is applied to the input image or feature map.\nEach filter is characterized by a set of learnable weights and biases.\n\nSpatial Weight Sharing:\n\nInstead of having unique weights for each spatial location in the input, weight sharing involves using the same set of weights across different spatial locations.\nFor example, if a filter detects a certain feature (e.g., an edge) at one location, the same filter with the same weights can be used to detect the same feature at a different location.\n\nBenefits:\n\nReduces the number of learnable parameters in the network, making it more computationally efficient.\nEncourages the learning of spatially invariant features, enhancing the network’s ability to recognize patterns across different locations.\n\nTranslation Invariance:\n\nWeight sharing helps the network achieve translation invariance, meaning that it can recognize features regardless of their position in the input.\n\n\n\n24.1 CNN and Weight Sharing\nCNN is primarily used for image classification and segmentation, and it works by finding similar patterns throughout the input. These patterns can be found by sliding a filter with shared weights across the input. The shared weights concept allows the network to learn the same pattern, regardless of its position in the input"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#what-shape-should-the",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#what-shape-should-the",
    "title": "Convolutional Neural Networks",
    "section": "25 What shape should the",
    "text": "25 What shape should the\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations",
    "title": "Convolutional Neural Networks",
    "section": "26 3D Activations",
    "text": "26 3D Activations\n\n\n\n\nSlide 30\n\n\n\n\nInstead of calling it RGB channels, we just call it channels"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-1",
    "title": "Convolutional Neural Networks",
    "section": "27 3D Activations",
    "text": "27 3D Activations\n\n\n\n\nSlide 32\n\n\n\n\nNow the activations contain width and height and also depth.\nThe depth is govern by the hidden dimensionality of the NN"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-2",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-2",
    "title": "Convolutional Neural Networks",
    "section": "28 3D Activations",
    "text": "28 3D Activations\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-3",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-3",
    "title": "Convolutional Neural Networks",
    "section": "29 3D Activations",
    "text": "29 3D Activations\n\n\n\n\nSlide 35\n\n\n\n\nHere this is a convolution kernel, with kernel size 5x5\nNow our neuron has a kernel size of 5x5 weights and also x3 because it has 3 channels\nSo each Neuron as a 3D filter"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-4",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-4",
    "title": "Convolutional Neural Networks",
    "section": "30 3D Activations",
    "text": "30 3D Activations\n\n\n\n\nSlide 38\n\n\n\n\n\n\n30.1 Example: Not-moving Filter\n\n\n\n\nHere we have:\n\nInput Layer: 3x32x32\nKernel 5-size: 3x5x5\n\nIf we do not slide the filter then we are gonna end up with:\n\nPre-output: 3x1x1 (three scalar values per each channel)\n\nNow we do a summation over the three channels and we have thus:\n\nOutput: 1x1x1\n\n\n\n30.2 Example: Sliding Filter\n \nImagine now that we slide this 3D filter along all the input layer, then we end up:\n\nPre-ouput: 3x(28)x(28), where we compute (width - kernel_size + 1) = (32-5+1)\n\nNow we sum over all three channel element wise and end up with:\n\nOutput: 1x28x28"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-5",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-5",
    "title": "Convolutional Neural Networks",
    "section": "31 3D Activations",
    "text": "31 3D Activations\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-6",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-6",
    "title": "Convolutional Neural Networks",
    "section": "32 3D Activations",
    "text": "32 3D Activations\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-7",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-7",
    "title": "Convolutional Neural Networks",
    "section": "33 3D Activations",
    "text": "33 3D Activations\n\n\n\n\nSlide 45\n\n\n\n\nIf you now slide the filter with as many neurons we will get:\n\nOuput: depth x (\\(l\\)) x (\\(l\\))\n\nWhere \\(l\\) = width - kernel_size + 1"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-8",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-8",
    "title": "Convolutional Neural Networks",
    "section": "34 3D Activations",
    "text": "34 3D Activations\n\n\n\n\nSlide 47"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-9",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-9",
    "title": "Convolutional Neural Networks",
    "section": "35 3D Activations",
    "text": "35 3D Activations\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-10",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-10",
    "title": "Convolutional Neural Networks",
    "section": "36 3D Activations",
    "text": "36 3D Activations\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-11",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-11",
    "title": "Convolutional Neural Networks",
    "section": "37 3D Activations",
    "text": "37 3D Activations\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-12",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-12",
    "title": "Convolutional Neural Networks",
    "section": "38 3D Activations",
    "text": "38 3D Activations\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-13",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-13",
    "title": "Convolutional Neural Networks",
    "section": "39 3D Activations",
    "text": "39 3D Activations\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-14",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#d-activations-14",
    "title": "Convolutional Neural Networks",
    "section": "40 3D Activations",
    "text": "40 3D Activations\n\n\n\n\nSlide 56"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together",
    "title": "Convolutional Neural Networks",
    "section": "41 Putting it together",
    "text": "41 Putting it together\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-1",
    "title": "Convolutional Neural Networks",
    "section": "42 Putting it together",
    "text": "42 Putting it together\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-2",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-2",
    "title": "Convolutional Neural Networks",
    "section": "43 Putting it together",
    "text": "43 Putting it together\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-3",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-3",
    "title": "Convolutional Neural Networks",
    "section": "44 Putting it together",
    "text": "44 Putting it together\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-4",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-4",
    "title": "Convolutional Neural Networks",
    "section": "45 Putting it together",
    "text": "45 Putting it together\n\n\n\n\nSlide 61"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-5",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-5",
    "title": "Convolutional Neural Networks",
    "section": "46 Putting it together",
    "text": "46 Putting it together\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-6",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-6",
    "title": "Convolutional Neural Networks",
    "section": "47 Putting it together",
    "text": "47 Putting it together\n\n\n\n\nSlide 63"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-7",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#putting-it-together-7",
    "title": "Convolutional Neural Networks",
    "section": "48 Putting it together",
    "text": "48 Putting it together\n\n\n\n\nSlide 64\n\n\n\n\nHere all these coloured layers in the cube are filters which are neurons. All these neurons act only in one hidden layer8"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride",
    "title": "Convolutional Neural Networks",
    "section": "49 Convolution: Stride",
    "text": "49 Convolution: Stride\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-1",
    "title": "Convolutional Neural Networks",
    "section": "50 Convolution: Stride",
    "text": "50 Convolution: Stride\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-2",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-2",
    "title": "Convolutional Neural Networks",
    "section": "51 Convolution: Stride",
    "text": "51 Convolution: Stride\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-3",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-3",
    "title": "Convolutional Neural Networks",
    "section": "52 Convolution: Stride",
    "text": "52 Convolution: Stride\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-4",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-4",
    "title": "Convolutional Neural Networks",
    "section": "53 Convolution: Stride",
    "text": "53 Convolution: Stride\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-5",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-5",
    "title": "Convolutional Neural Networks",
    "section": "54 Convolution: Stride",
    "text": "54 Convolution: Stride\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-6",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-6",
    "title": "Convolutional Neural Networks",
    "section": "55 Convolution: Stride",
    "text": "55 Convolution: Stride\n\n\n\n\nSlide 71\n\n\n\n\nIn each time sum across channels because if you want to detect something you want to use all the colors, all the incoming channels\nSo each convolution sums all channels like in RGB, because you dont want a filter that only looks at blue, one that only looks at red .."
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-7",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-7",
    "title": "Convolutional Neural Networks",
    "section": "56 Convolution: Stride",
    "text": "56 Convolution: Stride\n\n\n\n\nSlide 72\n\n\n\n\nHere in the next slide we see that stride is the number of squares that are moved, so the kernel filter will i.e stride=3 will slide every two squares"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-8",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-8",
    "title": "Convolutional Neural Networks",
    "section": "57 Convolution: Stride",
    "text": "57 Convolution: Stride\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-9",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-9",
    "title": "Convolutional Neural Networks",
    "section": "58 Convolution: Stride",
    "text": "58 Convolution: Stride\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-10",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-stride-10",
    "title": "Convolutional Neural Networks",
    "section": "59 Convolution: Stride",
    "text": "59 Convolution: Stride\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding",
    "title": "Convolutional Neural Networks",
    "section": "60 Convolution: Padding",
    "text": "60 Convolution: Padding\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding-1",
    "title": "Convolutional Neural Networks",
    "section": "61 Convolution: Padding",
    "text": "61 Convolution: Padding\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding-2",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding-2",
    "title": "Convolutional Neural Networks",
    "section": "62 Convolution: Padding",
    "text": "62 Convolution: Padding\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding-3",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution-padding-3",
    "title": "Convolutional Neural Networks",
    "section": "63 Convolution: Padding",
    "text": "63 Convolution: Padding\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convolution",
    "title": "Convolutional Neural Networks",
    "section": "64 Convolution:",
    "text": "64 Convolution:\n\n\n\n\nSlide 81\n\n\n\n\nW_out is what you get in the ouput layer of the slide (so for one filter)"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#x1-convolution",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#x1-convolution",
    "title": "Convolutional Neural Networks",
    "section": "65 1x1 Convolution",
    "text": "65 1x1 Convolution\n\n\n\n\nSlide 82\n\n\n\n\nIt looks at all the values in the depth, so in RGB, or more importantly in deep NNs the different hidden layers and they just mixed those information together"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#x1-convolution-a-computationally-cheap-method",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#x1-convolution-a-computationally-cheap-method",
    "title": "Convolutional Neural Networks",
    "section": "66 1x1 Convolution: a computationally cheap method",
    "text": "66 1x1 Convolution: a computationally cheap method\n\n\n\n\nSlide 83\n\n\n\n\nHere in the 5x5x32 the convolution will be done over the 192 channels, 32 times (because we have 32 filters as depth), so sliding the filter a lot\nIn the bottom case we reduce the number of channels so now we reduce computations"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#quiz-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#quiz-1",
    "title": "Convolutional Neural Networks",
    "section": "67 Quiz:",
    "text": "67 Quiz:\n\n\n\n\nSlide 84\n\n\n\n\n\n\n\n\nIn the case of a fully connected layer, it connects everything in the layer. And even the 1x1 convolution it takes the full input dimensionality meaning it takes a look at all 192 channels still. In the 1x1 it reduces the number of connections comparing to fully connected and also it mixes local information\n\n\n\n\n\n\n\nMore differences:\n\n\n\n\n\nA 1x1 convolutional layer and a fully-connected layer (dense layer) are similar in that they both perform a linear transformation on the input data, but there are key differences between the two.\n\n67.1 1x1 Convolutional Layer:\n\nSpatial Information:\n\nA 1x1 convolutional layer operates on spatial information in the input tensor.\nIt applies convolutional filters with a size of 1x1, which means it processes information at individual spatial locations.\nUseful for capturing relationships between channels but does not capture spatial patterns.\n\nParameter Sharing:\n\nUtilizes parameter sharing, similar to larger convolutional layers.\nEach element in the output is the result of a weighted sum of its input elements, considering all channels.\n\nOutput Dimensions:\n\nThe output dimensions depend on the number of 1x1 filters used.\n\n\n\n\n67.2 Fully-Connected Layer:\n\nFlattening:\n\nA fully-connected layer operates on the flattened version of the input.\nIt considers all elements in the input tensor as individual input features.\n\nParameter Sharing:\n\nEach neuron in a fully-connected layer has its set of weights for every input feature.\nNo parameter sharing between different neurons.\n\nOutput Dimensions:\n\nThe output dimensions are determined by the number of neurons in the layer.\n\n\n\n\n67.3 Differences:\n\nSpatial vs. Global Information:\n\n1x1 convolutional layers capture spatial information within each channel.\nFully-connected layers operate on global information, considering all elements as individual features.\n\nParameter Sharing:\n\n1x1 convolutions use parameter sharing, making them more efficient for processing spatially correlated features.\nFully-connected layers lack parameter sharing, resulting in a larger number of parameters.\n\nComputational Efficiency:\n\n1x1 convolutions are computationally more efficient than fully-connected layers, especially in scenarios with spatially structured data.\n\nUsage in Convolutional Networks:\n\n1x1 convolutions are commonly used in convolutional neural networks (CNNs) to adjust the number of channels and perform feature transformations.\nFully-connected layers are typically used in the final layers of a neural network for classification.\n\n\n\n\n\n\n\n\n\n\nYou dont loss necessarily information, we do not want to do it at the beginning because mixing, red, blue and green per pixel does not do much. It makes sense to do it later if you have edges on top of edges and then you mix this information, it makes more sense.\n\nIt is also not good to apply 1x1 when you do not want translation invariance\nevery 1x1 is strictly local, every neuron as a receptive field so there is actually spatial information there"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#dilated-convolutions",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#dilated-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "68 Dilated Convolutions",
    "text": "68 Dilated Convolutions\n\n\n\n\nSlide 85\n\n\n\n\nThis is very usefull if you need to deal with a huge image, but dont want huge hidden activations\nIf you do this then you can quickly downscale the image, without ignoring too many things\nAlso think that dilation its less expensive because doing 5x5 its more expensive than doing 3x3, so you can learn 3x3 but with holes in between and that is more efficient to consider large spatial footprint"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#pooling",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#pooling",
    "title": "Convolutional Neural Networks",
    "section": "69 Pooling",
    "text": "69 Pooling\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#pooling-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#pooling-1",
    "title": "Convolutional Neural Networks",
    "section": "70 Pooling",
    "text": "70 Pooling\n\n\n\n\nSlide 87"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#max-pooling",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#max-pooling",
    "title": "Convolutional Neural Networks",
    "section": "71 Max Pooling",
    "text": "71 Max Pooling\n\n\n\n\nSlide 88"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#getting-rid-of-pooling",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#getting-rid-of-pooling",
    "title": "Convolutional Neural Networks",
    "section": "72 Getting rid of pooling",
    "text": "72 Getting rid of pooling\n\n\n\n\nSlide 89\n\n\n\n\nInstead of using pooling you can use a larger stride (so how many squares we slide) that we talk about\nIn Transformers pooling it is also not used anymore\nIn CNN is used"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#example-convnet",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#example-convnet",
    "title": "Convolutional Neural Networks",
    "section": "73 Example ConvNet",
    "text": "73 Example ConvNet\n\n\n\n\nSlide 93\n\n\n\n\nEvery filter is one row here"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#quiz-2",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#quiz-2",
    "title": "Convolutional Neural Networks",
    "section": "74 Quiz",
    "text": "74 Quiz\n\n\n\n\nSlide 94\n\n\n\n\nIf you choose the kernel size to be the same as the input image then it is fully connected.\nMathematically 1\nImplementation wise 2"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#how-research-gets-done-part-4",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#how-research-gets-done-part-4",
    "title": "Convolutional Neural Networks",
    "section": "75 How research gets done part 4",
    "text": "75 How research gets done part 4\n\n\n\n\nSlide 95\n\n\n\n\n\n\n\n\nSlide 96"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#alexnet",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#alexnet",
    "title": "Convolutional Neural Networks",
    "section": "76 AlexNet",
    "text": "76 AlexNet\n\n\n\n\nSlide 97"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#alexnet-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#alexnet-1",
    "title": "Convolutional Neural Networks",
    "section": "77 AlexNet",
    "text": "77 AlexNet\n\n\n\n\nSlide 98"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#activation-function",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#activation-function",
    "title": "Convolutional Neural Networks",
    "section": "78 Activation function",
    "text": "78 Activation function\n\n\n\n\nSlide 99\n\n\n\n\nFaster to train because of simple Relu, and also the gradients are not vanishing because you have the gradient of 1 starting from the positive direction\nWhy does gradient do not vanish with Relu?\nThe vanishing gradient problem refers to the issue where the gradients of the loss function with respect to the weights become extremely small during backpropagation, making it challenging for the model to learn and update its parameters effectively. This problem is particularly associated with activation functions that squash their input into a small range, such as the sigmoid or hyperbolic tangent (tanh) functions.\nReLU (Rectified Linear Unit), on the other hand, has a non-saturating activation behavior, which means that it does not squash its input into a small range.\nReLU does not saturate in the positive region of its input. For positive input values, the gradient remains constant (1), leading to consistent and non-vanishing gradients during backpropagation."
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#activation-function-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#activation-function-1",
    "title": "Convolutional Neural Networks",
    "section": "79 Activation function",
    "text": "79 Activation function\n\n\n\n\nSlide 100"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#training-with-multiple-gpus",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#training-with-multiple-gpus",
    "title": "Convolutional Neural Networks",
    "section": "80 Training with multiple GPUs",
    "text": "80 Training with multiple GPUs\n\n\n\n\nSlide 101"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#training-with-multiple-gpus-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#training-with-multiple-gpus-1",
    "title": "Convolutional Neural Networks",
    "section": "81 Training with multiple GPUs",
    "text": "81 Training with multiple GPUs\n\n\n\n\nSlide 102"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#on-that-note-communicating-between-gpus-pytorch",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#on-that-note-communicating-between-gpus-pytorch",
    "title": "Convolutional Neural Networks",
    "section": "82 On that note: Communicating between GPUs: PyTorch",
    "text": "82 On that note: Communicating between GPUs: PyTorch\n\n\n\n\nSlide 103"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#local-response-normalization",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#local-response-normalization",
    "title": "Convolutional Neural Networks",
    "section": "83 Local Response Normalization",
    "text": "83 Local Response Normalization\n\n\n\n\nSlide 104"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#overlapping-pooling",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#overlapping-pooling",
    "title": "Convolutional Neural Networks",
    "section": "84 Overlapping Pooling",
    "text": "84 Overlapping Pooling\n\n\n\n\nSlide 105"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#overlapping-pooling-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#overlapping-pooling-1",
    "title": "Convolutional Neural Networks",
    "section": "85 Overlapping Pooling",
    "text": "85 Overlapping Pooling\n\n\n\n\nSlide 106"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#overall-architecture",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#overall-architecture",
    "title": "Convolutional Neural Networks",
    "section": "86 Overall architecture",
    "text": "86 Overall architecture\n\n\n\n\nSlide 107\n\n\n\n\nThe max pooling make a vector per every image"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-overfitting-problem",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-overfitting-problem",
    "title": "Convolutional Neural Networks",
    "section": "87 The Overfitting Problem",
    "text": "87 The Overfitting Problem\n\n\n\n\nSlide 108\n\n\n\n\nIf a have a cnn that has many parameter more than my data input will i overfit?\nIf your CNN has a large number of parameters (i.e., it’s a complex model) and you have a small dataset, there is an increased risk of overfitting. A complex model may have the capacity to memorize the training data, capturing noise and outliers instead of learning generalizable patterns.\nAlthoug all these increase training time but high performance"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#the-learned-filters",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#the-learned-filters",
    "title": "Convolutional Neural Networks",
    "section": "88 The learned filters",
    "text": "88 The learned filters\n\n\n\n\nSlide 109"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-7",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-7",
    "title": "Convolutional Neural Networks",
    "section": "89 Removing layer 7",
    "text": "89 Removing layer 7\n\n\n\n\nSlide 110"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-6-7",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-6-7",
    "title": "Convolutional Neural Networks",
    "section": "90 Removing layer 6, 7",
    "text": "90 Removing layer 6, 7\n\n\n\n\nSlide 111"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-3-4",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-3-4",
    "title": "Convolutional Neural Networks",
    "section": "91 Removing layer 3, 4",
    "text": "91 Removing layer 3, 4\n\n\n\n\nSlide 112\n\n\n\n\nWe dont save that much parameters because convolutional layers are more efficient (they are not fully connected, not too many parameters)"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-3-4-6-7",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#removing-layer-3-4-6-7",
    "title": "Convolutional Neural Networks",
    "section": "92 Removing layer 3, 4, 6, 7",
    "text": "92 Removing layer 3, 4, 6, 7\n\n\n\n\nSlide 113"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#translation-invariance",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#translation-invariance",
    "title": "Convolutional Neural Networks",
    "section": "93 Translation invariance",
    "text": "93 Translation invariance\n\n\n\n\nSlide 114\n\n\n\n\nDespite saying that CNN tend to be equivariant which means if you shift the input the output should also shift you can see that if you do that with these images, where you are just shifting the images the outputs do vary quite a lot\nSo CNN do not learn something that is explicit symmetrical or explicitly equivariant. Equivariance may be a good prior that we put in, but that does not mean that that really happens"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#scale-invariance",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#scale-invariance",
    "title": "Convolutional Neural Networks",
    "section": "94 Scale invariance",
    "text": "94 Scale invariance\n\n\n\n\nSlide 115\n\n\n\n\nSame with scale, we have said that we apply the pooling operations so therefore we can be a bit invariant to scaling, but still NNs tend not to be super scale invariant"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#rotation-invariance",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#rotation-invariance",
    "title": "Convolutional Neural Networks",
    "section": "95 Rotation invariance",
    "text": "95 Rotation invariance\n\n\n\n\nSlide 116"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#further-reading",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#further-reading",
    "title": "Convolutional Neural Networks",
    "section": "96 Further reading",
    "text": "96 Further reading\n\n\n\n\nSlide 117\n\n\n\n\n\n\n\n\nSlide 118"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#transfer-learning-carry-benefits-from-large-dataset-to-the-small-one",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#transfer-learning-carry-benefits-from-large-dataset-to-the-small-one",
    "title": "Convolutional Neural Networks",
    "section": "97 Transfer learning: carry benefits from large dataset to the small one!",
    "text": "97 Transfer learning: carry benefits from large dataset to the small one!\n\n\n\n\nSlide 119"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#update-transfer-learning",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#update-transfer-learning",
    "title": "Convolutional Neural Networks",
    "section": "98 UPDATE: Transfer learning",
    "text": "98 UPDATE: Transfer learning\n\n\n\n\nSlide 120"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#why-use-transfer-learning",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#why-use-transfer-learning",
    "title": "Convolutional Neural Networks",
    "section": "99 Why use Transfer Learning?",
    "text": "99 Why use Transfer Learning?\n\n\n\n\nSlide 121\n\n\n\n\nThe answer is yes even if you have saved the weights from a extremely good model and you have a small dataset"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#convnets-are-good-in-transfer-learning",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#convnets-are-good-in-transfer-learning",
    "title": "Convolutional Neural Networks",
    "section": "100 Convnets are good in transfer learning",
    "text": "100 Convnets are good in transfer learning\n\n\n\n\nSlide 122\n\n\n\n\nFine Tune the whole NN\nOr use the CNN as feature extractor"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#solution-i-fine-tune-ht-using-hs-as-initialization",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#solution-i-fine-tune-ht-using-hs-as-initialization",
    "title": "Convolutional Neural Networks",
    "section": "101 Solution I: Fine-tune hT using hS as initialization",
    "text": "101 Solution I: Fine-tune hT using hS as initialization\n\n\n\n\nSlide 123"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#initializing-ht-with-hs",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#initializing-ht-with-hs",
    "title": "Convolutional Neural Networks",
    "section": "102 Initializing hT with hS",
    "text": "102 Initializing hT with hS\n\n\n\n\nSlide 124\n\n\n\n\nImagnet, it outputs 1000 categories. If you want to classification for 30 categories then you need to throw that one away and restart training a new classifier to your needs\nAlexNet, you can start removing some layers depending on how much data you have"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#initializing-ht-with-hs-1",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#initializing-ht-with-hs-1",
    "title": "Convolutional Neural Networks",
    "section": "103 Initializing hT with hS",
    "text": "103 Initializing hT with hS\n\n\n\n\nSlide 125\n\n\n\n\nif you pertained your NN in ImgNet and now you want to do Satalite classification then it may be usefull to find tune even those layers the bottom ones"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#how-to-fine-tune",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#how-to-fine-tune",
    "title": "Convolutional Neural Networks",
    "section": "104 How to fine-tune?",
    "text": "104 How to fine-tune?\n\n\n\n\nSlide 126"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#solution-ii-use-hs-as-a-feature-extractor-for-ht",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#solution-ii-use-hs-as-a-feature-extractor-for-ht",
    "title": "Convolutional Neural Networks",
    "section": "105 Solution II: Use hS, as a feature extractor for hT",
    "text": "105 Solution II: Use hS, as a feature extractor for hT\n\n\n\n\nSlide 127"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#transfer-learning-benchmarks-techniques",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#transfer-learning-benchmarks-techniques",
    "title": "Convolutional Neural Networks",
    "section": "106 Transfer learning benchmarks & techniques",
    "text": "106 Transfer learning benchmarks & techniques\n\n\n\n\nSlide 128"
  },
  {
    "objectID": "blog/2023-11-20_convolutional-neural-networks/index.html#title",
    "href": "blog/2023-11-20_convolutional-neural-networks/index.html#title",
    "title": "Convolutional Neural Networks",
    "section": "107 Title",
    "text": "107 Title\n\n\n\n\nSlide 129"
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "Books\n\n\nThis section covers various books that I have read and think are worth recommending.\n\n\n\n\n\n\nGoal of this section\n\n\n\n\n\n\nRefer to this page when my friends ask me what I read\nKeep a colection of books including the author’s name\n\n\n\n\n\n\n\n\n\n        \n            \n                \n                Atomic Habits\n                James Clear\n            \n        \n\n\n        \n            \n                \n                Unclutter Your Life in One Week\n                Erin Rooney Doland\n            \n        \n\n\n\n         \n            \n                \n                \n                    The Tipping Point\n                    Malcolm Gladwell\n                \n            \n        \n\n\n\n        \n            \n                \n                Mindset\n                Carol S. Dweck\n            \n        \n\n\n\n        \n            \n                \n                The Four-Hour Workweek\n                Tim Ferriss\n            \n        \n\n\n\n        \n            \n                \n                Blink\n                Malcolm Gladwell\n            \n        \n\n\n\n        \n            \n                \n                The Tipping Point\n                Malcolm Gladwell\n            \n        \n\n\n\n        \n            \n                \n                The Information\n                James Gleick\n            \n        \n\n\n\n        \n            \n                \n                The Practice\n                Seth Godin\n            \n        \n\n\n\n        \n            \n                \n                All Marketers are Liars\n                Seth Godin\n            \n        \n\n\n\n        \n            \n                \n                Emotional Intelligence\n                Daniel Goleman\n            \n        \n\n\n\n        \n            \n                \n                Originals\n                Adam Grant\n            \n        \n\n\n\n        \n            \n                \n                You Can't Make This Stuff Up\n                Lee Gutkind\n            \n        \n\n\n\n        \n            \n                \n                Mindfulness in Plain English\n                Bhante Henepola Gunaratana\n            \n        \n\n\n\n        \n            \n                \n                The Joy of Less\n                Francine Jay\n            \n        \n\n\n\n        \n            \n                \n                Farsighted\n                Steven Johnson\n            \n        \n\n\n\n        \n            \n                \n                Where Good Ideas Come From\n                Steven Johnson"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Published",
    "section": "",
    "text": "# \n        \n        \n          projects:\n        \n      \n      Published\n    \n  \n\n  \n    \n      \n        \n          Show All Projects  \n        \n      \n    \n  \n  \n\n \n\n\n\n\n     \n        \n        \n            \n            1\n            \n            2\n            \n            3\n            \n            4\n        \n            \n                \n                1\n                \n                2\n        \n        \n            \n            5\n            \n            6\n            \n            7\n        \n            \n                \n                3\n        \n        \n            \n            8\n            \n            9\n            \n            10\n        \n            \n                \n                4\n        \n        \n            \n            11\n            \n            12\n            \n            13\n            \n            14\n        \n            \n                \n                5\n                \n                6\n        \n        \n            \n            15\n            \n            16\n            \n            17\n        \n            \n                \n                7\n                \n                8\n        \n        \n            \n            18\n            \n            19\n            \n            20\n            \n            21\n        \n            \n                \n                9\n        \n        \n            \n            22\n            \n            23\n        \n            \n                \n                10\n        \n        \n            \n            24\n            \n            25\n        \n            \n                \n                11\n\n\n        \n        \n            \n            \n                \n                    \n                        \n                            Website\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    In this project I have created a website to hold my notes, show my projects, share my contact details. If you are interested in the project make sure to click on the source icon to take a look at the complete source. Cheers \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                HTML\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                Javascript\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                SCSS\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Blog\n                                                                   \n                                                        \n                                                        \n                                                            \n                                                                \n                                                                    Research\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    Website\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    HTML\n                                   \n                            \n                            \n                                \n                                    Javascript\n                                   \n                            \n                            \n                                \n                                    SCSS\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Blog\n                                       \n                            \n                            \n                                \n                                    \n                                        Research\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            ML Project\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                Python\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                OpenSpiel\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Machine Learning\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    ML Project\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    Python\n                                   \n                            \n                            \n                                \n                                    OpenSpiel\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Machine Learning\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            EA Project\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                Python\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                Numba\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Optimization\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    EA Project\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    Python\n                                   \n                            \n                            \n                                \n                                    Numba\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Optimization\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            CV Project\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                Python\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                Kaggle\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                Tensorflow\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Computer Vision\n                                                                   \n                                                        \n                                                        \n                                                            \n                                                                \n                                                                    Control Systems\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    CV Project\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    Python\n                                   \n                            \n                            \n                                \n                                    Kaggle\n                                   \n                            \n                            \n                                \n                                    Tensorflow\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Computer Vision\n                                       \n                            \n                            \n                                \n                                    \n                                        Control Systems\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            AI Project\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                Python\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                LateX\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Machine Learning\n                                                                   \n                                                        \n                                                        \n                                                            \n                                                                \n                                                                    Algorithms\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    AI Project\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    Python\n                                   \n                            \n                            \n                                \n                                    LateX\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Machine Learning\n                                       \n                            \n                            \n                                \n                                    \n                                        Algorithms\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            BSc Thesis\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Paper\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                C++\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                LateX\n                                                               \n                                                        \n                                                        \n                                                            \n                                                            \n                                                                Arduino\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Electrical Engineering\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    BSc Thesis\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Paper\n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    C++\n                                   \n                            \n                            \n                                \n                                    LateX\n                                   \n                            \n                            \n                                \n                                    Arduino\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Electrical Engineering\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            C++ Project\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                C++\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Software Engineering\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    C++ Project\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    C++\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Software Engineering\n                                       \n                            \n                 \n            \n        \n        \n        \n            \n            \n                \n                    \n                        \n                            Control Systems Project\n                            \n                            ×\n                            \n                        \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                        \n                                    \n                                    \n                                    \n                                        \n                                                \n                                                    \n                                                \n                                            \n                                            \n                                                About\n                                             \n                                                \n                                                    Source\n                                                    \n                                                 \n                                                \n                                             \n                                                 \n                                                        \n                                                            \n                                                            \n                                                                20-sim\n                                                               \n                                                        \n                                             \n                                            \n                                            \n                                                \n                                                \n                                                    \n                                                    \n                                                    Tags:\n                                                    \n                                                \n                                                 \n                                                        \n                                                            \n                                                                \n                                                                    Electrical Engineering\n                                                                   \n                                                        \n                                             \n                                        \n                                     \n                                 \n                            \n                         \n                     \n                  \n             \n            \n            \n            \n                \n                    Control Systems Project\n                \n                \n            \n            \n                \n                \n                    About\n                 \n                    \n                        Source\n                        \n                        \n                     \n                    \n                 \n                     \n                            \n                                \n                                    20-sim\n                                   \n                            \n                 \n                \n                \n                    \n                    \n                        \n                        \n                        \n                        Tags:\n                        \n                    \n                     \n                            \n                                \n                                    \n                                        Electrical Engineering\n                                       \n                            \n                 \n            \n        \n\n\n         \n            All\n        \n         \n            HTML\n        \n         \n            Javascript\n        \n         \n            SCSS\n        \n         \n            All\n        \n         \n            Python\n        \n         \n            OpenSpiel\n        \n         \n            All\n        \n         \n            Python\n        \n         \n            Numba\n        \n         \n            All\n        \n         \n            Python\n        \n         \n            Kaggle\n        \n         \n            Tensorflow\n        \n         \n            All\n        \n         \n            Python\n        \n         \n            LateX\n        \n         \n            All\n        \n         \n            C++\n        \n         \n            LateX\n        \n         \n            Arduino\n        \n         \n            All\n        \n         \n            C++\n        \n         \n            All\n        \n         \n            20-sim\n        \n         \n            Blog\n        \n         \n            Research\n        \n         \n            Machine Learning\n        \n         \n            Optimization\n        \n         \n            Computer Vision\n        \n         \n            Control Systems\n        \n         \n            Machine Learning\n        \n         \n            Algorithms\n        \n         \n            Electrical Engineering\n        \n         \n            Software Engineering\n        \n         \n            Electrical Engineering\n        \n         \n            website\n        \n         \n            ml_project\n        \n         \n            ea_project\n        \n         \n            cv_project\n        \n         \n            ai_project\n        \n         \n            bsc_thesis\n        \n         \n            c_project\n        \n         \n            control_systems_project\n        \n\n\nNo matching items"
  },
  {
    "objectID": "coming-soon.html",
    "href": "coming-soon.html",
    "title": "Coming Soon",
    "section": "",
    "text": "Coming Soon\n        \n    \n    \n        \n            The page you requested is under construction."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Danilo Toapanta",
    "section": "",
    "text": "Hi, there! This is Danilo.\n                    \n                    I’m an AI researcher based in the Netherlands.\n                    I studied Computer Science & Electrical Engineering in my undergraduate programme and now I am doing my master in Artificial Intelligence.\n                    When I am not trying to figure out how my computer works, I like to read books, run, go for a swim, and spend some quality time with my family.\n                    \n                    \n                        You can look me up on Github↗. \n                        Check out my LinkedIn↗. \n                        Read my lastest post. \n                        \n                        Or send me an\n                            email↗."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Danilo Toapanta",
    "section": "",
    "text": "Quick links:  Slides  |  Projects  | Books \n\n\nNews\n\n\n    \n        \n            Sep 2023 - I am attending, OpenAI: AI and the Future of Humanity\n            Jun 2023 - I will do an intership at IMEC\n            \n            Feb 2023 - Now the website has a Blog section\n            Nov 2023 - Have completed AI Berkeley Course\n        \n    \n\n\n\n\n\n\n\nNote\n\n\n\nThis website is under construction. To review changes visit DEV page of this site."
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html",
    "href": "blog/2023-09-04_mnist-classification/index.html",
    "title": "MNIST Classification",
    "section": "",
    "text": "Classification example using Tensorflow\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 4, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                    \n                 Notebook\n                        \n                                    \n                 requirements.txt"
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#how-to-run-locally",
    "href": "blog/2023-09-04_mnist-classification/index.html#how-to-run-locally",
    "title": "MNIST Classification",
    "section": "1 How to run locally",
    "text": "1 How to run locally\n$ pip install -r requirements.txt"
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#importing-all-libraries",
    "href": "blog/2023-09-04_mnist-classification/index.html#importing-all-libraries",
    "title": "MNIST Classification",
    "section": "2 Importing all libraries",
    "text": "2 Importing all libraries\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport cv2\nnp.random.seed(42)                          # This allows us to reproduce the results from our script\nfrom keras.models import Sequential             \nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam, SGD\nfrom keras.utils import to_categorical \n\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nprint('Total no of Images: ',X_train.shape[0]) \nprint('Size of Image:', X_train.shape[1:])\nprint('Total no of labels:', y_train.shape)\n\nTotal no of Images:  60000\nSize of Image: (28, 28)\nTotal no of labels: (60000,)\n\n\n\n# Look input data\nnum = 10\nnum_row = 2\nnum_col = 5\nimages = X_train[:num]\nlabels = y_train[:num]\n\n# Ploting images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i//num_col, i%num_col]\n    ax.imshow(images[i], cmap='gray')\n    ax.set_title('Label: {}'.format(labels[i]))\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#prepare-input-data",
    "href": "blog/2023-09-04_mnist-classification/index.html#prepare-input-data",
    "title": "MNIST Classification",
    "section": "3 Prepare input data",
    "text": "3 Prepare input data\n\nX_train = X_train.reshape((X_train.shape[0],-1))\nX_test = X_test.reshape((X_test.shape[0], -1))\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nprint(X_train.shape, X_test.shape)\n\n(60000, 784) (10000, 784)\n\n\n\n# Normalize data\nX_train = X_train/255\nX_test = X_test/255\n\n# print(X_train[0])\nX_train.shape\n\n(60000, 784)\n\n\n\n# Perfom one encoding\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(y_train.shape)\n\n(60000, 10)\n\n\n\nnum_classes = y_test.shape[1]\nnum_pixels = 784"
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#defining-the-model",
    "href": "blog/2023-09-04_mnist-classification/index.html#defining-the-model",
    "title": "MNIST Classification",
    "section": "4 Defining the model",
    "text": "4 Defining the model\n\n# Define baseline model\n\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(256, input_dim=num_pixels, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    return model\n\n\n# Build the model\nmodel = baseline_model()\nmodel.summary()\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_6 (Dense)             (None, 256)               200960    \n                                                                 \n dense_7 (Dense)             (None, 64)                16448     \n                                                                 \n dense_8 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 218058 (851.79 KB)\nTrainable params: 218058 (851.79 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nopt = SGD(lr = 0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer= opt, metrics=['accuracy'])\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD."
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#train-model",
    "href": "blog/2023-09-04_mnist-classification/index.html#train-model",
    "title": "MNIST Classification",
    "section": "5 Train model",
    "text": "5 Train model\n\nmodel.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n\nEpoch 1/5\n1875/1875 [==============================] - 9s 4ms/step - loss: 0.6029 - accuracy: 0.8422\nEpoch 2/5\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.2849 - accuracy: 0.9181\nEpoch 3/5\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.2314 - accuracy: 0.9346\nEpoch 4/5\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1962 - accuracy: 0.9440\nEpoch 5/5\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1701 - accuracy: 0.9510\n\n\n&lt;keras.src.callbacks.History at 0x13e2b0b50&gt;"
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#test-model",
    "href": "blog/2023-09-04_mnist-classification/index.html#test-model",
    "title": "MNIST Classification",
    "section": "6 Test model",
    "text": "6 Test model\n\nscores = model.evaluate(X_test, y_test, verbose=1)\nprint(\"Error: %.2f%%\" % (100-scores[1]*100))\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.1672 - accuracy: 0.9516\nError: 4.84%"
  },
  {
    "objectID": "blog/2023-09-04_mnist-classification/index.html#predicting",
    "href": "blog/2023-09-04_mnist-classification/index.html#predicting",
    "title": "MNIST Classification",
    "section": "7 Predicting",
    "text": "7 Predicting\n\nimg_width, img_height = 28, 28\ngray_image = X_test[0]\nplt.imshow(gray_image,cmap='Greys')\nplt.show()\n# gray_image.shape\nx = np.expand_dims(gray_image, axis=0)\nx = x.reshape((1, -1))\n\n\n\n\n\npreds = model.predict(x)\nprob = np.argmax(preds, axis=1)\n\nprint('Predicted value is ', prob)\nprint('Probability across all numbers :', preds[0])\n\n1/1 [==============================] - 0s 30ms/step\nPredicted value is  [7]\nProbability across all numbers : [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html",
    "href": "blog/2023-11-20_modern-convnets/index.html",
    "title": "Modern ConvNets",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 20, 2023"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#lecture-overview",
    "href": "blog/2023-11-20_modern-convnets/index.html#lecture-overview",
    "title": "Modern ConvNets",
    "section": "1 Lecture overview",
    "text": "1 Lecture overview\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#understanding-deep-embeddings",
    "href": "blog/2023-11-20_modern-convnets/index.html#understanding-deep-embeddings",
    "title": "Modern ConvNets",
    "section": "2 Understanding deep embeddings",
    "text": "2 Understanding deep embeddings\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#the-deep-layers-will-gradually-learn-more-abstract-features.",
    "href": "blog/2023-11-20_modern-convnets/index.html#the-deep-layers-will-gradually-learn-more-abstract-features.",
    "title": "Modern ConvNets",
    "section": "3 The deep layers will gradually learn more abstract features.",
    "text": "3 The deep layers will gradually learn more abstract features.\n\n\n\n\nSlide 4\n\n\n\n\nThe things that are not important are guided by SGD"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#this-deep-lower-dimensional-space-learns-meaningful-structures",
    "href": "blog/2023-11-20_modern-convnets/index.html#this-deep-lower-dimensional-space-learns-meaningful-structures",
    "title": "Modern ConvNets",
    "section": "4 This deep, lower dimensional space learns meaningful structures",
    "text": "4 This deep, lower dimensional space learns meaningful structures\n\n\n\n\nSlide 5\n\n\n\n\nCall lower dimension manifold. RGB does not have meaningfull transformations"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#what-do-the-different-layers-in-a-deep-neural-netwok-learn",
    "href": "blog/2023-11-20_modern-convnets/index.html#what-do-the-different-layers-in-a-deep-neural-netwok-learn",
    "title": "Modern ConvNets",
    "section": "5 What do the different layers in a deep neural netwok learn",
    "text": "5 What do the different layers in a deep neural netwok learn\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#what-do-the-different-layers-in-a-deep-neural-network-learn",
    "href": "blog/2023-11-20_modern-convnets/index.html#what-do-the-different-layers-in-a-deep-neural-network-learn",
    "title": "Modern ConvNets",
    "section": "6 What do the different layers in a deep neural network learn",
    "text": "6 What do the different layers in a deep neural network learn\n\n\n\n\nSlide 7"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#how-do-these-layers-correspond-to-semantics-numerical-evaluation",
    "href": "blog/2023-11-20_modern-convnets/index.html#how-do-these-layers-correspond-to-semantics-numerical-evaluation",
    "title": "Modern ConvNets",
    "section": "7 How do these layers correspond to “semantics”: numerical evaluation",
    "text": "7 How do these layers correspond to “semantics”: numerical evaluation\n\n\n\n\nSlide 8\n\n\n\n\nWe Pooled them because the features are too big, we get smaller size inputs\n\n7.1 1. 2D Pooling (e.g., Max Pooling) in CNNs:\nConsider an input feature map with dimensions [batch_size, channels, height, width]:\n[32, 64, 32, 32]\n\nMax Pooling (2x2):\n\nApply a 2x2 max pooling operation, reducing height and width by half.\nResulting feature map: [32, 64, 16, 16].\n\n\nInput:           [32, 64, 32, 32]\nMax Pooling:     [32, 64, 16, 16]\n\n\n\n\nConvolution uses this formula remember:\n\n\n\n\n\nThis is AlexNet, Source\n\n\n\n\n\n\nRemember: the num of kernels for Convolution determines the new depth-dimension.\n\n\nWhat if the deepth was 3 channels, then we apply convolution to the 3 channels and then we summed them over. That is using only one kernel, now if we have 6 kernels we would sum the 3 channels, 6 times, but at the end we end up with a new deepth of 6. Check this\n\nFlattening Operation: multiply all its dimensions to form a 1D vector\n\nOutput: [batch_size, num_filters * reduced_height * reduced_width]\n\n\n\n\n\n\n\nHow to train a linear layer on top of a pretrained model?\n\n\n\n\n\nTraining a linear layer on top of a pretrained model is a common practice in transfer learning. Here’s an example using a pretrained convolutional neural network (CNN) as a feature extractor, and then adding a linear layer on top for a specific task, such as image classification.\nLet’s assume we have a pretrained ResNet18 model, and we want to use it for a new classification task. The final classification layer of ResNet18 is typically a linear layer. We will replace this final layer with our custom linear layer.\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load pretrained ResNet18\npretrained_resnet18 = models.resnet18(pretrained=True)\n\n# Freeze all layers except the final classification layer\nfor param in pretrained_resnet18.parameters():\n    param.requires_grad = False\n\n# Modify the final classification layer\nin_features = pretrained_resnet18.fc.in_features\nnum_classes = 10  # Assuming 10 classes for the new task\ncustom_linear_layer = nn.Linear(in_features, num_classes)\npretrained_resnet18.fc = custom_linear_layer\n\n# Now, you can train the modified model for your specific task\n# Let's assume you have input data of shape (batch_size, 3, 224, 224) for RGB images\n# and corresponding labels of shape (batch_size)\n\n# Example training loop\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(pretrained_resnet18.parameters(), lr=0.001)\n\n# Training iterations\nfor epoch in range(num_epochs):\n    for inputs, labels in dataloader:  # Assume you have a DataLoader for your dataset\n        optimizer.zero_grad()\n        outputs = pretrained_resnet18(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n# After training, you can use the modified model for predictions\nIn this example: - We load a pretrained ResNet18 model from torchvision. - We freeze all layers of the pretrained model to retain their weights during training. - We replace the final classification layer with our custom linear layer (custom_linear_layer). - We then train the modified model for the new task using a suitable loss function and optimizer.\nThe dimensions involved depend on your specific input data, but in this case, assuming RGB images of size 224x224, the input shape would be (batch_size, 3, 224, 224), and the linear layer would map to the number of classes in your new task.\n\n8 How does this linear layer looks like?\nIn the context of a linear layer in neural networks, such as nn.Linear(in_features, num_classes), here’s what the terms mean:\n\nin_features: This is the number of input features (or neurons) coming into the linear layer. In the example I provided earlier with the modified ResNet18 model, in_features is the number of features produced by the previous layer, which is the final layer of the feature extractor part of ResNet18. This value depends on the architecture of the pretrained model; you can check it using pretrained_resnet18.fc.in_features.\nnum_classes: This is the number of output features (or neurons) produced by the linear layer. In the context of a classification task, num_classes typically represents the number of classes you have in your specific classification problem. Each output neuron corresponds to a class, and the model will learn to assign higher values to the correct class during training.\n\nNow, for the number of weights and neurons:\n\nWeights: The linear layer has a weight matrix of size (out_features, in_features), and a bias vector of size (out_features). In this case, the weight matrix has dimensions (num_classes, in_features).\nTotal number of trainable weights = (num_classes * in_features) + num_classes.\nNeurons: The linear layer has num_classes output neurons. Each neuron receives input from all in_features neurons in the previous layer.\n\nSo, if you have a linear layer defined as nn.Linear(512, 10), for example:\n\nin_features is 512.\nnum_classes is 10.\n\nThe total number of trainable weights would be (10 * 512) + 10 = 5130, and there would be 10 neurons in the output layer. Each neuron in the output layer is associated with a specific class, and the weights determine how strongly each input feature contributes to the prediction for that class.\n\n8.1 Imaging it using Pytorch:\nMore at source:"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#how-does-this-linear-layer-looks-like",
    "href": "blog/2023-11-20_modern-convnets/index.html#how-does-this-linear-layer-looks-like",
    "title": "Modern ConvNets",
    "section": "8 How does this linear layer looks like?",
    "text": "8 How does this linear layer looks like?\nIn the context of a linear layer in neural networks, such as nn.Linear(in_features, num_classes), here’s what the terms mean:\n\nin_features: This is the number of input features (or neurons) coming into the linear layer. In the example I provided earlier with the modified ResNet18 model, in_features is the number of features produced by the previous layer, which is the final layer of the feature extractor part of ResNet18. This value depends on the architecture of the pretrained model; you can check it using pretrained_resnet18.fc.in_features.\nnum_classes: This is the number of output features (or neurons) produced by the linear layer. In the context of a classification task, num_classes typically represents the number of classes you have in your specific classification problem. Each output neuron corresponds to a class, and the model will learn to assign higher values to the correct class during training.\n\nNow, for the number of weights and neurons:\n\nWeights: The linear layer has a weight matrix of size (out_features, in_features), and a bias vector of size (out_features). In this case, the weight matrix has dimensions (num_classes, in_features).\nTotal number of trainable weights = (num_classes * in_features) + num_classes.\nNeurons: The linear layer has num_classes output neurons. Each neuron receives input from all in_features neurons in the previous layer.\n\nSo, if you have a linear layer defined as nn.Linear(512, 10), for example:\n\nin_features is 512.\nnum_classes is 10.\n\nThe total number of trainable weights would be (10 * 512) + 10 = 5130, and there would be 10 neurons in the output layer. Each neuron in the output layer is associated with a specific class, and the weights determine how strongly each input feature contributes to the prediction for that class.\n\n8.1 Imaging it using Pytorch:\nMore at source:"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#lets-wake-up-summarize-the-last-few-minutes-to-your-neighbor.",
    "href": "blog/2023-11-20_modern-convnets/index.html#lets-wake-up-summarize-the-last-few-minutes-to-your-neighbor.",
    "title": "Modern ConvNets",
    "section": "9 . Let’s wake up: Summarize the last few minutes to your neighbor. |",
    "text": "9 . Let’s wake up: Summarize the last few minutes to your neighbor. |\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#x1-convolution-a-computationally-cheap-method",
    "href": "blog/2023-11-20_modern-convnets/index.html#x1-convolution-a-computationally-cheap-method",
    "title": "Modern ConvNets",
    "section": "10 1x1 Convolution: a computationally cheap method",
    "text": "10 1x1 Convolution: a computationally cheap method\n\n\n\n\nSlide 10\n\n\n\n\n\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#convnet-configuration",
    "href": "blog/2023-11-20_modern-convnets/index.html#convnet-configuration",
    "title": "Modern ConvNets",
    "section": "11 ConvNet Configuration",
    "text": "11 ConvNet Configuration"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#vgg-16",
    "href": "blog/2023-11-20_modern-convnets/index.html#vgg-16",
    "title": "Modern ConvNets",
    "section": "12 VGG 16",
    "text": "12 VGG 16\n\n\n\n\n\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#characteristics",
    "href": "blog/2023-11-20_modern-convnets/index.html#characteristics",
    "title": "Modern ConvNets",
    "section": "13 Characteristics",
    "text": "13 Characteristics\n\n\n\n\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters",
    "href": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters",
    "title": "Modern ConvNets",
    "section": "14 Why 3x3 filters?",
    "text": "14 Why 3x3 filters?\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters-1",
    "title": "Modern ConvNets",
    "section": "15 Why 3x3 filters?",
    "text": "15 Why 3x3 filters?\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters-2",
    "title": "Modern ConvNets",
    "section": "16 Why 3x3 filters?",
    "text": "16 Why 3x3 filters?\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters-3",
    "href": "blog/2023-11-20_modern-convnets/index.html#why-3x3-filters-3",
    "title": "Modern ConvNets",
    "section": "17 Why 3x3 filters?",
    "text": "17 Why 3x3 filters?\n\n\n\n\nSlide 18\n\n\n\n\nHaving 3 filters of 3x3 –&gt; 7 is better because it can learn more non-linearities, more non-trivial functions, having large kernels is expensive, having small kernels but multiple of them is more cheaper"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#even-smaller-filters",
    "href": "blog/2023-11-20_modern-convnets/index.html#even-smaller-filters",
    "title": "Modern ConvNets",
    "section": "18 Even smaller filters?",
    "text": "18 Even smaller filters?\n\n\n\n\nSlide 19\n\n\n\n\nWhen we remove dimensions is because we are getting rid of unimportant features, i.e. background, we also call th 1x1 kernel a bottleneck"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#overall-shapes-and-sizes-when-inputting-a-224x224-image",
    "href": "blog/2023-11-20_modern-convnets/index.html#overall-shapes-and-sizes-when-inputting-a-224x224-image",
    "title": "Modern ConvNets",
    "section": "19 Overall shapes and sizes when inputting a 224x224 image:",
    "text": "19 Overall shapes and sizes when inputting a 224x224 image:\nVgg architecture\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#training",
    "href": "blog/2023-11-20_modern-convnets/index.html#training",
    "title": "Modern ConvNets",
    "section": "20 Training",
    "text": "20 Training\n\n\n\n\nSlide 21\n\n\n\n\nThey use dropout on the FC layers because they tend to overfit quite easily"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#feature-maps",
    "href": "blog/2023-11-20_modern-convnets/index.html#feature-maps",
    "title": "Modern ConvNets",
    "section": "21 Feature maps",
    "text": "21 Feature maps\n\n\n\n\nSlide 22\n\n\n\n\nSome neuros (each item in the row, in total 8) for some the neuron does not fire it because we i.e have the background.\n\nIn the first block we recognize edges\nLater stages. our dimensionality decreases, that’s why we get block structures"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#filters",
    "href": "blog/2023-11-20_modern-convnets/index.html#filters",
    "title": "Modern ConvNets",
    "section": "22 Filters",
    "text": "22 Filters\n\n\n\n\nSlide 23\n\n\n\n\n\nRemember: Train only the parameters of the linear classifier while keeping the parameters of the pre-trained model frozen. This is sometimes called “freezing” the pre-trained layers\n\nBecause the above its not understandable, so for that we can keep the network frozen and now have an incoming image that is parametrized. Now your input image is torch.nnParameter(3,224,224). And we run gradient descent on this input image.\nWith that we want to maximize the activation function for a particular filter, so the loss function is the activation negative value of this filter, and you do backpropagation in the incoming image.\nSo then you get for example for some filter, some edges in one direction. And for higuer level featuers you can see some slighly more complex patterns"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#class-outputs",
    "href": "blog/2023-11-20_modern-convnets/index.html#class-outputs",
    "title": "Modern ConvNets",
    "section": "23 Class Outputs",
    "text": "23 Class Outputs\n\n\n\n\nSlide 24\n\n\n\n\nHere you see that the neuron will fire up for instance if the incoming image is a rabit, then the first filter will fire up"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#another-architecture",
    "href": "blog/2023-11-20_modern-convnets/index.html#another-architecture",
    "title": "Modern ConvNets",
    "section": "24 Another Architecture",
    "text": "24 Another Architecture\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#basic-idea",
    "href": "blog/2023-11-20_modern-convnets/index.html#basic-idea",
    "title": "Modern ConvNets",
    "section": "25 Basic idea",
    "text": "25 Basic idea\n\n\n\n\nSlide 26"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#basic-idea-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#basic-idea-1",
    "title": "Modern ConvNets",
    "section": "26 Basic idea",
    "text": "26 Basic idea\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#inception-module",
    "href": "blog/2023-11-20_modern-convnets/index.html#inception-module",
    "title": "Modern ConvNets",
    "section": "27 Inception module",
    "text": "27 Inception module\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#inception-module-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#inception-module-1",
    "title": "Modern ConvNets",
    "section": "28 Inception module",
    "text": "28 Inception module\n\n\n\n\nSlide 29\n\n\n\n\nThey are expensive because we have this:\n\n\n\n\nSo more parameters to train hence expensive.\nHence we apply 1x1 intermediate convolutions to reduce the dimensionality and have less parameters to train"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#architecture",
    "href": "blog/2023-11-20_modern-convnets/index.html#architecture",
    "title": "Modern ConvNets",
    "section": "29 Architecture",
    "text": "29 Architecture\n\n\n\n\nSlide 30"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#architecture-the-inception-module",
    "href": "blog/2023-11-20_modern-convnets/index.html#architecture-the-inception-module",
    "title": "Modern ConvNets",
    "section": "30 Architecture: the “Inception” module",
    "text": "30 Architecture: the “Inception” module\n\n\n\n\nSlide 31\n\n\n\n\nThe green block is concatenation because spatially they have the same size, so you can stack them together"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#architecture-the-auxiliary-classifier-idea",
    "href": "blog/2023-11-20_modern-convnets/index.html#architecture-the-auxiliary-classifier-idea",
    "title": "Modern ConvNets",
    "section": "31 Architecture: the auxiliary classifier idea",
    "text": "31 Architecture: the auxiliary classifier idea\n\n\n\n\nSlide 32\n\n\n\n\nHere in this auxilliary classifier, they predict the classes. This gives you gradients even if you havent reach the end of the network"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#why-aux-classifiers-vanishing-gradients",
    "href": "blog/2023-11-20_modern-convnets/index.html#why-aux-classifiers-vanishing-gradients",
    "title": "Modern ConvNets",
    "section": "32 Why aux classifiers? Vanishing gradients",
    "text": "32 Why aux classifiers? Vanishing gradients\n\n\n\n\nSlide 33\n\n\n\n\nWe do this becaus otherwhise we end up with the vanishing problem so at one stage you can just use you aux classifier. If you get extremely small gradients then it is very slow to train"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#architecture-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#architecture-1",
    "title": "Modern ConvNets",
    "section": "33 Architecture",
    "text": "33 Architecture\n\n\n\n\nSlide 34\n\n\n\n\nAfter training you dont need them anymore you can trhow them away (for aux classifier)"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#inceptions-v2-v3-v4-.",
    "href": "blog/2023-11-20_modern-convnets/index.html#inceptions-v2-v3-v4-.",
    "title": "Modern ConvNets",
    "section": "34 Inceptions v2, v3, V4, ….",
    "text": "34 Inceptions v2, v3, V4, ….\n\n\n\n\nSlide 35\n\n\n\n\nThe first picture refers to this two 3x3 then making 5x5, this inclusion of filters make the computations less expensive while introduction non-linearitties to be learn"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#resnets",
    "href": "blog/2023-11-20_modern-convnets/index.html#resnets",
    "title": "Modern ConvNets",
    "section": "35 ResNets",
    "text": "35 ResNets\n\n\n\n\nSlide 36"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#alexnet-2012",
    "href": "blog/2023-11-20_modern-convnets/index.html#alexnet-2012",
    "title": "Modern ConvNets",
    "section": "36 AlexNet (2012)",
    "text": "36 AlexNet (2012)\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#evolution",
    "href": "blog/2023-11-20_modern-convnets/index.html#evolution",
    "title": "Modern ConvNets",
    "section": "37 Evolution",
    "text": "37 Evolution\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#title",
    "href": "blog/2023-11-20_modern-convnets/index.html#title",
    "title": "Modern ConvNets",
    "section": "38 Title",
    "text": "38 Title\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#evolution-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#evolution-1",
    "title": "Modern ConvNets",
    "section": "39 Evolution",
    "text": "39 Evolution\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#evolution-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#evolution-2",
    "title": "Modern ConvNets",
    "section": "40 Evolution",
    "text": "40 Evolution\n\n\n\n\nSlide 41"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#why-care-about-architectures-heres-why",
    "href": "blog/2023-11-20_modern-convnets/index.html#why-care-about-architectures-heres-why",
    "title": "Modern ConvNets",
    "section": "41 Why care about architectures… here’s why:",
    "text": "41 Why care about architectures… here’s why:\n\n\n\n\nSlide 42\n\n\n\n\nThey become more accurate because the parameters did not increase that much despite having more layers. This is because we interchange the 5x5 filter in convolution by i.e two 3x3 kernels"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#some-facts-about-resnets",
    "href": "blog/2023-11-20_modern-convnets/index.html#some-facts-about-resnets",
    "title": "Modern ConvNets",
    "section": "42 Some facts about ResNets",
    "text": "42 Some facts about ResNets\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#hypothesis",
    "href": "blog/2023-11-20_modern-convnets/index.html#hypothesis",
    "title": "Modern ConvNets",
    "section": "43 Hypothesis",
    "text": "43 Hypothesis\n\n\n\n\nSlide 44\n\n\n\n\nIf your problem only required, the depth of a CNNA, then B in terms of performance would be the same"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#hypothesis-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#hypothesis-1",
    "title": "Modern ConvNets",
    "section": "44 Hypothesis",
    "text": "44 Hypothesis\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#hypothesis-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#hypothesis-2",
    "title": "Modern ConvNets",
    "section": "45 Hypothesis",
    "text": "45 Hypothesis\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#however-when-trained-the-deeper-network-has-higher-training-error",
    "href": "blog/2023-11-20_modern-convnets/index.html#however-when-trained-the-deeper-network-has-higher-training-error",
    "title": "Modern ConvNets",
    "section": "46 However, when trained the deeper network has higher training error",
    "text": "46 However, when trained the deeper network has higher training error\n\n\n\n\nSlide 47\n\n\n\n\nHere th problem is that we say that the deeper CNN would yield the same ouput as the smaller architecture but in this case when looking at the error the larger one has more error. So what happen in next slide. It may be with regards to optimization because in theory it should be able to lear in because of more flexibility by the NN"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#testing-the-hypothesis",
    "href": "blog/2023-11-20_modern-convnets/index.html#testing-the-hypothesis",
    "title": "Modern ConvNets",
    "section": "47 Testing the hypothesis",
    "text": "47 Testing the hypothesis\n\n\n\n\nSlide 48\n\n\n\n\nHere the optimization is the problem, it is more harder to optimize this landscape"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#observation",
    "href": "blog/2023-11-20_modern-convnets/index.html#observation",
    "title": "Modern ConvNets",
    "section": "48 Observation",
    "text": "48 Observation\n\n\n\n\nSlide 49\n\n\n\n\nBecause you have many layers the training layers get lost, and model does not learn anymore"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#the-residual-idea-intuitively",
    "href": "blog/2023-11-20_modern-convnets/index.html#the-residual-idea-intuitively",
    "title": "Modern ConvNets",
    "section": "49 The “residual idea”, intuitively",
    "text": "49 The “residual idea”, intuitively\n\n\n\n\nSlide 50\n\n\n\n\nHere the intuition is that technically it would be possible, so why not we make it easy for the NN to learn this easy relationship, here is the residual idea.\nSo instead of learning how you map things instead lets learn how you change it. So a difference that we need to learn not the mapping. So then here we are making the NN to explicitly model the difference in mappings"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#the-residual-block",
    "href": "blog/2023-11-20_modern-convnets/index.html#the-residual-block",
    "title": "Modern ConvNets",
    "section": "50 The residual block",
    "text": "50 The residual block\n\n\n\n\nSlide 51\n\n\n\n\nWith these new connection it make the vanishing problem not to occurr because the input can pass to the next layers\nSo here if the dimensions do not matche we need to make them amtch because we are just saying x=f(x) so f() here should make things equal, because we said that they were the identity functions\nAdvantages and Disadvantages of Residual networks here:"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#no-degradation-anymore",
    "href": "blog/2023-11-20_modern-convnets/index.html#no-degradation-anymore",
    "title": "Modern ConvNets",
    "section": "51 No degradation anymore",
    "text": "51 No degradation anymore\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#resnet-breaks-records",
    "href": "blog/2023-11-20_modern-convnets/index.html#resnet-breaks-records",
    "title": "Modern ConvNets",
    "section": "52 ResNet breaks records",
    "text": "52 ResNet breaks records\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#resnet-variants-resnext",
    "href": "blog/2023-11-20_modern-convnets/index.html#resnet-variants-resnext",
    "title": "Modern ConvNets",
    "section": "53 ResNet variants & ResNeXt",
    "text": "53 ResNet variants & ResNeXt\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#some-observations",
    "href": "blog/2023-11-20_modern-convnets/index.html#some-observations",
    "title": "Modern ConvNets",
    "section": "54 Some observations",
    "text": "54 Some observations\n\n\n\n\nSlide 55\n\n\n\n\nResidual connections or identity shortcuts\n\n\n\n\n\n\nWhy do we use batchnormas and how does this relate so vanishing gradient problems?\n\n\n\n\n\nBatch Normalization (BatchNorm) is a technique used in neural networks to normalize the inputs of a layer, typically by subtracting the mean and dividing by the standard deviation of the batch. BatchNorm has several benefits and is commonly used for the following reasons:\n\nStabilizing and Accelerating Training:\n\nBatchNorm can help stabilize and accelerate the training of deep neural networks. It mitigates issues related to internal covariate shift, which is the change in the distribution of network activations due to parameter updates during training. By normalizing the inputs, it helps maintain a more consistent distribution of activations throughout the training process.\n\nRegularization:\n\nBatchNorm acts as a form of regularization by introducing noise during training. It adds a small amount of noise to the hidden unit activations, which has a similar effect to dropout, helping prevent overfitting.\n\nReducing Sensitivity to Initialization:\n\nBatchNorm reduces the sensitivity of neural networks to weight initialization. It allows the use of higher learning rates and makes the training less dependent on the choice of initial weights.\n\nAddressing Gradient Problems:\n\nDuring training, neural networks often encounter problems related to vanishing or exploding gradients. BatchNorm helps mitigate these issues by normalizing the inputs, which can prevent gradients from becoming too small or too large.\n\nEnabling Higher Learning Rates:\n\nBatchNorm enables the use of higher learning rates during training. This can lead to faster convergence and shorter training times.\n\nImproved Generalization:\n\nBatchNorm can improve the generalization performance of a model by providing a form of noise during training.\n\n\nBatch Normalization is a widely used technique that improves the stability, speed, and generalization of neural network training. It addresses various challenges associated with training deep networks, including gradient-related problems, and has become a standard component in many modern neural network architectures."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#quiz",
    "href": "blog/2023-11-20_modern-convnets/index.html#quiz",
    "title": "Modern ConvNets",
    "section": "55 Quiz",
    "text": "55 Quiz\nOn the right you see the..\n\n\n\n\nSlide 56\n\n\n\n\n\nFalse, batch + ReLu makes half of the value zero\nFalse, it is okay to write it like that\nSay conv2 has a dimensionality of 256, but the input has dimension of 128, then downsample function will donwsample to 128 to match the input dimensions. So that means the residual operations have always need to have the same channel\n\nExample in residuals you can do concat or add\n\nconcat: you end up with\n\n\nTensor A: Shape (ch, 3, 4)\nTensor B: Shape (ch, 2, 4)\nOut: Shape (ch, 2, 4)\n\n\nAdd, both tensors needs to have same shape\n\n\nTensor A: Shape (ch, 6, 4)\nTensor B: Shape (ch, 6, 4)\nOut: Shape (ch, 6, 4)\n\nSo here they ar saying that to do residual connections the channels needs to be of same dimensions thus why we need the donwsample function. Zero padding is only used for spatial dimensions"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#highwaynet-slightly-earlier-than-resnets-in-2015",
    "href": "blog/2023-11-20_modern-convnets/index.html#highwaynet-slightly-earlier-than-resnets-in-2015",
    "title": "Modern ConvNets",
    "section": "56 HighwayNet (slightly earlier than ResNets in 2015)",
    "text": "56 HighwayNet (slightly earlier than ResNets in 2015)\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#densenet",
    "href": "blog/2023-11-20_modern-convnets/index.html#densenet",
    "title": "Modern ConvNets",
    "section": "57 DenseNet",
    "text": "57 DenseNet\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#densenet-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#densenet-1",
    "title": "Modern ConvNets",
    "section": "58 DenseNet",
    "text": "58 DenseNet\n\n\n\n\nSlide 59\n\n\n\n\nBecause of the skip connections it also has benefits for optimization"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#densenet-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#densenet-2",
    "title": "Modern ConvNets",
    "section": "59 DenseNet",
    "text": "59 DenseNet\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#densenets",
    "href": "blog/2023-11-20_modern-convnets/index.html#densenets",
    "title": "Modern ConvNets",
    "section": "60 DenseNets",
    "text": "60 DenseNets\n\n\n\n\nSlide 61"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#trend-has-not-stopped-with-densenet",
    "href": "blog/2023-11-20_modern-convnets/index.html#trend-has-not-stopped-with-densenet",
    "title": "Modern ConvNets",
    "section": "61 Trend has not stopped with DenseNet",
    "text": "61 Trend has not stopped with DenseNet\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#mobilenets-depthwise-convolutions-for-high-latency",
    "href": "blog/2023-11-20_modern-convnets/index.html#mobilenets-depthwise-convolutions-for-high-latency",
    "title": "Modern ConvNets",
    "section": "62 MobileNets: Depthwise convolutions for high latency",
    "text": "62 MobileNets: Depthwise convolutions for high latency\n\n\n\n\nSlide 63\n\n\n\n\nDeepthwise it only looks at one ch, so the filer is kxkx1\nPointwise then mixes channels\n\n\n\n\n\n\nHow does conv 1x1 can change channels?\n\n\n\n\n\nA 1x1 convolutional layer is often used for channel-wise transformations because it operates independently on each pixel across the spatial dimensions but can modify the depth (number of channels) of the input tensor. Here’s an example with tensors to illustrate this concept:\nLet’s consider an input tensor with dimensions [batch_size, height, width, channels], where: - batch_size is the number of samples in the batch, - height and width are the spatial dimensions of the feature map, and - channels is the number of channels (or features) at each spatial location.\nNow, let’s apply a 1x1 convolutional layer with, say, 3 output channels. The operation is channel-wise, meaning it independently transforms each channel without considering information from other channels. However, it changes the number of channels.\nInput Tensor: [batch_size, height, width, channels_in]\n\n1x1 Convolutional Layer (3 output channels):\n\nOutput Tensor: [batch_size, height, width, 3]\nIn this example, the 1x1 convolutional layer has transformed the input tensor by performing a linear operation on each channel independently. The resulting tensor now has 3 output channels. This operation is useful for adjusting the channel dimensions while keeping the spatial dimensions intact.\nIt’s computationally efficient because it involves fewer parameters compared to larger convolutional kernels, and it introduces non-linearity through activation functions applied to each channel independently.\nHere’s how the channel-wise transformation works without changing the spatial context. Suppose we have the following input tensor:\nInput Tensor: [batch_size, height, width, 5]\nAfter applying a 1x1 convolutional layer with 3 output channels, the output tensor would be:\nOutput Tensor: [batch_size, height, width, 3]\nEach channel in the output tensor is a linear combination of the corresponding channels in the input tensor, and non-linearity is introduced through activation functions applied independently to each channel."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#last-architecture-bagnet-solving-imagenet-with-tiny-9x9-sized-puzzle-pieces",
    "href": "blog/2023-11-20_modern-convnets/index.html#last-architecture-bagnet-solving-imagenet-with-tiny-9x9-sized-puzzle-pieces",
    "title": "Modern ConvNets",
    "section": "63 Last Architecture BagNet: Solving ImageNet with tiny 9x9 sized puzzle pieces?",
    "text": "63 Last Architecture BagNet: Solving ImageNet with tiny 9x9 sized puzzle pieces?\n\n\n\n\nSlide 64\n\n\n\n\nhaving only 1x1 conv means that the receptive field does not grow"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#imagenet-mostly-textures",
    "href": "blog/2023-11-20_modern-convnets/index.html#imagenet-mostly-textures",
    "title": "Modern ConvNets",
    "section": "64 ImageNet: mostly textures?",
    "text": "64 ImageNet: mostly textures?\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#how-research-gets-done-part-5",
    "href": "blog/2023-11-20_modern-convnets/index.html#how-research-gets-done-part-5",
    "title": "Modern ConvNets",
    "section": "65 How research gets done part 5",
    "text": "65 How research gets done part 5\nIsamu Akasaki: “As Thomas Edison said, ‘Genius is one percent inspiration and 99 perspiration.’ | say this to younger\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#object-detection",
    "href": "blog/2023-11-20_modern-convnets/index.html#object-detection",
    "title": "Modern ConvNets",
    "section": "66 Object detection",
    "text": "66 Object detection\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#region-based-convolutional-neural-network-r-cnn",
    "href": "blog/2023-11-20_modern-convnets/index.html#region-based-convolutional-neural-network-r-cnn",
    "title": "Modern ConvNets",
    "section": "67 Region-based Convolutional Neural Network (R-CNN)",
    "text": "67 Region-based Convolutional Neural Network (R-CNN)\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#r-cnn",
    "href": "blog/2023-11-20_modern-convnets/index.html#r-cnn",
    "title": "Modern ConvNets",
    "section": "68 R-CNN",
    "text": "68 R-CNN\n\n\n\n\nSlide 69\n\n\n\n\nTo all the 2k regions boxes we apply CNN, and 2k times the model needs to say which for each of these 2k boses what appears on the image, eg, a car, a plane etc."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#improving-the-bounding-boxes",
    "href": "blog/2023-11-20_modern-convnets/index.html#improving-the-bounding-boxes",
    "title": "Modern ConvNets",
    "section": "69 Improving the Bounding Boxes",
    "text": "69 Improving the Bounding Boxes\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#to-summarize",
    "href": "blog/2023-11-20_modern-convnets/index.html#to-summarize",
    "title": "Modern ConvNets",
    "section": "70 To summarize",
    "text": "70 To summarize\n\n\n\n\nSlide 71"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#r-cnn-is-really-quite-slow-for-a-few-simple-reasons",
    "href": "blog/2023-11-20_modern-convnets/index.html#r-cnn-is-really-quite-slow-for-a-few-simple-reasons",
    "title": "Modern ConvNets",
    "section": "71 R-CNN is really quite slow for a few simple reasons:",
    "text": "71 R-CNN is really quite slow for a few simple reasons:\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#some-results",
    "href": "blog/2023-11-20_modern-convnets/index.html#some-results",
    "title": "Modern ConvNets",
    "section": "72 Some results",
    "text": "72 Some results\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn",
    "title": "Modern ConvNets",
    "section": "73 Fast R-CNN",
    "text": "73 Fast R-CNN\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-insight-1-region-of-interest-pooling-roipool",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-insight-1-region-of-interest-pooling-roipool",
    "title": "Modern ConvNets",
    "section": "74 Fast R-CNN Insight 1: Region of Interest Pooling (ROIPool)",
    "text": "74 Fast R-CNN Insight 1: Region of Interest Pooling (ROIPool)\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool",
    "href": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool",
    "title": "Modern ConvNets",
    "section": "75 Region of Interest Pooling (ROIPool)",
    "text": "75 Region of Interest Pooling (ROIPool)\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool-1",
    "title": "Modern ConvNets",
    "section": "76 Region of Interest Pooling (ROIPool)",
    "text": "76 Region of Interest Pooling (ROIPool)\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool-2",
    "title": "Modern ConvNets",
    "section": "77 Region of Interest Pooling (ROIPool)",
    "text": "77 Region of Interest Pooling (ROIPool)\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool-3",
    "href": "blog/2023-11-20_modern-convnets/index.html#region-of-interest-pooling-roipool-3",
    "title": "Modern ConvNets",
    "section": "78 Region of Interest Pooling (ROIPool)",
    "text": "78 Region of Interest Pooling (ROIPool)\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-insight-2-combine-all-models-into-one-network",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-insight-2-combine-all-models-into-one-network",
    "title": "Modern ConvNets",
    "section": "79 Fast R-CNN Insight 2: Combine All Models into One Network",
    "text": "79 Fast R-CNN Insight 2: Combine All Models into One Network\n\n\n\n\nSlide 80"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-joint-training-framework",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-joint-training-framework",
    "title": "Modern ConvNets",
    "section": "80 Fast R-CNN: Joint training framework",
    "text": "80 Fast R-CNN: Joint training framework\n\n\n\n\nSlide 81"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps",
    "title": "Modern ConvNets",
    "section": "81 Fast R-CNN: Steps",
    "text": "81 Fast R-CNN: Steps\n\n\n\n\nSlide 82"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-1",
    "title": "Modern ConvNets",
    "section": "82 Fast R-CNN: Steps",
    "text": "82 Fast R-CNN: Steps\n\n\n\n\nSlide 83"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-2",
    "title": "Modern ConvNets",
    "section": "83 Fast R-CNN: Steps",
    "text": "83 Fast R-CNN: Steps\n\n\n\n\nSlide 84"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-3",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-3",
    "title": "Modern ConvNets",
    "section": "84 Fast R-CNN: Steps",
    "text": "84 Fast R-CNN: Steps\n\n\n\n\nSlide 85"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-4",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-r-cnn-steps-4",
    "title": "Modern ConvNets",
    "section": "85 Fast R-CNN: Steps",
    "text": "85 Fast R-CNN: Steps\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#smart-training",
    "href": "blog/2023-11-20_modern-convnets/index.html#smart-training",
    "title": "Modern ConvNets",
    "section": "86 Smart training",
    "text": "86 Smart training\n\n\n\n\nSlide 87"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#some-results-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#some-results-1",
    "title": "Modern ConvNets",
    "section": "87 Some results",
    "text": "87 Some results\n\n\n\n\nSlide 88"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#fast-rcnn",
    "href": "blog/2023-11-20_modern-convnets/index.html#fast-rcnn",
    "title": "Modern ConvNets",
    "section": "88 Fast-RCNN",
    "text": "88 Fast-RCNN\n\n\n\n\nSlide 89"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn---speeding-up-region-proposal",
    "href": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn---speeding-up-region-proposal",
    "title": "Modern ConvNets",
    "section": "89 Faster R-CNN - Speeding Up Region Proposal",
    "text": "89 Faster R-CNN - Speeding Up Region Proposal\n\n\n\n\nSlide 90"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn",
    "href": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn",
    "title": "Modern ConvNets",
    "section": "90 Faster R-CNN",
    "text": "90 Faster R-CNN\n\n\n\n\nSlide 91"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn-1",
    "title": "Modern ConvNets",
    "section": "91 Faster R-CNN",
    "text": "91 Faster R-CNN\n\n\n\n\nSlide 92"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn-girshick2016",
    "href": "blog/2023-11-20_modern-convnets/index.html#faster-r-cnn-girshick2016",
    "title": "Modern ConvNets",
    "section": "92 Faster R-CNN [Girshick2016]",
    "text": "92 Faster R-CNN [Girshick2016]\n\n\n\n\nSlide 93"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#mask-r-cnn",
    "href": "blog/2023-11-20_modern-convnets/index.html#mask-r-cnn",
    "title": "Modern ConvNets",
    "section": "93 Mask R-CNN",
    "text": "93 Mask R-CNN\n\n\n\n\nSlide 94"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#mask-r-cnn-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#mask-r-cnn-1",
    "title": "Modern ConvNets",
    "section": "94 Mask R-CNN",
    "text": "94 Mask R-CNN\n\n\n\n\nSlide 95"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#mask-r-cnn-2",
    "href": "blog/2023-11-20_modern-convnets/index.html#mask-r-cnn-2",
    "title": "Modern ConvNets",
    "section": "95 Mask R-CNN",
    "text": "95 Mask R-CNN\n\n\n\n\nSlide 96"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#roialign---realigning-roipool-to-be-more-accurate",
    "href": "blog/2023-11-20_modern-convnets/index.html#roialign---realigning-roipool-to-be-more-accurate",
    "title": "Modern ConvNets",
    "section": "96 RoIAlign - Realigning RoIPool to be More Accurate",
    "text": "96 RoIAlign - Realigning RoIPool to be More Accurate\n\n\n\n\nSlide 97"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#roialign---realigning-roipool-to-be-more-accurate-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#roialign---realigning-roipool-to-be-more-accurate-1",
    "title": "Modern ConvNets",
    "section": "97 RoIAlign - Realigning RoIPool to be More Accurate",
    "text": "97 RoIAlign - Realigning RoIPool to be More Accurate\n\n\n\n\nSlide 98"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#section",
    "href": "blog/2023-11-20_modern-convnets/index.html#section",
    "title": "Modern ConvNets",
    "section": "98 99",
    "text": "98 99\n\n\n\n\n\nSlide 99"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#becoming-fully-convolutional",
    "href": "blog/2023-11-20_modern-convnets/index.html#becoming-fully-convolutional",
    "title": "Modern ConvNets",
    "section": "99 Becoming fully convolutional",
    "text": "99 Becoming fully convolutional\n\n\n\n\nSlide 100\n\n\n\n\n\n\n\n\n\n\nTurning a fully connected layer into a 1x1 convolutional layer\n\n\n\n\n\nThis involves reshaping the weight matrix of the fully connected layer to match the dimensions of a convolutional kernel. Let’s consider an example with tensor dimensions.\nSuppose you have a fully connected layer with input size C_in and output size C_out. The weight matrix of this fully connected layer is of shape (C_out, C_in).\nNow, to turn it into a 1x1 convolutional layer, you can reshape the weight matrix into the shape (C_out, C_in, 1, 1). The resulting operation is equivalent to a 1x1 convolution with C_in input channels and C_out output channels.\nHere’s an example:\nFully Connected Layer:\nInput: [batch_size, C_in]\nWeights: [C_out, C_in]\n\nReshaped Weights for 1x1 Convolution:\nWeights: [C_out, C_in, 1, 1]\n\nInput Tensor for 1x1 Convolution:\nInput: [batch_size, C_in, 1, 1]\n\nOutput Tensor for 1x1 Convolution:\nOutput: [batch_size, C_out, 1, 1]\nIn this example, the reshaped weights effectively create a 1x1 convolutional kernel that operates on each channel independently and produces an output tensor with the specified number of channels.\nThis transformation allows you to apply convolutional operations even in scenarios where the spatial dimensions are reduced to 1x1. It’s particularly useful in the context of neural network architectures, where convolutional layers are preferred for their ability to capture spatial hierarchies.\nWhat is the input dimensionality of a 1x1 conv? The input dimensionality of a 1x1 convolutional layer is typically three-dimensional. The dimensions correspond to:\n\nBatch Size (B): The number of samples in a mini-batch.\nNumber of Input Channels (C_in): The depth or number of channels in the input feature map.\nSpatial Dimension (H x W): Although a 1x1 convolution operates on a spatial dimension, it often involves 1x1 spatial dimensions (height and width). This is different from traditional convolutions that operate on larger spatial dimensions.\n\nSo, the input tensor shape for a 1x1 convolutional layer is often represented as [B, C_in, 1, 1], where B is the batch size, C_in is the number of input channels, and the spatial dimensions are 1x1.\n1:48 check in the video\nThey are not the same the 1x1 conv and the FC layer but they share the same weights and same meaning of the weight\nHow would a max pooling reduces spatial dimension to 1?\nMax pooling reduces spatial dimensions by selecting the maximum value within each pooling window. The pooling window slides over the input data, and for each window, only the maximum value is retained in the pooled output. This process effectively downsamples the input.\nLet’s consider an example with a 1D input tensor of size 6 and a max pooling operation with a window size of 2. Here’s the input tensor:\n\\(\\text{Input Tensor: } [1, 3, 5, 2, 8, 6]\\)\nApplying max pooling with a window size of 2 reduces the spatial dimension by selecting the maximum value in each window:\n\\(\\text{Max Pooled Output: } [3, 5, 8]\\)\nIn this example, the original input had 6 elements, and after max pooling, the output has 3 elements, effectively reducing the spatial dimension. The reduction factor depends on the size of the pooling window and the stride (the step size at which the window moves).\nYou take the weights of a FC layer and you input them into a 1x1 convolution layer or you just apply the FC layer at every location\n\n100 What is the input dimensionality of a one by 1x1 convolution?\nThe input dimensionality of a 1x1 convolutional layer is typically three-dimensional. The dimensions correspond to:\n\nBatch Size (B): The number of samples in a mini-batch.\nNumber of Input Channels (C_in): The depth or number of channels in the input feature map.\nSpatial Dimension (H x W): Although a 1x1 convolution operates on a spatial dimension, it often involves 1x1 spatial dimensions (height and width). This is different from traditional convolutions that operate on larger spatial dimensions.\n\nSo, the input tensor shape for a 1x1 convolutional layer is often represented as [B, C_in, 1, 1], where B is the batch size, C_in is the number of input channels, and the spatial dimensions are 1x1.\n\n\n101 What is the input dimensionality of a FC layer?\nFor a fully connected layer (dense layer) with \\(N\\) input neurons and \\(M\\) output neurons, the number of weights is given by:\n\\(W = N \\times M\\)\nSo, for a fully connected layer, the number of weights depends on the number of input and output neurons.\nComparing this with the 1x1 convolutional layer discussed earlier:\n\nFor a 1x1 convolution with \\(C_{\\text{in}}\\) input channels and \\(C_{\\text{out}}\\) output channels, the number of weights is \\(C_{\\text{in}} \\times C_{\\text{out}}\\).\n\nIn general, the number of weights for a fully connected layer is not necessarily the same as that for a 1x1 convolutional layer, as it depends on the specific architecture and dimensions of the layers involved. The key difference lies in how the connections are structured in each type of layer.\n\n\n102 1x1 conv & FC similarity\n\nYou take the weights of a FC layer and you input them into a 1x1 convolution layer or you just apply the FC layer at every location\n\nThe statement refers to a conceptual similarity between a fully connected (FC) layer and a 1x1 convolutional layer in terms of their weight organization.\n\nFC Layer: In a traditional fully connected layer, all neurons are connected to every element in the input. If the input has dimensions \\(N \\times M\\), where \\(N\\) is the batch size and \\(M\\) is the number of input features, the FC layer has \\(M\\) weights per neuron.\n1x1 Convolutional Layer: A 1x1 convolutional layer, despite being convolutional, can be thought of as a fully connected layer applied at every spatial location independently. It has a kernel size of \\(1 \\times 1\\), meaning it considers only the individual elements at each location. The weights in this case are shared across all spatial locations but applied independently to each location.\n\nSo, conceptually, you can take the weights of a FC layer and use them in a 1x1 convolutional layer. This is based on the idea that a 1x1 convolution can capture the essence of a fully connected layer when applied independently across spatial dimensions.\nHere’s a simplified example to illustrate:\n\nFC Layer: \\(M\\) weights per neuron, where \\(M\\) is the number of input features.\n1x1 Conv Layer: \\(M\\) shared weights applied independently at each spatial location.\n\nThis conceptual equivalence is often used in practice, especially in neural network architectures that leverage convolutional layers for spatial hierarchies and fully connected layers for global relationships.\nMore into 1x1 conv similarity with FC\n\nConvolutional Nature: A 1x1 convolutional layer is a convolutional layer, which means it applies a set of filters (kernels) to the input data. In traditional convolution, these filters scan through local regions of the input, capturing spatial patterns.\nKernel Size: The term “1x1” refers to the size of the filters. A 1x1 convolutional layer uses filters that are 1x1 in size. This means the filter considers only one element at a time during convolution.\nFully Connected Analogy: Despite being a convolutional layer, a 1x1 convolutional layer can be conceptually thought of as a fully connected layer. In a fully connected layer, each neuron is connected to every element in the input. Similarly, a 1x1 convolutional layer can be viewed as having a filter that is as wide and tall as the input, effectively connecting each element in the input to the corresponding neuron in the output.\nSpatial Independence: The key distinction is that, unlike a traditional fully connected layer, the weights in a 1x1 convolutional layer are shared across all spatial locations. This means the same set of weights is used at every position in the input. However, these shared weights are applied independently to each location, capturing local patterns.\n\nTherefore, a 1x1 convolutional layer behaves like a fully connected layer applied independently at each spatial location, using shared weights for efficiency. This provides a way to introduce non-linearity and channel-wise transformations without the need for a fully connected layer, especially in the context of convolutional neural networks (CNNs)."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#what-is-the-input-dimensionality-of-a-one-by-1x1-convolution",
    "href": "blog/2023-11-20_modern-convnets/index.html#what-is-the-input-dimensionality-of-a-one-by-1x1-convolution",
    "title": "Modern ConvNets",
    "section": "100 What is the input dimensionality of a one by 1x1 convolution?",
    "text": "100 What is the input dimensionality of a one by 1x1 convolution?\nThe input dimensionality of a 1x1 convolutional layer is typically three-dimensional. The dimensions correspond to:\n\nBatch Size (B): The number of samples in a mini-batch.\nNumber of Input Channels (C_in): The depth or number of channels in the input feature map.\nSpatial Dimension (H x W): Although a 1x1 convolution operates on a spatial dimension, it often involves 1x1 spatial dimensions (height and width). This is different from traditional convolutions that operate on larger spatial dimensions.\n\nSo, the input tensor shape for a 1x1 convolutional layer is often represented as [B, C_in, 1, 1], where B is the batch size, C_in is the number of input channels, and the spatial dimensions are 1x1."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#what-is-the-input-dimensionality-of-a-fc-layer",
    "href": "blog/2023-11-20_modern-convnets/index.html#what-is-the-input-dimensionality-of-a-fc-layer",
    "title": "Modern ConvNets",
    "section": "101 What is the input dimensionality of a FC layer?",
    "text": "101 What is the input dimensionality of a FC layer?\nFor a fully connected layer (dense layer) with \\(N\\) input neurons and \\(M\\) output neurons, the number of weights is given by:\n\\(W = N \\times M\\)\nSo, for a fully connected layer, the number of weights depends on the number of input and output neurons.\nComparing this with the 1x1 convolutional layer discussed earlier:\n\nFor a 1x1 convolution with \\(C_{\\text{in}}\\) input channels and \\(C_{\\text{out}}\\) output channels, the number of weights is \\(C_{\\text{in}} \\times C_{\\text{out}}\\).\n\nIn general, the number of weights for a fully connected layer is not necessarily the same as that for a 1x1 convolutional layer, as it depends on the specific architecture and dimensions of the layers involved. The key difference lies in how the connections are structured in each type of layer."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#x1-conv-fc-similarity",
    "href": "blog/2023-11-20_modern-convnets/index.html#x1-conv-fc-similarity",
    "title": "Modern ConvNets",
    "section": "102 1x1 conv & FC similarity",
    "text": "102 1x1 conv & FC similarity\n\nYou take the weights of a FC layer and you input them into a 1x1 convolution layer or you just apply the FC layer at every location\n\nThe statement refers to a conceptual similarity between a fully connected (FC) layer and a 1x1 convolutional layer in terms of their weight organization.\n\nFC Layer: In a traditional fully connected layer, all neurons are connected to every element in the input. If the input has dimensions \\(N \\times M\\), where \\(N\\) is the batch size and \\(M\\) is the number of input features, the FC layer has \\(M\\) weights per neuron.\n1x1 Convolutional Layer: A 1x1 convolutional layer, despite being convolutional, can be thought of as a fully connected layer applied at every spatial location independently. It has a kernel size of \\(1 \\times 1\\), meaning it considers only the individual elements at each location. The weights in this case are shared across all spatial locations but applied independently to each location.\n\nSo, conceptually, you can take the weights of a FC layer and use them in a 1x1 convolutional layer. This is based on the idea that a 1x1 convolution can capture the essence of a fully connected layer when applied independently across spatial dimensions.\nHere’s a simplified example to illustrate:\n\nFC Layer: \\(M\\) weights per neuron, where \\(M\\) is the number of input features.\n1x1 Conv Layer: \\(M\\) shared weights applied independently at each spatial location.\n\nThis conceptual equivalence is often used in practice, especially in neural network architectures that leverage convolutional layers for spatial hierarchies and fully connected layers for global relationships.\nMore into 1x1 conv similarity with FC\n\nConvolutional Nature: A 1x1 convolutional layer is a convolutional layer, which means it applies a set of filters (kernels) to the input data. In traditional convolution, these filters scan through local regions of the input, capturing spatial patterns.\nKernel Size: The term “1x1” refers to the size of the filters. A 1x1 convolutional layer uses filters that are 1x1 in size. This means the filter considers only one element at a time during convolution.\nFully Connected Analogy: Despite being a convolutional layer, a 1x1 convolutional layer can be conceptually thought of as a fully connected layer. In a fully connected layer, each neuron is connected to every element in the input. Similarly, a 1x1 convolutional layer can be viewed as having a filter that is as wide and tall as the input, effectively connecting each element in the input to the corresponding neuron in the output.\nSpatial Independence: The key distinction is that, unlike a traditional fully connected layer, the weights in a 1x1 convolutional layer are shared across all spatial locations. This means the same set of weights is used at every position in the input. However, these shared weights are applied independently to each location, capturing local patterns.\n\nTherefore, a 1x1 convolutional layer behaves like a fully connected layer applied independently at each spatial location, using shared weights for efficiency. This provides a way to introduce non-linearity and channel-wise transformations without the need for a fully connected layer, especially in the context of convolutional neural networks (CNNs)."
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#upsampling-the-output",
    "href": "blog/2023-11-20_modern-convnets/index.html#upsampling-the-output",
    "title": "Modern ConvNets",
    "section": "103 Upsampling the output",
    "text": "103 Upsampling the output\n\n\n\n\nSlide 101"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#deconvolution",
    "href": "blog/2023-11-20_modern-convnets/index.html#deconvolution",
    "title": "Modern ConvNets",
    "section": "104 “Deconvolution”",
    "text": "104 “Deconvolution”\n\n\n\n\nSlide 102"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#end-to-end-pixels-to-pixels-network",
    "href": "blog/2023-11-20_modern-convnets/index.html#end-to-end-pixels-to-pixels-network",
    "title": "Modern ConvNets",
    "section": "105 End-to-end, pixels-to-pixels network",
    "text": "105 End-to-end, pixels-to-pixels network\n\n\n\n\nSlide 103"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#end-to-end-pixels-to-pixels-network-1",
    "href": "blog/2023-11-20_modern-convnets/index.html#end-to-end-pixels-to-pixels-network-1",
    "title": "Modern ConvNets",
    "section": "106 End-to-end, pixels-to-pixels network",
    "text": "106 End-to-end, pixels-to-pixels network\n\n\n\n\nSlide 104"
  },
  {
    "objectID": "blog/2023-11-20_modern-convnets/index.html#references",
    "href": "blog/2023-11-20_modern-convnets/index.html#references",
    "title": "Modern ConvNets",
    "section": "107 References",
    "text": "107 References\n\n\n\n\nSlide 105"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html",
    "title": "Lexical semantics and word embeddings",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 22, 2023"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#outline.",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#outline.",
    "title": "Lexical semantics and word embeddings",
    "section": "1 Outline.",
    "text": "1 Outline.\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#semantics",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#semantics",
    "title": "Lexical semantics and word embeddings",
    "section": "2 Semantics",
    "text": "2 Semantics\n\n\n\n\nSlide 3\n\n\n\n\nSemantics is concerned with modelling meaning\n\nCompositional semantics: meaning of phrases\nLexical semantics: meaning of individuals words"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#what-is-lexical-meaning",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#what-is-lexical-meaning",
    "title": "Lexical semantics and word embeddings",
    "section": "3 What is lexical meaning?",
    "text": "3 What is lexical meaning?\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#how-to-approach-lexical-meaning",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#how-to-approach-lexical-meaning",
    "title": "Lexical semantics and word embeddings",
    "section": "4 How to approach lexical meaning?",
    "text": "4 How to approach lexical meaning?\n\n\n\n\nSlide 5\n\n\n\n\nIn formal semantics: The meaning of words are represented as sets\ni.e Bachelors -&gt; if is a man and also unmmaried"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#how-to-approach-lexical-meaning-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#how-to-approach-lexical-meaning-1",
    "title": "Lexical semantics and word embeddings",
    "section": "5 How to approach lexical meaning?",
    "text": "5 How to approach lexical meaning?\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#prototype-theory",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#prototype-theory",
    "title": "Lexical semantics and word embeddings",
    "section": "6 Prototype theory",
    "text": "6 Prototype theory\n\n\n\n\nSlide 7\n\n\n\n\nPrototype theory: concepts are represents as a graded category, it is like a human categorization\n\nnot all members needs to share a property\n\ni.e furniture -&gt; chair is more central (prototypical) than stool or a couch"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#semantic-relations",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#semantic-relations",
    "title": "Lexical semantics and word embeddings",
    "section": "7 Semantic relations",
    "text": "7 Semantic relations\n\n\n\n\nSlide 8\n\n\n\n\nTaxonomy refers to the science of classification, specifically the classification of living organisms into various categories based on shared characteristics"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#other-semantic-relations",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#other-semantic-relations",
    "title": "Lexical semantics and word embeddings",
    "section": "8 Other semantic relations",
    "text": "8 Other semantic relations\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#polysemy-and-word-senses",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#polysemy-and-word-senses",
    "title": "Lexical semantics and word embeddings",
    "section": "9 Polysemy and word senses",
    "text": "9 Polysemy and word senses\n\n\n\n\nSlide 10\n\n\n\n\nPolysemy is the ability of a word to have multiple meanings\nA word can mean different things in a different context"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#polysemy",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#polysemy",
    "title": "Lexical semantics and word embeddings",
    "section": "10 Polysemy",
    "text": "10 Polysemy\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#outline.-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#outline.-1",
    "title": "Lexical semantics and word embeddings",
    "section": "11 Outline.",
    "text": "11 Outline.\n\n\n\n\nSlide 12\n\n\n\n\nThis a modeling framework"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis",
    "title": "Lexical semantics and word embeddings",
    "section": "12 Distributional hypothesis",
    "text": "12 Distributional hypothesis\n\n\n\n\nSlide 13\n\n\n\n\nWe can analysis the word depending of how it is used in a large corpus\nA corpus (plural: corpora) refers to a large and structured set of texts,"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-1",
    "title": "Lexical semantics and word embeddings",
    "section": "13 Distributional hypothesis",
    "text": "13 Distributional hypothesis\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-2",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-2",
    "title": "Lexical semantics and word embeddings",
    "section": "14 Distributional hypothesis",
    "text": "14 Distributional hypothesis\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-3",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-3",
    "title": "Lexical semantics and word embeddings",
    "section": "15 Distributional hypothesis",
    "text": "15 Distributional hypothesis\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-4",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-4",
    "title": "Lexical semantics and word embeddings",
    "section": "16 Distributional hypothesis",
    "text": "16 Distributional hypothesis\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#scrumpy",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#scrumpy",
    "title": "Lexical semantics and word embeddings",
    "section": "17 Scrumpy",
    "text": "17 Scrumpy\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-5",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-hypothesis-5",
    "title": "Lexical semantics and word embeddings",
    "section": "18 Distributional hypothesis",
    "text": "18 Distributional hypothesis\n\n\n\n\nSlide 19\n\n\n\n\nThe context about a word provides the information about its meaning\nMeaning similarity, could be then be the vector that is also similar to another where each element of the vector is a context, so the more similar context the closer the meaning"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#the-general-intuition",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#the-general-intuition",
    "title": "Lexical semantics and word embeddings",
    "section": "19 The general intuition",
    "text": "19 The general intuition\n\n\n\n\nSlide 20\n\n\n\n\neach word is a point: a row\nDimensions: are all possible contexts in our dataset\nthe values are the frequencies that ocurred in that context"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#vectors",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#vectors",
    "title": "Lexical semantics and word embeddings",
    "section": "20 Vectors",
    "text": "20 Vectors\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#the-notion-of-context",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#the-notion-of-context",
    "title": "Lexical semantics and word embeddings",
    "section": "21 The notion of context",
    "text": "21 The notion of context\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context",
    "title": "Lexical semantics and word embeddings",
    "section": "22 Context",
    "text": "22 Context\n\n\n\n\nSlide 23\n\n\n\n\nHere we delte words that are frequent\nHere the window stays the same"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context-1",
    "title": "Lexical semantics and word embeddings",
    "section": "23 Context",
    "text": "23 Context\n\n\n\n\nSlide 24\n\n\n\n\nNow we just take the stem of each word and then we do a count on this stem words; here acknowledged -&gt; acknowledge\nThis is handy if your corpus is not very large, because then your vectors would be very sparce meaning we would have zero entries because of the different variants of a word, so instead you want to aggregate context that mean the same so then we fix sparse vectors"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context-2",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context-2",
    "title": "Lexical semantics and word embeddings",
    "section": "24 Context",
    "text": "24 Context\n\n\n\n\nSlide 25\n\n\n\n\nInstead of word window, we can use syntatic relations. That is we can extract syntactic context even for those words that are further way but have some relation with the wording that we are looking."
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context-weighting",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#context-weighting",
    "title": "Lexical semantics and word embeddings",
    "section": "25 Context weighting",
    "text": "25 Context weighting\n\n\n\n\nSlide 26\n\n\n\n\nThe first decision: how to model\nThe second: how to weight the context, so which metric we wil use"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#characteristic-model",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#characteristic-model",
    "title": "Lexical semantics and word embeddings",
    "section": "26 Characteristic model",
    "text": "26 Characteristic model\n\n\n\n\nSlide 27\n\n\n\n\nHere we are saying that there are some words that are more characteristic of a context. For i.e ‘floffy’ can be refered to dogs or cats, or toys but not so much of ‘computer’, ‘screwdriver’\nIt measures the joint probability of the word and the context (numerator) and the probability of them occurring together if they were independent (denominator)\nP(c|w): how likely it is the context given that we are seeing this word.\nSo we want to compute the probability of the word occurance in the corpus to their probability of occurrence independently"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#what-semantic-space",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#what-semantic-space",
    "title": "Lexical semantics and word embeddings",
    "section": "27 What semantic space?",
    "text": "27 What semantic space?\n\n\n\n\nSlide 28\n\n\n\n\nThe first decision: how to model\nThe second: how to weight the context, so which metric we wil use\nThird design decision: what kind of semantic space to use? aka how many context to include\n\nWe can use entire vocabulary\nA CON is that it will be very sparse if we use the whole vocab"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#word-frequency-zipfian-distribution",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#word-frequency-zipfian-distribution",
    "title": "Lexical semantics and word embeddings",
    "section": "28 Word frequency: Zipfian distribution",
    "text": "28 Word frequency: Zipfian distribution\n\n\n\n\nSlide 29"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#what-semantic-space-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#what-semantic-space-1",
    "title": "Lexical semantics and word embeddings",
    "section": "29 What semantic space?",
    "text": "29 What semantic space?\n\n\n\n\nSlide 30\n\n\n\n\nDimensionality reduction is to benefit from approach 1 and 2"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#an-example-noun",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#an-example-noun",
    "title": "Lexical semantics and word embeddings",
    "section": "30 An example noun",
    "text": "30 An example noun\n\n\n\n\nSlide 31\n\n\n\n\nthe values here are PMI values"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#an-example-adjective",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#an-example-adjective",
    "title": "Lexical semantics and word embeddings",
    "section": "31 An example adjective",
    "text": "31 An example adjective\n\n\n\n\nSlide 32\n\n\n\n\nDecathlon is strange to see in the first place\nPMI property: no matter which data you apply to is that you would get unreseanable PMI values for rare events.\nSo Decathlon is rare, and if it appears with academic once, then it will have a high PMI, because has a low prior probability"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#polysemy-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#polysemy-1",
    "title": "Lexical semantics and word embeddings",
    "section": "32 Polysemy",
    "text": "32 Polysemy\n\n\n\n\nSlide 33\n\n\n\n\nPolysemy is the ability of a word to have multiple meanings\nAll these senses are encoded and collapse toguether within a single distribution. Basically you have different context that correspond to different meanings and"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#calculating-similarity-in-a-distributional-space",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#calculating-similarity-in-a-distributional-space",
    "title": "Lexical semantics and word embeddings",
    "section": "33 Calculating similarity in a distributional space",
    "text": "33 Calculating similarity in a distributional space\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#measuring-similarity",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#measuring-similarity",
    "title": "Lexical semantics and word embeddings",
    "section": "34 Measuring similarity",
    "text": "34 Measuring similarity\n\n\n\n\nSlide 35\n\n\n\n\nThe dot product of the two vectors that is normalized by the vector lenght\nThe Euclidean distance considers the length of the vectors:\n\n\n\n\nThe euclidean distance would be quite large, so that is why we need to normalize"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#the-scale-of-similarity-some-examples",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#the-scale-of-similarity-some-examples",
    "title": "Lexical semantics and word embeddings",
    "section": "35 The scale of similarity: some examples",
    "text": "35 The scale of similarity: some examples\n\n\n\n\nSlide 36"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#words-most-similar-to-cat",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#words-most-similar-to-cat",
    "title": "Lexical semantics and word embeddings",
    "section": "36 Words most similar to cat",
    "text": "36 Words most similar to cat\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#but-what-is-similarity",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#but-what-is-similarity",
    "title": "Lexical semantics and word embeddings",
    "section": "37 But what is similarity?",
    "text": "37 But what is similarity?\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-methods-are-a-usage-representation",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-methods-are-a-usage-representation",
    "title": "Lexical semantics and word embeddings",
    "section": "38 Distributional methods are a usage representation",
    "text": "38 Distributional methods are a usage representation\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distribution-for-policeman",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distribution-for-policeman",
    "title": "Lexical semantics and word embeddings",
    "section": "39 Distribution for policeman",
    "text": "39 Distribution for policeman\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distribution-for-cop",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distribution-for-cop",
    "title": "Lexical semantics and word embeddings",
    "section": "40 Distribution for cop",
    "text": "40 Distribution for cop\n\n\n\n\nSlide 41\n\n\n\n\nCop and policeman, even though the words seems to be the same, there is cultural associations with cop that is highly negative.\nThis means that words have relative meanings to the culture and thus if compared this terms between cultures their meanings could be totally different which then evaluating a similarity metric will yield that they are not similar.\nTha being, we take two words from the same corpus but because of cultural use of words they may have different distributions (cognotations). Unquestionable, this is a property of the data.\n1set carriage bike vehicle train truck lorry coach taxi – official officer inspector journalist detective constable policeman reporter – sister daughter parent relative lover cousin friend wife mother husband brother father\n2set car engine petrol road driver wheel trip steering seat fo, highway sign speed - concert singer stage light music show audience performance ticket - experiment research scientist paper result publication laboratory finding"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#clustering-nouns",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#clustering-nouns",
    "title": "Lexical semantics and word embeddings",
    "section": "41 Clustering nouns",
    "text": "41 Clustering nouns\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#clustering-nouns-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#clustering-nouns-1",
    "title": "Lexical semantics and word embeddings",
    "section": "42 Clustering nouns",
    "text": "42 Clustering nouns\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#outline.-2",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#outline.-2",
    "title": "Lexical semantics and word embeddings",
    "section": "43 Outline.",
    "text": "43 Outline.\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-semantic-models",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#distributional-semantic-models",
    "title": "Lexical semantics and word embeddings",
    "section": "44 Distributional semantic models",
    "text": "44 Distributional semantic models\n\n\n\n\nSlide 45\n\n\n\n\nDense vectors or word embeddings\nCount-based models we can see clearly\nHere we train the model to predict what makes a good context for a word.\nDense in the sense that the dimensionals are used and there are fewer dimensions, the model learns some interactions between them. However the dimensions are latent so if the model has a strange behaviour is very hard to know why."
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#sparse-vs.-dense-vectors",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#sparse-vs.-dense-vectors",
    "title": "Lexical semantics and word embeddings",
    "section": "45 Sparse vs. dense vectors",
    "text": "45 Sparse vs. dense vectors\n\n\n\n\nSlide 46\n\n\n\n\nIn traditional distribution models we can have tends of thousands of contexts in a large corpus\nthey generalize better, by having those latent dimensions that are trained in the prediction task, the model learns to map similar context toguether to the same dimensions.\nIf in a traditional distribution model you would have distints concepts for car and automobile, there is no way for the model to know that they provide the same information, whereas in a dense vector we can agregate over similar context, you can map them to the same dimension and you can reduce the redundancy in the data but also end up with a model that is more generalizable"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#prediction-based-distributional-models",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#prediction-based-distributional-models",
    "title": "Lexical semantics and word embeddings",
    "section": "46 Prediction-based distributional models",
    "text": "46 Prediction-based distributional models\n\n\n\n\nSlide 47\n\n\n\n\nThe most popular model of word embedding of prediction base is the Skip-gram model\nProbabilistic language models, such as n-gram models where our goal is given a sequence of words we want to predict the next word that comes next\nHere, the task is the same exceot that we use a NN to perform this prediction.\nThe idea is that we can learn word representations in the process"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram",
    "title": "Lexical semantics and word embeddings",
    "section": "47 Skip-gram",
    "text": "47 Skip-gram\n\n\n\n\nSlide 48\n\n\n\n\nin Skip-gram we do not care about the sequence, but rather we want to take individual words as input, and we want to output the words that it can occur in the data i.e. if we take a 5-word window, basically we want to train the model to predict the valid context for thee word and then we lear the world representation in the process (in the projection layer)"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-1",
    "title": "Lexical semantics and word embeddings",
    "section": "48 Skip-gram",
    "text": "48 Skip-gram\n\n\n\n\nSlide 49\n\n\n\n\nWords that occur toguether in the data should have some similarity in meaning. This is different from i.e we compare context towards the current and the relation in their similar meaning but rather a word should be similar in meaning to its neighbors.\nEssentially we want to compare the vectors of the word and their neighbors\nGiven word at time t\nGoal: predict all its neighboring words within a window. For instance we use a 5-word windows then these are the words to predict"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-parameter-matrices",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-parameter-matrices",
    "title": "Lexical semantics and word embeddings",
    "section": "49 Skip-gram: Parameter matrices",
    "text": "49 Skip-gram: Parameter matrices\n\n\n\n\nSlide 50\n\n\n\n\nFor each word skip gram learns 2 parameter vectors:\nFor each word it learns two vectors:\n\nThe word vector, that is the word embedding v, in word matrix W\nThis is the vector that represents the meaning of that word\nA context vector that is the vector that represents its behavior as context for other words\n\nIn a sense each word can have two rows: it can act as a word from which we are learning the word meaning AND it can also act as context for other words\nW: is the matrix of word embeddings\nC: the matrix of context embeddings\nand so then we choose the dimensionality of our embeddings to have, for instance between 50 and 500 dimensions\nHere for a whole Vocabulary we have a vector for every word, so the dimensionality here is the size of our vocabulary, so the columns are the word embeddings for all of our words\nAgain:\nW:\nSo in the columns we have the vector oer word so we go from 1..Vw\nThe rows represent the dimensionality of the vector\nC:\nThe columns are the dimensions\nThe rows we have the size of our vocabulary so we have a word, so the number of rows are the number of word in the vocabulary\nIn practice, these are the same, so we say we use individual words as context, so all words will learn embeddings, and each word can acts as a context. However there is no requirement to be the same, in fact the context vocabulary can be different and we can use any definition of context as we discuss in distributional models"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-setup",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-setup",
    "title": "Lexical semantics and word embeddings",
    "section": "50 Skip-gram: Setup",
    "text": "50 Skip-gram: Setup\n\n\n\n\nSlide 51\n\n\n\n\nwe go word by word extracting its context.\nThis may look like Bigram prob but is not, and is not because position does not matter. So basically we consider each context in isolation like a set, so the context that the word might have and we operate over context pairs, the order does not matter in the sequence. So word and each context in isolation. And which context are possible we define based on the word window for any other criteria like we explain before like:\n\nContext window unfilter\nContext window filtered\nContext window lexeme filtered\nConnected by a grammatical relation\n\nSo the position of the context does not matter once we have extracted the context but of course it does matter in the process of extracting it, so it has to be whithin the word window\nIntuition of skip-gram: to compute that probability we need to compute the similarity between w_j and w_k"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-computing-similarity",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-computing-similarity",
    "title": "Lexical semantics and word embeddings",
    "section": "51 Skip-gram: Computing similarity",
    "text": "51 Skip-gram: Computing similarity\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-similarity-as-dot-product",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-similarity-as-dot-product",
    "title": "Lexical semantics and word embeddings",
    "section": "52 Skip-gram: Similarity as dot product",
    "text": "52 Skip-gram: Similarity as dot product\n\n\n\n\nSlide 53\n\n\n\n\nWhy do we use dot product to compute the similarity. Again we use cosine, here we used it because it does not matter about the mangnitude but if they are pointing in the same direction then they are considered similar.\nSo vectors that are similar in this case w_j (current word) and w_k (word to predict) will be similar if they have a high dot product"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-compute-probabilities",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-compute-probabilities",
    "title": "Lexical semantics and word embeddings",
    "section": "53 Skip-gram: Compute probabilities",
    "text": "53 Skip-gram: Compute probabilities\n\n\n\n\nSlide 54\n\n\n\n\ncase w_j (current word) and w_k (word to predict) v_j (current word) and c_k (word to predict)\nSo here P(w_k|w_j) we want to predict how likely is that this context is predicted.\nWe end up with having a vector of dot products, so we are comparing the word with all of the context vectors, that gives a vector of products over the whole vocabulary and then we normalize it using softmax\nAt the end we use softmax to make probabilities summ up to one"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-learning",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-learning",
    "title": "Lexical semantics and word embeddings",
    "section": "54 Skip-gram: Learning",
    "text": "54 Skip-gram: Learning\n\n\n\n\nSlide 55\n\n\n\n\nwe iterative update the embeddings to make the embeddings of our words more similar to the real seen context words and less similar to the embeddings of everything else (other context words)"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-objective",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-objective",
    "title": "Lexical semantics and word embeddings",
    "section": "55 Skip-gram: Objective",
    "text": "55 Skip-gram: Objective\n\n\n\n\nSlide 56\n\n\n\n\nSo we want to maximize the overall corpus probability, that is the probability of the real seen context.\nHere we assume that the context is independent of each other, so the whole sequence does not matter. We assume that the probabilities are independent of each other and so basically the objective becomes to maximize the product of the probabilities of the real seen context pairs\nWhere:\n\nc_k is the vector representation of the context word w_k\nv_j is the vector representation of the word w_j"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#visualising-skip-gram-as-a-network",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#visualising-skip-gram-as-a-network",
    "title": "Lexical semantics and word embeddings",
    "section": "56 Visualising skip-gram as a network",
    "text": "56 Visualising skip-gram as a network\n\n\n\n\nSlide 57\n\n\n\n\nour input here is the vector representation of the word\nThe last layer tell us all the possible contexts.\nSo we are predicting a context word at a time. For ie. here w_t is a vector and we predict all the probabilities of context words"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#one-hot-vectors",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#one-hot-vectors",
    "title": "Lexical semantics and word embeddings",
    "section": "57 One hot vectors",
    "text": "57 One hot vectors\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#visualising-skip-gram-as-a-network-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#visualising-skip-gram-as-a-network-1",
    "title": "Lexical semantics and word embeddings",
    "section": "58 Visualising skip-gram as a network",
    "text": "58 Visualising skip-gram as a network\n\n\n\n\nSlide 59\n\n\n\n\nThe problem arises at computing softmax. If our corpus is large then the denominator will go over this large text thus being computationally expensive. So basically we have to iterate over all the vocabulary to perform the update"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling",
    "title": "Lexical semantics and word embeddings",
    "section": "59 Skip-gram with negative sampling",
    "text": "59 Skip-gram with negative sampling\n\n\n\n\nSlide 60\n\n\n\n\nInstead of iterating over the whole vocabulary, for each word and positive context pair we will now sum over noise or negative samples i.e words that are not context to that word. In the above formula this translates in suming ‘i’ number of negative or noisy samples in the denominator therefore reducing the amount of computations."
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-1",
    "title": "Lexical semantics and word embeddings",
    "section": "60 Skip-gram with negative sampling",
    "text": "60 Skip-gram with negative sampling\n\n\n\n\nSlide 61\n\n\n\n\nFor each word of the window: so for word pair (‘tablespppon’, ‘apricot’), we are gonna randomly sample some negative examples, so sampling other possible contexts. You can decide how many you want to have per possitve context i.e 2 or 10. Say we are going to choose randomly two noise examples i.e ‘cement’ and ‘iddle’, those are words that have nothing to do with ‘apricot’ so those are the negative examples.\nHow to choose negative examples? it is just random and it is done over the whole vocabulary but you could be more especific and i.e pick from their unigram probability with more frequent words\nSo here if we take 2 negative examples per positive we will have 8 different pairs because our windows is 4"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-training-examples",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-training-examples",
    "title": "Lexical semantics and word embeddings",
    "section": "61 Skip-gram with negative sampling: Training examples",
    "text": "61 Skip-gram with negative sampling: Training examples\n\n\n\n\nSlide 62\n\n\n\n\nSo basically we will convert our dataset into word pairs"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-2",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-2",
    "title": "Lexical semantics and word embeddings",
    "section": "62 Skip-gram with negative sampling",
    "text": "62 Skip-gram with negative sampling\n\n\n\n\nSlide 63\n\n\n\n\nSo now for a given pair we are going to predict if it is a negative or positive example, basically we converted into a binary clarification problem\nSo basically we will have a probability that a pair (w_j: context, w_k: example) is a positive example and then the probability that is a negative example."
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-3",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-3",
    "title": "Lexical semantics and word embeddings",
    "section": "63 Skip-gram with negative sampling",
    "text": "63 Skip-gram with negative sampling\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-objective",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#skip-gram-with-negative-sampling-objective",
    "title": "Lexical semantics and word embeddings",
    "section": "64 Skip-gram with negative sampling: Objective",
    "text": "64 Skip-gram with negative sampling: Objective\n\n\n\n\nSlide 68\n\n\n\n\nSo now the intuition is that we want to make the word vector like the context vector and unlike the context vector of the negative examples.\nSo here we want to maximize the probability of positive examples being positive and maximize the probability of negative examples being negative\n\n64.1 Savings from negative Skip-gram. Softmax –&gt; Sigmoid\n\n\n\n\nEdit:\nTo sum up we see from the last equation that instead of iterating over the whole vocabulary, now we will maximize over the positive and negative sets which their size is a design choice that as explained during the lectures is more computational efficient (less dot products to compute) as iterating over the whole vocabulary as it was the case with Softamx."
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings",
    "title": "Lexical semantics and word embeddings",
    "section": "65 Properties of embeddings",
    "text": "65 Properties of embeddings\n\n\n\n\nSlide 69\n\n\n\n\nThe number below France is the ranking of most similar words. The interesting thing is that if you do not use lexematitation, it also captures more find grained aspects of meaning. For instance in ‘Reddish’ we do not get other colors but we also get a red-like similar words is not about ish words because we also get silvery. This show us that the model cna capture fine grained aspects, which is what we want."
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings-1",
    "title": "Lexical semantics and word embeddings",
    "section": "66 Properties of embeddings",
    "text": "66 Properties of embeddings\n\n\n\n\nSlide 71\n\n\n\n\nWe compare two pairs of words in terms of their relation\n\nhere in apples in the plural relation\nhere in man and woman there is a gender relation\n\nSo the model needs to complete the analogy given a is b and then given c is to what?"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings-2",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings-2",
    "title": "Lexical semantics and word embeddings",
    "section": "67 Properties of embeddings",
    "text": "67 Properties of embeddings\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings-3",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#properties-of-embeddings-3",
    "title": "Lexical semantics and word embeddings",
    "section": "68 Properties of embeddings",
    "text": "68 Properties of embeddings\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#word-embeddings-in-practice",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#word-embeddings-in-practice",
    "title": "Lexical semantics and word embeddings",
    "section": "69 Word embeddings in practice",
    "text": "69 Word embeddings in practice\n\n\n\n\nSlide 74\n\n\n\n\nIn your NN the first layer are going to be the word embeddings which are some representations of the word"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#count-based-models-vs.-skip-gram-word-embeddings",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#count-based-models-vs.-skip-gram-word-embeddings",
    "title": "Lexical semantics and word embeddings",
    "section": "70 Count-based models vs. skip-gram word embeddings",
    "text": "70 Count-based models vs. skip-gram word embeddings\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#count-based-models-vs.-skip-gram-word-embeddings-1",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#count-based-models-vs.-skip-gram-word-embeddings-1",
    "title": "Lexical semantics and word embeddings",
    "section": "71 Count-based models vs. skip-gram word embeddings",
    "text": "71 Count-based models vs. skip-gram word embeddings\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#acknowledgement",
    "href": "blog/2023-11-22_lexical-semantics-and-word-embeddings/index.html#acknowledgement",
    "title": "Lexical semantics and word embeddings",
    "section": "72 Acknowledgement",
    "text": "72 Acknowledgement\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html",
    "title": "Compositional semantics and sentence representations",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 22, 2023"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#compositional-semantics",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#compositional-semantics",
    "title": "Compositional semantics and sentence representations",
    "section": "1 Compositional semantics",
    "text": "1 Compositional semantics\n\n\n\n\nSlide 3\n\n\n\n\nDeep here we mean as deeper understanding of language no NN"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#compositional-semantics-alongside-syntax",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#compositional-semantics-alongside-syntax",
    "title": "Compositional semantics and sentence representations",
    "section": "2 Compositional semantics alongside syntax",
    "text": "2 Compositional semantics alongside syntax\n\n\n\n\nSlide 4\n\n\n\n\nIf we want to model semantics alongside syntax. Word meaning then phrase meaning, then sentence meaning."
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#non-trivial-issues-with-semantic-composition",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#non-trivial-issues-with-semantic-composition",
    "title": "Compositional semantics and sentence representations",
    "section": "3 Non-trivial issues with semantic composition",
    "text": "3 Non-trivial issues with semantic composition\n\n\n\n\nSlide 5\n\n\n\n\nHere in the first i, it refers maybe to a dog. The second one it does not refer to anything. So even if they have same syntax structure they have different meanings\nThe problem with last one is that even though these phrases can mean soomthing unqiue like the second refere to a person has passed away. Sometimes we may also express that a person just kick the bucket"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#non-trivial-issues-with-semantic-composition-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#non-trivial-issues-with-semantic-composition-1",
    "title": "Compositional semantics and sentence representations",
    "section": "4 Non-trivial issues with semantic composition",
    "text": "4 Non-trivial issues with semantic composition\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#issues-with-semantic-composition",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#issues-with-semantic-composition",
    "title": "Compositional semantics and sentence representations",
    "section": "5 Issues with semantic composition",
    "text": "5 Issues with semantic composition\n\n\n\n\nSlide 7\n\n\n\n\nThis represent recursion"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#modelling-compositional-semantics",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#modelling-compositional-semantics",
    "title": "Compositional semantics and sentence representations",
    "section": "6 Modelling compositional semantics",
    "text": "6 Modelling compositional semantics\n\n\n\n\nSlide 8\n\n\n\n\nThese are two modelling frameworks\n\nHere we do the composition directly in vector space\n\nUnsupervised methods, they are general purpose. They capture the meaning of a word based on the similarity with other words.\n\nHere you train your representation in a supervised way, which means you need a task to get the learning signal from. For example in sentiment classification.\n\nFor instance if you train your representations to sentiment analysis and then you want to do translation this will not work, cause you train on a different task\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#compositional-distributional-semantics",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#compositional-distributional-semantics",
    "title": "Compositional semantics and sentence representations",
    "section": "7 Compositional distributional semantics",
    "text": "7 Compositional distributional semantics\nThese are the all general purpose unsupervised way\n\n\n\n\nSlide 10\n\n\n\n\nThe idea come up, we were successful to create word representations, why not phrases, then why not in the sentence level.\nIf you have a finite vocab you can still create an infinitely amount of sentences\nIt is unfeseable because you dont have every possible sentence there and create a sentence representation for that. But we can do somthing similar which is, instead of learning sentence representatins directly you would try to use the word representation for the sentences and composed to create a sentence representatio.\nIn principle you need only word representations for all words which is more doable than getting word representations for all sentences"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#vector-mixture-models",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#vector-mixture-models",
    "title": "Compositional semantics and sentence representations",
    "section": "8 Vector mixture models",
    "text": "8 Vector mixture models\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#additive-and-multiplicative-models",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#additive-and-multiplicative-models",
    "title": "Compositional semantics and sentence representations",
    "section": "9 Additive and multiplicative models",
    "text": "9 Additive and multiplicative models\n\n\n\n\nSlide 12\n\n\n\n\nBecause summation is symmetric representation, we get the same representation so this model has a flaw\nPrepositions are used flexibly in any position, which means they dont have a strong behavioral profile but the content words they do. For example they appear in the same position they co-ocurr in similar context"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lexical-function-models",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lexical-function-models",
    "title": "Compositional semantics and sentence representations",
    "section": "10 Lexical function models",
    "text": "10 Lexical function models\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lexical-function-models-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lexical-function-models-1",
    "title": "Compositional semantics and sentence representations",
    "section": "11 Lexical function models",
    "text": "11 Lexical function models\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#learning-adjective-matrices",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#learning-adjective-matrices",
    "title": "Compositional semantics and sentence representations",
    "section": "12 Learning adjective matrices",
    "text": "12 Learning adjective matrices\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#learning-adjective-matrices-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#learning-adjective-matrices-1",
    "title": "Compositional semantics and sentence representations",
    "section": "13 Learning adjective matrices",
    "text": "13 Learning adjective matrices\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title",
    "title": "Compositional semantics and sentence representations",
    "section": "14 Title",
    "text": "14 Title\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-1",
    "title": "Compositional semantics and sentence representations",
    "section": "15 Title",
    "text": "15 Title\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-2",
    "title": "Compositional semantics and sentence representations",
    "section": "16 Title",
    "text": "16 Title\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#task-sentiment-classification-of-movie-reviews",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#task-sentiment-classification-of-movie-reviews",
    "title": "Compositional semantics and sentence representations",
    "section": "17 Task: Sentiment classification of movie reviews",
    "text": "17 Task: Sentiment classification of movie reviews\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#words-and-sentences-into-vectors",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#words-and-sentences-into-vectors",
    "title": "Compositional semantics and sentence representations",
    "section": "18 Words (and sentences) into vectors",
    "text": "18 Words (and sentences) into vectors\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#sentence-representation-a-very-simplified-picture",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#sentence-representation-a-very-simplified-picture",
    "title": "Compositional semantics and sentence representations",
    "section": "19 Sentence representation: A (very) simplified picture",
    "text": "19 Sentence representation: A (very) simplified picture\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-3",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-3",
    "title": "Compositional semantics and sentence representations",
    "section": "20 Title",
    "text": "20 Title\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#dataset-stanford-sentiment-treebank-sst",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#dataset-stanford-sentiment-treebank-sst",
    "title": "Compositional semantics and sentence representations",
    "section": "21 Dataset: Stanford Sentiment Treebank (SST)",
    "text": "21 Dataset: Stanford Sentiment Treebank (SST)\n\n\n\n\nSlide 24"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#binary-parse-tree-one-example",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#binary-parse-tree-one-example",
    "title": "Compositional semantics and sentence representations",
    "section": "22 Binary parse tree: One example",
    "text": "22 Binary parse tree: One example\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-4",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-4",
    "title": "Compositional semantics and sentence representations",
    "section": "23 Title",
    "text": "23 Title\n\n\n\n\nSlide 26"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#models",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#models",
    "title": "Compositional semantics and sentence representations",
    "section": "24 Models",
    "text": "24 Models\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#first-approach-sentence-sentiment",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#first-approach-sentence-sentiment",
    "title": "Compositional semantics and sentence representations",
    "section": "25 First approach: Sentence + Sentiment",
    "text": "25 First approach: Sentence + Sentiment\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-5",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-5",
    "title": "Compositional semantics and sentence representations",
    "section": "26 Title",
    "text": "26 Title\n\n\n\n\nSlide 29"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-6",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-6",
    "title": "Compositional semantics and sentence representations",
    "section": "27 Title",
    "text": "27 Title\n\n\n\n\nSlide 30\n\n\n\n\nHere we do not model order or syntax"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#bag-of-words",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#bag-of-words",
    "title": "Compositional semantics and sentence representations",
    "section": "28 Bag of Words",
    "text": "28 Bag of Words\n\n\n\n\nSlide 31"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#bag-of-words-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#bag-of-words-1",
    "title": "Compositional semantics and sentence representations",
    "section": "29 Bag of Words",
    "text": "29 Bag of Words\n\n\n\n\nSlide 32\n\n\n\n\nBecause you don’t consider order the example in the sentece is the same, so there is a flaw in this model"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#turning-words-into-numbers",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#turning-words-into-numbers",
    "title": "Compositional semantics and sentence representations",
    "section": "30 Turning words into numbers",
    "text": "30 Turning words into numbers\n\n\n\n\nSlide 33"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#one-hot-vectors-select-word-embeddings",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#one-hot-vectors-select-word-embeddings",
    "title": "Compositional semantics and sentence representations",
    "section": "31 One-hot vectors select word embeddings",
    "text": "31 One-hot vectors select word embeddings\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-7",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-7",
    "title": "Compositional semantics and sentence representations",
    "section": "32 Title",
    "text": "32 Title\n\n\n\n\nSlide 35"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-8",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-8",
    "title": "Compositional semantics and sentence representations",
    "section": "33 Title",
    "text": "33 Title\n\n\n\n\nSlide 36\n\n\n\n\nNow because the vector representation can squezze more detail in the embedding about the word, then this increase in dimensionality is better"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#continuous-bag-of-words-cbow",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#continuous-bag-of-words-cbow",
    "title": "Compositional semantics and sentence representations",
    "section": "34 Continuous Bag of Words (CBOW)",
    "text": "34 Continuous Bag of Words (CBOW)\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recall-matrix-multiplication",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recall-matrix-multiplication",
    "title": "Compositional semantics and sentence representations",
    "section": "35 Recall: Matrix Multiplication",
    "text": "35 Recall: Matrix Multiplication\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this",
    "title": "Compositional semantics and sentence representations",
    "section": "36 What about this?",
    "text": "36 What about this?\n\n\n\n\nSlide 39\n\n\n\n\nHere the problem of just concatenating is that you dont know the size of the W to multiply because the sentence embeddings are vary in length"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this-1",
    "title": "Compositional semantics and sentence representations",
    "section": "37 What about this?",
    "text": "37 What about this?\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-9",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-9",
    "title": "Compositional semantics and sentence representations",
    "section": "38 Title",
    "text": "38 Title\n\n\n\n\nSlide 41"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-10",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-10",
    "title": "Compositional semantics and sentence representations",
    "section": "39 Title",
    "text": "39 Title\n\n\n\n\nSlide 42\n\n\n\n\nHere we just learn more layers, so more complexity to the model\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this-2",
    "title": "Compositional semantics and sentence representations",
    "section": "40 What about this?",
    "text": "40 What about this?\n\n\n\n\nSlide 44\n\n\n\n\nDeeper is not always better because we might start to overfit, and at test time it will not generalize well"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#question",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#question",
    "title": "Compositional semantics and sentence representations",
    "section": "41 Question",
    "text": "41 Question\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-11",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-11",
    "title": "Compositional semantics and sentence representations",
    "section": "42 Title",
    "text": "42 Title\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#deep-cbow-with-pretrained-embeddings",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#deep-cbow-with-pretrained-embeddings",
    "title": "Compositional semantics and sentence representations",
    "section": "43 Deep CBOW with pretrained embeddings",
    "text": "43 Deep CBOW with pretrained embeddings\n\n\n\n\nSlide 47\n\n\n\n\nIt will be easier if the models already know the word meaning and then it can get the sentoment fo the sentece. Here we have a prior which is our word representation"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-12",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-12",
    "title": "Compositional semantics and sentence representations",
    "section": "44 Title",
    "text": "44 Title\n\n\n\n\nSlide 48\n\n\n\n\nThere is two paradigms by using pre-trained embeddings\n\nYou can keep these representations frozen: so not for training\nor you fine-tune the word representations toguether with your task. This means the word represenation becomes more specialized for the task"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recap-training-a-neural-network",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recap-training-a-neural-network",
    "title": "Compositional semantics and sentence representations",
    "section": "45 Recap: Training a neural network",
    "text": "45 Recap: Training a neural network\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#cross-entropy-loss",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#cross-entropy-loss",
    "title": "Compositional semantics and sentence representations",
    "section": "46 Cross Entropy Loss",
    "text": "46 Cross Entropy Loss\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#softmax",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#softmax",
    "title": "Compositional semantics and sentence representations",
    "section": "47 Softmax",
    "text": "47 Softmax\n\n\n\n\nSlide 51"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-13",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-13",
    "title": "Compositional semantics and sentence representations",
    "section": "48 Title",
    "text": "48 Title\n\n\n\n\nSlide 52\n\n\n\n\nFeed forward NNs were not able to contain word order information, RNN can do it\nHere the words would be conditioned in the previous words"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-recurrent-neural-network-rnn",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-recurrent-neural-network-rnn",
    "title": "Compositional semantics and sentence representations",
    "section": "49 Introduction: Recurrent Neural Network (RNN)",
    "text": "49 Introduction: Recurrent Neural Network (RNN)\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-recurrent-neural-network-rnn-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-recurrent-neural-network-rnn-1",
    "title": "Compositional semantics and sentence representations",
    "section": "50 Introduction: Recurrent Neural Network (RNN)",
    "text": "50 Introduction: Recurrent Neural Network (RNN)\n\n\n\n\nSlide 54\n\n\n\n\n6 words 6 times steps"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-recurrent-neural-network-rnn-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-recurrent-neural-network-rnn-2",
    "title": "Compositional semantics and sentence representations",
    "section": "51 Introduction: Recurrent Neural Network (RNN)",
    "text": "51 Introduction: Recurrent Neural Network (RNN)\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-unfolding-the-rnn",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-unfolding-the-rnn",
    "title": "Compositional semantics and sentence representations",
    "section": "52 Introduction: Unfolding the RNN",
    "text": "52 Introduction: Unfolding the RNN\n\n\n\n\nSlide 56\n\n\n\n\nThe W and the R amtrix are the same, they are shared"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-making-a-prediction",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-making-a-prediction",
    "title": "Compositional semantics and sentence representations",
    "section": "53 Introduction: Making a prediction",
    "text": "53 Introduction: Making a prediction\n\n\n\n\nSlide 57\n\n\n\n\nWhen you reach the end of the sentence, then you use the ouput vector for this word as the sentence representation. We can do this vecause this last time step was influenced by the entire history so that is why we substitute, this as our sentence representation. And then we can project it as 5 dimensional representation and then we take the argmax or softmax based on this representation"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-the-vanishing-gradient-problem",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-the-vanishing-gradient-problem",
    "title": "Compositional semantics and sentence representations",
    "section": "54 Introduction: The vanishing gradient problem",
    "text": "54 Introduction: The vanishing gradient problem\n\n\n\n\nSlide 58\n\n\n\n\nThey tend to surfer from the vanishing problem"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-the-vanishing-gradient-problem-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-the-vanishing-gradient-problem-1",
    "title": "Compositional semantics and sentence representations",
    "section": "55 Introduction: The vanishing gradient problem",
    "text": "55 Introduction: The vanishing gradient problem\n\n\n\n\nSlide 59\n\n\n\n\nHere 5 num unrolls N refers to the amount of words you have in a sentence"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this-3",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#what-about-this-3",
    "title": "Compositional semantics and sentence representations",
    "section": "56 What about this?",
    "text": "56 What about this?\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#rnn-vs-ann",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#rnn-vs-ann",
    "title": "Compositional semantics and sentence representations",
    "section": "57 RNN vs ANN",
    "text": "57 RNN vs ANN\n\n\n\n\nSlide 61\n\n\n\n\nIn the ANN you have different parameters for your matrix in each layer and the problem could cancel out. However even in ANN you can run into vanishing/exploding gradients\n\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#long-short-term-memory-lstm",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#long-short-term-memory-lstm",
    "title": "Compositional semantics and sentence representations",
    "section": "58 Long Short-Term Memory (LSTM)",
    "text": "58 Long Short-Term Memory (LSTM)\nCV: resNET skip connections to aliviate exploding/vanishing gradients\nNLP: LSTM were introduced\n\n\n\n\nSlide 63\n\n\n\n\nLSTM are good to deal with long-term dependencies because they are able to cpe with exploding/vanishing gradients"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-core-idea",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-core-idea",
    "title": "Compositional semantics and sentence representations",
    "section": "59 LSTM: Core idea",
    "text": "59 LSTM: Core idea\n\n\n\n\nSlide 64\n\n\n\n\nThe cells are supposed to capture the long term memory information from the sentence\nThis is good because backpropagation through time with have this partially uninterrupted gradient flow"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstms",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstms",
    "title": "Compositional semantics and sentence representations",
    "section": "60 LSTMs",
    "text": "60 LSTMs\n\n\n\n\nSlide 65\n\n\n\n\nNow the activation function here would be the LSTM, so each copy would contain an LSTM cell where before we have one layer and now we will have the cell with four different layers interacting with each other"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-cell",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-cell",
    "title": "Compositional semantics and sentence representations",
    "section": "61 LSTM cell",
    "text": "61 LSTM cell\n\n\n\n\nSlide 66\n\n\n\n\n3 gates:\n\nforget gate\ninput gate\noutput gate\n\nc_t is the memory cell, here we do not apply any weights, we just do multiplication and summation. This is why people call it as a conveyers belt. So here we forget information, we add information but information flows interrupted"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-cell-state",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-cell-state",
    "title": "Compositional semantics and sentence representations",
    "section": "62 LSTM: Cell state",
    "text": "62 LSTM: Cell state\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-forget-gate",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-forget-gate",
    "title": "Compositional semantics and sentence representations",
    "section": "63 LSTM: Forget gate",
    "text": "63 LSTM: Forget gate\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-candidate-cell",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-candidate-cell",
    "title": "Compositional semantics and sentence representations",
    "section": "64 LSTM: Candidate cell",
    "text": "64 LSTM: Candidate cell\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-input-gate",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-input-gate",
    "title": "Compositional semantics and sentence representations",
    "section": "65 LSTM: Input gate",
    "text": "65 LSTM: Input gate\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm",
    "title": "Compositional semantics and sentence representations",
    "section": "66 LSTM",
    "text": "66 LSTM\n\n\n\n\nSlide 71"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-output-gate",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstm-output-gate",
    "title": "Compositional semantics and sentence representations",
    "section": "67 LSTM: Output gate",
    "text": "67 LSTM: Output gate\n\n\n\n\nSlide 72\n\n\n\n\nHere we are saying what do I want to keep from my long term memory. The ouput of that is going to be my new output vector.\n\n67.1 Recap\n\n\n\n\n\nUse the forget gate to get the input word x_t and the previous h_{t-1} with that you do apply softmax which then you multiply to the cell state which is the memory.\n\nHere if you multiply by 1, then you wan to keep those items in memory. If 0 then you do not want to conserve them.\n\nWe use the candidate gate where you mutliply the ouput of the tanh which gives candidate values between -1 and 1 with some scaled softmax from the input gate. With this we selectively add what to conserve in the memory cell\nWe update the values of the ouput gate which we take: from the cell memory values between -1 to 1 and we multiply these by a softmax version from the input words x_t and also the previous state h_{t-1}\n\n\nStep 1 & 2 is called long-term memory\nStep 3, is the short term memory. This is a filtered version of the long-term memory."
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#long-short-term-memory-lstm-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#long-short-term-memory-lstm-1",
    "title": "Compositional semantics and sentence representations",
    "section": "68 Long Short-Term Memory (LSTM)",
    "text": "68 Long Short-Term Memory (LSTM)\n\n\n\n\nSlide 73\n\n\n\n\n\nCell state is the long term memory\nHidden state is your short term memory"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstms-applications-success-in-nlp",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstms-applications-success-in-nlp",
    "title": "Compositional semantics and sentence representations",
    "section": "69 LSTMs: Applications & Success in NLP",
    "text": "69 LSTMs: Applications & Success in NLP\n\n\n\n\nSlide 74\n\n\n\n\n\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#summary-fo-models-seen-so-far",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#summary-fo-models-seen-so-far",
    "title": "Compositional semantics and sentence representations",
    "section": "70 Summary fo models seen so far",
    "text": "70 Summary fo models seen so far\n\n\n\n\nSlide 76\n\n\n\n\nSequence models :\n\nRNN\nLSTM\n\nTree-structure models are the ones that are also sensitive to the syntactic structure"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#second-approach-sentence-sentiment-syntax",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#second-approach-sentence-sentiment-syntax",
    "title": "Compositional semantics and sentence representations",
    "section": "71 Second approach: Sentence + Sentiment + Syntax",
    "text": "71 Second approach: Sentence + Sentiment + Syntax\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#exploiting-tree-structure",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#exploiting-tree-structure",
    "title": "Compositional semantics and sentence representations",
    "section": "72 Exploiting tree structure",
    "text": "72 Exploiting tree structure\n\n\n\n\nSlide 78\n\n\n\n\nCompositionality was this idea that you cannot derive the meaning of sentences from the meaning of the individual words.\nIn the models seen so far, we were getting the sentence representation deriving it from the individual words. But we were not taken the syntactic structure into account. These Tree LSTM allow us to do both. So we will get the meaning of the words and also the rules that combine them"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#why-would-it-be-useful",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#why-would-it-be-useful",
    "title": "Compositional semantics and sentence representations",
    "section": "73 Why would it be useful?",
    "text": "73 Why would it be useful?\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#constituency-parse",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#constituency-parse",
    "title": "Compositional semantics and sentence representations",
    "section": "74 Constituency Parse",
    "text": "74 Constituency Parse\n\n\n\n\nSlide 80"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recurrent-vs-tree-recursive-nn",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recurrent-vs-tree-recursive-nn",
    "title": "Compositional semantics and sentence representations",
    "section": "75 Recurrent vs Tree Recursive NN",
    "text": "75 Recurrent vs Tree Recursive NN\n\n\n\n\nSlide 81\n\n\n\n\nRecurrent NNS that are LSTMs but you also have tree RNN which are recursive\nIf you input “I loved this movie” to the RNN you will not be able to model, the phrase independetly of the previous words in the sentence. So for instance “this movie” is dependent of having seen “I Loved” that means I cannot extract separate phrase representations from your sentence representations\nThis is different in the three recursive NN, because you explicitly first compose “this movie” into a phrase representation and then you would make it dependent on the previous word while you go up in the three"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#tree-recursive-nn",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#tree-recursive-nn",
    "title": "Compositional semantics and sentence representations",
    "section": "76 Tree Recursive NN",
    "text": "76 Tree Recursive NN\n\n\n\n\nSlide 82"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#practical-ii-data-set-stanford-sentiment-treebank-sst",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#practical-ii-data-set-stanford-sentiment-treebank-sst",
    "title": "Compositional semantics and sentence representations",
    "section": "77 Practical II data set: Stanford Sentiment Treebank (SST)",
    "text": "77 Practical II data set: Stanford Sentiment Treebank (SST)\n\n\n\n\nSlide 83"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#tree-lstms-generalize-lstm-to-tree-structure",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#tree-lstms-generalize-lstm-to-tree-structure",
    "title": "Compositional semantics and sentence representations",
    "section": "78 Tree LSTMs: Generalize LSTM to tree structure",
    "text": "78 Tree LSTMs: Generalize LSTM to tree structure\n\n\n\n\nSlide 84\n\n\n\n\nWe can input multiple children in each time step"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#tree-lstms",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#tree-lstms",
    "title": "Compositional semantics and sentence representations",
    "section": "79 Tree LSTMs",
    "text": "79 Tree LSTMs\n\n\n\n\nSlide 85\n\n\n\n\n\nYou can use any number of children that you want but you will loose child order information\nN-ary Tree LSTM: in practical Binary parse tree"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#child-sum-tree-lstm",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#child-sum-tree-lstm",
    "title": "Compositional semantics and sentence representations",
    "section": "80 Child-Sum Tree LSTM",
    "text": "80 Child-Sum Tree LSTM\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#child-sum-tree-lstm-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#child-sum-tree-lstm-1",
    "title": "Compositional semantics and sentence representations",
    "section": "81 Child-Sum Tree LSTM",
    "text": "81 Child-Sum Tree LSTM\n\n\n\n\nSlide 87"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#n-ary-tree-lstm",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#n-ary-tree-lstm",
    "title": "Compositional semantics and sentence representations",
    "section": "82 N-ary Tree LSTM",
    "text": "82 N-ary Tree LSTM\n\n\n\n\nSlide 88\n\n\n\n\nThat means I have to input separatly to the model because they have separate parameters matrices, so you not just summed them up."
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#n-ary-tree-lstm-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#n-ary-tree-lstm-1",
    "title": "Compositional semantics and sentence representations",
    "section": "83 N-ary Tree LSTM",
    "text": "83 N-ary Tree LSTM\n\n\n\n\nSlide 66\n\n\n\n\n\n\n\n\nSlide 89"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#n-ary-tree-lstm-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#n-ary-tree-lstm-2",
    "title": "Compositional semantics and sentence representations",
    "section": "84 N-ary Tree LSTM",
    "text": "84 N-ary Tree LSTM\n\n\n\n\nSlide 90\n\n\n\n\n\\(u_j\\) is for the candidate gate\nFor each child \\(h_j\\), we have a separate parameter matrix and you ill be summing\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstms-vs-tree-lstms",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#lstms-vs-tree-lstms",
    "title": "Compositional semantics and sentence representations",
    "section": "85 LSTMs vs Tree-LSTMs",
    "text": "85 LSTMs vs Tree-LSTMs\n\n\n\n\nSlide 91\n\n\n\n\nTree-LSTM general, general LSTM its just a Tree-LSTM with one child. So if you have one child then you have your standard tree LSTM"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-14",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-14",
    "title": "Compositional semantics and sentence representations",
    "section": "86 Title",
    "text": "86 Title\n\n\n\n\nSlide 92"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-15",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-15",
    "title": "Compositional semantics and sentence representations",
    "section": "87 Title",
    "text": "87 Title\n\n\n\n\nSlide 93"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#building-a-tree-with-a-transition-sequence",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#building-a-tree-with-a-transition-sequence",
    "title": "Compositional semantics and sentence representations",
    "section": "88 Building a tree with a transition sequence",
    "text": "88 Building a tree with a transition sequence\n\n\n\n\nSlide 94"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example",
    "title": "Compositional semantics and sentence representations",
    "section": "89 Transition sequence example",
    "text": "89 Transition sequence example\n\n\n\n\nSlide 95"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-1",
    "title": "Compositional semantics and sentence representations",
    "section": "90 Transition sequence example",
    "text": "90 Transition sequence example\n\n\n\n\nSlide 96"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-2",
    "title": "Compositional semantics and sentence representations",
    "section": "91 Transition sequence example",
    "text": "91 Transition sequence example\n\n\n\n\nSlide 97"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-3",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-3",
    "title": "Compositional semantics and sentence representations",
    "section": "92 Transition sequence example",
    "text": "92 Transition sequence example\n\n\n\n\nSlide 98"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-4",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-4",
    "title": "Compositional semantics and sentence representations",
    "section": "93 Transition sequence example",
    "text": "93 Transition sequence example\n\n\n\n\nSlide 99"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-5",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-5",
    "title": "Compositional semantics and sentence representations",
    "section": "94 Transition sequence example",
    "text": "94 Transition sequence example\n\n\n\n\nSlide 100"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-6",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-6",
    "title": "Compositional semantics and sentence representations",
    "section": "95 Transition sequence example",
    "text": "95 Transition sequence example\n\n\n\n\nSlide 101"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-7",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-7",
    "title": "Compositional semantics and sentence representations",
    "section": "96 Transition sequence example",
    "text": "96 Transition sequence example\n\n\n\n\nSlide 102"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-16",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-16",
    "title": "Compositional semantics and sentence representations",
    "section": "97 Title",
    "text": "97 Title\n\n\n\n\nSlide 103\n\n\n\n\nBecause we are doing this in sequence so putting thins on the stack and then to the tree we cannot dot his in parallel, so this is slow. Thus, we want to do mini-batch where you process multiple sentences at the same time and"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched",
    "title": "Compositional semantics and sentence representations",
    "section": "98 Transition sequence example (mini-batched)",
    "text": "98 Transition sequence example (mini-batched)\n\n\n\n\nSlide 104"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-1",
    "title": "Compositional semantics and sentence representations",
    "section": "99 Transition sequence example (mini-batched)",
    "text": "99 Transition sequence example (mini-batched)\n\n\n\n\nSlide 105"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-2",
    "title": "Compositional semantics and sentence representations",
    "section": "100 Transition sequence example (mini-batched)",
    "text": "100 Transition sequence example (mini-batched)\n\n\n\n\nSlide 106"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-3",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-3",
    "title": "Compositional semantics and sentence representations",
    "section": "101 Transition sequence example (mini-batched)",
    "text": "101 Transition sequence example (mini-batched)\n\n\n\n\nSlide 107"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-4",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-4",
    "title": "Compositional semantics and sentence representations",
    "section": "102 Transition sequence example (mini-batched)",
    "text": "102 Transition sequence example (mini-batched)\n\n\n\n\nSlide 108"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-5",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#transition-sequence-example-mini-batched-5",
    "title": "Compositional semantics and sentence representations",
    "section": "103 Transition sequence example (mini-batched)",
    "text": "103 Transition sequence example (mini-batched)\n\n\n\n\nSlide 109"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#optional-approach-sentence-sentiment-syntax-node-level-sentiment",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#optional-approach-sentence-sentiment-syntax-node-level-sentiment",
    "title": "Compositional semantics and sentence representations",
    "section": "104 Optional approach: Sentence + Sentiment + Syntax + Node-level sentiment",
    "text": "104 Optional approach: Sentence + Sentiment + Syntax + Node-level sentiment\n\n\n\n\nSlide 110"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-17",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-17",
    "title": "Compositional semantics and sentence representations",
    "section": "105 Title",
    "text": "105 Title\n\n\n\n\nSlide 111"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recap-1",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recap-1",
    "title": "Compositional semantics and sentence representations",
    "section": "106 Recap",
    "text": "106 Recap\n\n\n\n\nSlide 112"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-18",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#title-18",
    "title": "Compositional semantics and sentence representations",
    "section": "107 Title",
    "text": "107 Title\n\n\n\n\nSlide 113"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#input",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#input",
    "title": "Compositional semantics and sentence representations",
    "section": "108 Input",
    "text": "108 Input\n\n\n\n\nSlide 114"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recap-activation-functions",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#recap-activation-functions",
    "title": "Compositional semantics and sentence representations",
    "section": "109 Recap: Activation functions",
    "text": "109 Recap: Activation functions\n\n\n\n\nSlide 115"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-intuition-to-solving-the-vanishing-gradient",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-intuition-to-solving-the-vanishing-gradient",
    "title": "Compositional semantics and sentence representations",
    "section": "110 Introduction: Intuition to solving the vanishing gradient",
    "text": "110 Introduction: Intuition to solving the vanishing gradient\n\n\n\n\nSlide 116"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-a-small-improvement",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#introduction-a-small-improvement",
    "title": "Compositional semantics and sentence representations",
    "section": "111 Introduction: A small improvement",
    "text": "111 Introduction: A small improvement\n\n\n\n\nSlide 117"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#child-sum-tree-lstm-2",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#child-sum-tree-lstm-2",
    "title": "Compositional semantics and sentence representations",
    "section": "112 Child-Sum Tree LSTM",
    "text": "112 Child-Sum Tree LSTM\n\n\n\n\nSlide 118"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#a-naive-recursive-nn",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#a-naive-recursive-nn",
    "title": "Compositional semantics and sentence representations",
    "section": "113 A naive recursive NN",
    "text": "113 A naive recursive NN\n\n\n\n\nSlide 119"
  },
  {
    "objectID": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#sgd-vs-gd",
    "href": "blog/2023-11-22_compositional-semantics-and-sentence-representations/index.html#sgd-vs-gd",
    "title": "Compositional semantics and sentence representations",
    "section": "114 SGD vs GD",
    "text": "114 SGD vs GD\n\n\n\n\nSlide 120"
  },
  {
    "objectID": "blog/2023-11-13_modelling-syntactic-structure/index.html",
    "href": "blog/2023-11-13_modelling-syntactic-structure/index.html",
    "title": "Modelling Syntactic Structure",
    "section": "",
    "text": "Modelling Syntactic Structure\n        \n        \n                    \n                \n                    Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 13, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n\n\n\n\n\nSlide 2\n\n\n\n\n\n\n\n\nSlide 3\n\n\n\n\n\nBoW\n\nBag of Words does not consider order, it is basically counting what occur in a sentence in arbitrary order\n\nMarkov Models\n\nThey are bags of phrases, they have better units but are very sparse\n\nHMMs:\n\nNow we work in the tag space. Here we push the memory layer again to another space, in the tag space. So no memory in the word space itself. We capture shallo syntactic patterns such as NOUN is preceeded by an ADJECTIVE in english\nHMMs does not really have power for semantics because words are drawn conditionally independently\n\n\n\n\nSlide 4\n\n\n\n\nNow instead of just categories we talk about phrase categories.\nYou can now group words into phrases and these phrases can be labeled for their syntactic role\n\n\n\n\n\n\n\n\n\nSlide 10\n\n\n\n\nNow instead of isolated tags now we considered as phrases\n\n\n\n\nSlide 11\n\n\n\n\nA phrase usually has what is called a syntactic head, and usually is the first occurrences that name the phrase for i.e NP, NOUN is the head. All remaining after or before its optional but in needs to contain the head\n\n\n\n\nSlide 12\n\n\n\n\nConstituency is groups of words that act as a unit once you identify them. An evidence for this is that they appear in similar syntactic environments i.e NP ‘Nounn Phrases’ tend to appear before a verb\n\n\n\n\n\nSlide 14\n\n\n\n\n\n\n\n\nSlide 15\n\n\n\n\n\n\n\n\nSlide 16\n\n\n\n\n\nSymbols\n\n\nWe start from a vocabulary of symbols or (the constants): words\ni.e ‘I’, ‘eat’, ‘pizza’\nYou have non-terminals or (variables): phrasal categories\n‘S’, ‘NP’, ‘VP’\n\n\nRules: tells you how to rewrite a non-terminal category into a string that is made up of terminal and non-terminal symbols\n\nThe grammar is context free if on the left side of the any rule you have a single non-terminal. That makes it context free. IF there would be a collection of terminals on the left hand side, that would be context sensitive grammar\n\n\n\n\nSlide 17\n\n\n\n\nEvery single CFGs is an algorithm to represent a (in)finite set of strings. Is a finite representation of an infinite object\n\n\n\n\nSlide 18\n\n\n\n\n(constants): - \\(\\sum\\) terminals symbols: words\ni.e ‘I’, ‘eat’, ‘pizza’\n(variables): - \\(\\textbf{V}\\) non-terminals symbols: phrasal categories\ni.e ‘S’, ‘NP’, ‘VP’, ‘ADJ’, ‘DET’\nIn this set there a start symbol called \\(S\\) that belongs to \\(\\textbf{V}\\). We will put this one at the top of the context free grammar tree\n\n\n\n\nSlide 19\n\n\n\n\nThe arity is defined by the longest right hand side on the rule.\nAll CFGs can be rewritten to be binary that accepts the same exact strings, so the sentences that one accept is the same as the one the other would accept\n\n\n\n\nSlide 20\n\n\n\n\nCFG gives a recipe to generate text.\nA derivation is a sequence of strings. We start with the ‘start’ symbol then recursively we rewrite the leftmost non-terminal (‘S’,‘NN’.. ) sequence X.\nX is a sequence that belongs to \\(\\textbf{V}\\) so it can be a subset like:\nX:{‘S’, ‘DET’}\n\n\n\n\nSlide 21\n\n\n\n\nIf we are able to determine with the CFG then the sequence its part of the language, the grammar specifies the language, the set of words sequences. If you can derive a word sequences then it is part of the grammar implied by this CFG, its name then you would use is called the hug of the derivation\n\n\n\n\nSlide 22\n\n\n\n\n\nYou start with S\nThe left most symbol is S, then this is rewriten as Np, VP. There is a rule in the grammar that says S -&gt; NP, VP\nNP are usually DETERMINANTS or NOUNS, so we write that\nD, now can be replace by its terminal/constant i.e ‘the’\nN, can be rewrite it as ‘dog’ and so son..\n\nWe accomplished with this ‘the dog has barked loudly’ which is a full sentence. We use a depth-first order going always left.\nFrom top to bottom: generation, the opposite its recognition.\n\n\n\n\nSlide 23\n\n\n\n\nWhere does the preposition attach? Does it attach to the noun pizza? So it is pizza with anchovies on top, or does the preposition attach to the verb phrase eat pizza, which then becomes the instrument of eating\nPrepositions are words that show the relationship between a noun (or pronoun) and other elements in a sentence:\n\nIn: I live in the city.\nOn: The book is on the table.\nUnder: The cat is under the chair.\nBetween: Choose between the two options.\nBehind: The sun sets behind the mountains.\nAcross: They walked across the bridge.\nThrough: We walked through the park.\nAbove: The plane is flying above the clouds.\nBeneath: The treasure is buried beneath the sand.\nNear: The store is near the school.\n\nSo here there is ambiguity in syntactic parsing. The way we deal with structural ambiguity is by learning a probability distribution\nThere is not a tree for every single sentence in english so now we have to rely on a prob distribution\n\n\n\n\n\n\n\nSlide 27\n\n\n\n\n\n\n\n\nSlide 28\n\n\n\n\n\n\n\n\nSlide 29\n\n\n\n\nTo assign prob to a ‘Derivation’ we need random variables again. Here the length of the rule derivation R1…R_m is not the same as the length of the words sequence W1…W_L\nThe derivations length depends on the grammar, but for every tree structure at the bottom then you find words, so for every sequence of M rule applications there is a sequence of words that you can read at the bottom. I will assign a probability to a derivation by asigning probs to each one of the rules that I applied given the rules that I have already applied (so that is chain rule as how we did it for a sequence of words, but now we do chain rule for a sequence of rules)\n\n\n\n\nSlide 30\n\n\n\n\nFor now we are gonna make Markov assumptions and make rules independently. We are gonna assign probabilities to each rule independently fo the next. A rule in itself it is not an atomic thing. A rule is a pair, a rule has not terminal on the left and a string on the right\nThis is the trick to introduce some dependency:\nWe will generate the right hand side of the rule given is not a terminal symbol. So basically we imitate the process by which you go deeper and deeper until you find a terminal\nSo you look at the non-terminal and you will rewrite it without access to more context.\n\n\n\n\nSlide 31\n\n\n\n\n\n\n\n\nSlide 32\n\n\n\n\nYou can sample from the generative history.\n\nStart a queue with an S inside. If all symbols there are terminals, then you are go to go that is a sentence\nIF not true, pick the left most, pop it, take it from that thing and replace it by the RHS of a rule (that has that symbol on the LHS and note that you will have multiple rules, but they have probabilities so you draw one of them follwoing the distribution of their probabilities) rewrite that symbol. Do that with certain probability and repeat from 2.\n\n\n\n\n\nSlide 33\n\n\n\n\nWe have a condition distribution where we condition on, the LHS non-terminan symbol\nWe generate the RHS string so i.e.\nIf S is the variable you are rewritting and you happen to know two rules, NP-&gt;VP and S-&gt; VP. So you either write a NOUN PHRASE and concatenate with it or you have a VERB PHRASE and you concatenate it. So here we have two parameters, they are probabilities, they are normalized and they summ up to one\nPCFG (usually the number of outcomes will be fixed but here is different), here it is not for every symbol you condition on, the number of outcomes depends on how many rules you know for that symbol. So I know two rules for the start S, but I know 3 terminals for the non-terminal NOUN, cat, dog and bird. So the categorical distribution will have different number of probabilities this time\nSo the categorical distribution will have different number of probs in them, so when you are given a derivation how do we get its probability mass. Yoy multiply the probabilities for the rules that are inside. And where do we find these probabilities, well this is tabular, so in table\n\n\n\n\nSlide 34\n\n\n\n\nGiven a dataset can you do MLE, to find the probability that you would rewrite a varaible ‘b’ into a string Beta. ie.e maybe ‘v=NP’ and BETA=the terminal NOUN\nSo what is the probability that you would write a NOUN PHRASE as a sequence of DETERMINER followed by a NOUN. For that to come up I give you a model: PCFG, I give you the model description, I give you an al algorithm so MLE, and give you a dataset.\nSo this in next slide is how we compute the MLE\n\\(\\theta\\) is the name of the table, the row is ‘v’ and the column=sequence on the right so Beta\nSo we count how many times v has rewrite to B and you divide the numer of times you have seen ‘v’\nSo if this was a NP rewrite to a DETERMINER and a NOUN then, we count how many times I have seen this divide by how many times I have seen NP being rewrite by whatever\n\n\n\n\n\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-10-12_what-is-the-covariance-matrix/index.html",
    "href": "blog/2023-10-12_what-is-the-covariance-matrix/index.html",
    "title": "The Covariance Matrix and relation with PCA",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 12, 2023\nIn a 2D dimensional feature space with 10 data points, the covariance matrix provides a measure of the relationship between the two features (dimensions) and how they vary together. The covariance matrix is a 2x2 matrix that quantifies the degree to which the two features change together\nLet’s say you have two features, \\(\\textbf{x}\\) and \\(\\textbf{y}\\), and you have 10 data points with values (x_1, y_1), (x_2, y_2), …, (x_10, y_10). The covariance matrix Σ is calculated as:\n\\[\n\\Sigma = \\begin{bmatrix}\n\\text{cov}(\\textbf{x}, \\textbf{x}) & \\text{cov}(\\textbf{x}, \\textbf{y}) \\\\\n\\text{cov}(\\textbf{y}, \\textbf{x}) & \\text{cov}(\\textbf{y}, \\textbf{y})\n\\end{bmatrix}\n\\]\nWhere:\nThe covariances of two random variables (the two features) are calculated using the formula:\n\\[\n\\text{cov}(\\textbf{x}, \\textbf{y}) = \\frac{1}{N} \\sum_{n=1}^{N} (x_n - {\\bar{x}})({y}_n - \\bar{y})\n\\]\nWhere:\nNote:\nThe calculation for the covariance matrix can be also expressed as:\n\\[\n\\Sigma = \\frac{1}{N} \\sum_{n=1}^{N} (\\textbf{x}_n - \\mathbf{\\bar{x}})(\\textbf{x}_n - \\mathbf{\\bar{x}})^T\n\\]\nWhere:\nBelow some example:"
  },
  {
    "objectID": "blog/2023-10-12_what-is-the-covariance-matrix/index.html#python-example",
    "href": "blog/2023-10-12_what-is-the-covariance-matrix/index.html#python-example",
    "title": "The Covariance Matrix and relation with PCA",
    "section": "1 Python Example",
    "text": "1 Python Example\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('ggplot')\n# plt.rcParams['figure.figsize'] = (12, 8)\n\n# Normal distributed x and y vector with mean 0 and standard deviation 1\nx = np.random.normal(0, 1, 500)\ny = np.random.normal(0, 1, 500)\nX = np.vstack((x, y)).T\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.title('Generated Data')\nplt.axis('equal')\nplt.show()\n\n\n\n\n\nIf two feature vectors are independent (or uncorrelated) the matrix matrix would be:\n\\[\n\\Sigma = \\begin{bmatrix}\n\\text{cov}(\\textbf{x}, \\textbf{x}) & 0 \\\\\n0 & \\text{cov}(\\textbf{y}, \\textbf{y})\n\\end{bmatrix}\n\\]\nIf this data was generated with unit cov(x,x) and unit cov(y,y) then we have a Identity covariance matrix\n\n\n\n\n\nAs x1 increases x2 increases too, because we have 0.8. We have a positive correlation on the data."
  },
  {
    "objectID": "blog/2023-10-12_what-is-the-covariance-matrix/index.html#relation-with-pca",
    "href": "blog/2023-10-12_what-is-the-covariance-matrix/index.html#relation-with-pca",
    "title": "The Covariance Matrix and relation with PCA",
    "section": "2 Relation with PCA",
    "text": "2 Relation with PCA\nWhen you multiply the \\(\\Sigma\\) covariance matrix times a vector and then again and again you end up with a vector that points to directions where you have the greatest variance in the data. So like where all points are spread out.\n\n\n\n\n\nIf I multiply the vector \\(e2\\) by the cov matrix it will not turn but will get longer and longer but will point in the same direction\nThus we want to find some vector \\(e\\) that when we multiply it with the cov matrix do not change direction. The vectors are called eigenvectors and the lambdas so the scalar version will be called eigenvalues.\nThe later point will be called our principal components which are the eigenvectors with the largest eigenvalues"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html",
    "title": "Deep Learning Optimizations II",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 20, 2023"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#lecture-overview",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#lecture-overview",
    "title": "Deep Learning Optimizations II",
    "section": "1 Lecture overview",
    "text": "1 Lecture overview\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#start-with",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#start-with",
    "title": "Deep Learning Optimizations II",
    "section": "2 Start with",
    "text": "2 Start with\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#quiz",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#quiz",
    "title": "Deep Learning Optimizations II",
    "section": "3 Quiz",
    "text": "3 Quiz\n\n\n\n\nSlide 6\n\n\n\n\nWhy left would be preferred?\n\nsimpler model\ngeneralize better to unseen data\n\nWhy the right is better?\n\nWe are actually fitting all the data points\nThe right hand side is the ground truth\nYou dont care about extrapolating"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#digression-gravitation",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#digression-gravitation",
    "title": "Deep Learning Optimizations II",
    "section": "4 Digression: Gravitation",
    "text": "4 Digression: Gravitation\n\n\n\n\nSlide 7"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#bias-variance-tradeoff",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#bias-variance-tradeoff",
    "title": "Deep Learning Optimizations II",
    "section": "5 Bias-variance tradeoff",
    "text": "5 Bias-variance tradeoff\n\n\n\n\nSlide 8\n\n\n\n\nThe single best prediction of the parameters\nA good estimator is a function whose output is close to the true underlying thetha that generated the data"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#bias-variance-tradeoff-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#bias-variance-tradeoff-1",
    "title": "Deep Learning Optimizations II",
    "section": "6 Bias-variance tradeoff",
    "text": "6 Bias-variance tradeoff\n\n\n\n\nSlide 9\n\n\n\n\nBias\nEstimator’s expected value= so the ouput of our thetha estimate and the true value of that parameter\nBias comes from not being able to model the real model in a correct way\nVariance\nSo if you use a different training set or a different split how different are the learnt NNs parameters. If differ a lot then high variance"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#bias-variance-tradeoff-2",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#bias-variance-tradeoff-2",
    "title": "Deep Learning Optimizations II",
    "section": "7 Bias-variance tradeoff",
    "text": "7 Bias-variance tradeoff\n\n\n\n\nSlide 10\n\n\n\n\nHigh variance & low bias, your model on average get it right but it has a high variance because it wants to model all noise, so overfilling, it spread all over the place\nHigh bias and low variance, in average it does not even get it correct so underfitting"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting",
    "title": "Deep Learning Optimizations II",
    "section": "8 Overfitting",
    "text": "8 Overfitting\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting-1",
    "title": "Deep Learning Optimizations II",
    "section": "9 Overfitting",
    "text": "9 Overfitting\n\n\n\n\nSlide 12\n\n\n\n\nIt will learn to recognize that the img contains hourses not by the hourses but the watermark. This is seen in the heat-map where the point of attention is in the watermark. This is overfitting"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting-2",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting-2",
    "title": "Deep Learning Optimizations II",
    "section": "10 Overfitting",
    "text": "10 Overfitting\n\n\n\n\nSlide 13\n\n\n\n\nTo avoid overfitting we use regularization"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting-how-powerful-are-neural-networks",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#overfitting-how-powerful-are-neural-networks",
    "title": "Deep Learning Optimizations II",
    "section": "11 Overfitting: how “powerful” are neural networks?",
    "text": "11 Overfitting: how “powerful” are neural networks?\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#this-isnt-the-full-story..",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#this-isnt-the-full-story..",
    "title": "Deep Learning Optimizations II",
    "section": "12 This isn’t the full story..",
    "text": "12 This isn’t the full story..\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#update-2019-double-descent",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#update-2019-double-descent",
    "title": "Deep Learning Optimizations II",
    "section": "13 Update 2019: Double Descent",
    "text": "13 Update 2019: Double Descent\n\n\n\n\nSlide 16\n\n\n\n\nDash line is where the data samples equals the number of parameters\nSo in the top right image even if we have increase the number of hidden layers and our training error is zero then our test set is still decreasing that is weird.\nSo here we then presume that bigger model will have lower error.\nBefore we use to have a curve upwrads from the tipical bias/variance curve in the prev slide, but the weird thing is that where the dash line meets then this error starts to decrease again.\nTwo answers: smoothness and regularization"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#double-descent-smoothness-from-bigger-models",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#double-descent-smoothness-from-bigger-models",
    "title": "Deep Learning Optimizations II",
    "section": "14 Double-descent: Smoothness from bigger models",
    "text": "14 Double-descent: Smoothness from bigger models\n\n\n\n\nSlide 17\n\n\n\n\nIn the x axis you have the number of times SGD was proceed, so the amount of units of texts akak ‘words’ were processed\nWe can see that with larger models I require fewer samples to reach a lower test loss\nAlso when they reach stability they can provide with a test loss which is the best shot they can give and this depends on their size, larger models give more accurate results\nAlso it shows that the quicker models learns quickler than the smaller model. So while the smaller model for a given number of tokens the large model learns more quickler.\nThis is Language models still not applicable for vision.\nWe also can say that (in language models) in terms of flops is more efficient to learn large models for fewer steps than to learn small models for larger amount of steps"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#double-descent-in-practice",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#double-descent-in-practice",
    "title": "Deep Learning Optimizations II",
    "section": "15 Double-descent in practice?",
    "text": "15 Double-descent in practice?\n\n\n\n\nSlide 18\n\n\n\n\nIn practice if you increase the number of neurons you may be closer to overfitting. For that we need regularization"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#regularization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#regularization",
    "title": "Deep Learning Optimizations II",
    "section": "16 Regularization",
    "text": "16 Regularization\n\n\n\n\nSlide 19\n\n\n\n\nHere we reduce the complexity of a NN and avoid ovverfitting"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l2-regularization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l2-regularization",
    "title": "Deep Learning Optimizations II",
    "section": "17 1) L2-regularization",
    "text": "17 1) L2-regularization\n\n\n\n\nSlide 20\n\n\n\n\nReferred to weight decay or Ritch regression in the linear case\nOmega is proportional to how large the weights are\nMinimizing this is also the same as if you assume a Gaussian prior on your weight, here you assume the weights are Gaussian distributed"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l2-regularization-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l2-regularization-1",
    "title": "Deep Learning Optimizations II",
    "section": "18 1) L2-regularization",
    "text": "18 1) L2-regularization\n\n18.1 L2 & L1 Formula\n\n\n\n\n\n\n\n\n\n\nL2 example Python\n\n\n\n\n\nThe L2 loss, also known as the Euclidean loss or Mean Squared Error (MSE), is a common loss function used in regression problems. It measures the average squared difference between the predicted values and the actual values.\nThe formula for L2 loss is given by:\n\\(L2\\_loss = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\\)\nWhere:\n\n\\(N\\) is the number of data points.\n\\(y_i\\) is the true (ground truth) value for the i-th data point.\n\\(\\hat{y}_i\\) is the predicted value for the i-th data point.\n\nHere’s a simple example in Python using NumPy:\n\n\nCode\nimport numpy as np\n\n# Generate some example data\ntrue_values = np.array([2, 4, 5, 4, 5])\npredicted_values = np.array([1.5, 3.5, 4.8, 4.2, 5.2])\n\n# Calculate L2 loss\nl2_loss = np.mean((true_values - predicted_values)**2)\n\nprint(\"L2 Loss:\", l2_loss)\n\n\nL2 Loss: 0.12400000000000003\n\n\nThe L2 loss is calculated by taking the mean of the squared differences between the true and predicted values. The smaller the L2 loss, the better the model’s predictions align with the true values.\n\n\n\n\n\n\n\nSlide 21\n\n\n\n\nLamda tells you how important the regularization is, if:\n\nLambda high then puts all the weights towards zero\nLambda is zero, then it learns the typical loss and do not care about regularization\n\nLambda during trainnig is fixed, if you want to find the best one you need to run lots of experiments"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l1-regularization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l1-regularization",
    "title": "Deep Learning Optimizations II",
    "section": "19 2) L1-regularization",
    "text": "19 2) L1-regularization\n\n\n\n\nSlide 22\n\n\n\n\nIsotropics means is equal in all directions\nNow if you take the gradient of this, now if you take derivative of w, because you want to optimize then it end ups a 1/2. So you end up with a constant factor in the loss that keeps getting substracted. So what happens is that we end up substracting a constant to our weights.\nSo it substract a bit from the positive weights and adds a bit to the negative weights and pushed them towards zero"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l1-regularization-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#l1-regularization-1",
    "title": "Deep Learning Optimizations II",
    "section": "20 2) L1-regularization",
    "text": "20 2) L1-regularization\n\n\n\n\nSlide 23\n\n\n\n\nL1 leads to sparce weights, so that means more weights will be closer to zero\nIf alpha here increases then that means weights become zero"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#why-do-l1-and-l2-regularizations-work",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#why-do-l1-and-l2-regularizations-work",
    "title": "Deep Learning Optimizations II",
    "section": "21 Why do L1 and L2 Regularizations work?",
    "text": "21 Why do L1 and L2 Regularizations work?\n\n\n\n\nSlide 24\n\n\n\n\nL2 regularization has basically a circular constraint area because you have w1^2 + w2^2 needs to be constant so all these combinations would be a circle\nSo the contours of the loss function in red will intercept the constrains regions at an axis. What that means is that if you are trying to find in this case the optimal loss, then it touches the constraint region where one of the values is set to zero while with L2 regularization there is no particular point where you could make one of the weights zero\nThis is because it needs to have the sum of the squares to be a small value but there is no particular motivation to have a similar weigth dimension to be equal to zero, so there is no reason to have sparse weights in L2"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#effect-linear-regression-example",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#effect-linear-regression-example",
    "title": "Deep Learning Optimizations II",
    "section": "22 Effect: linear regression example",
    "text": "22 Effect: linear regression example\n\n\n\n\nSlide 25\n\n\n\n\nHere the alpha goes from strong to weak\nIn L1 it puches some weights to zero and then stay zero aftwer a while and it is not like all of them get smaller at the same time but some of them stay quite high for a loong time, and now increasing alpha here you are making individual weights close to zero.\nL2 Regularization (Weight Decay): Encourages smaller weights but does not force them to be exactly zero. It smoothens the weights but doesn’t induce sparsity.\nL1 Regularization: Promotes sparsity by adding a penalty term based on the absolute values of the weights. This can lead to some weights being exactly zero."
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#early-stopping",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#early-stopping",
    "title": "Deep Learning Optimizations II",
    "section": "23 3) Early stopping",
    "text": "23 3) Early stopping\n\n\n\n\nSlide 26"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#early-stopping-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#early-stopping-1",
    "title": "Deep Learning Optimizations II",
    "section": "24 3) Early stopping",
    "text": "24 3) Early stopping\n\n\n\n\nSlide 27\n\n\n\n\n*Typo: with better test set error\nThe model at this stage have low variance because they are not overfitting"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#early-stopping-2",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#early-stopping-2",
    "title": "Deep Learning Optimizations II",
    "section": "25 3) Early stopping",
    "text": "25 3) Early stopping\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#why-does-early-stopping-work-as-regularization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#why-does-early-stopping-work-as-regularization",
    "title": "Deep Learning Optimizations II",
    "section": "26 Why does early-stopping work as regularization?",
    "text": "26 Why does early-stopping work as regularization?\n\n\n\n\nSlide 29"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#why-does-early-stopping-work-as-regularization-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#why-does-early-stopping-work-as-regularization-1",
    "title": "Deep Learning Optimizations II",
    "section": "27 Why does early-stopping work as regularization?",
    "text": "27 Why does early-stopping work as regularization?\n\n\n\n\nSlide 30\n\n\n\n\nHere weight decay they mean by L2-regularization\n\n\n\n\n\n\nWeight decay (L2-loss) vs Early Stopping\n\n\n\n\n\nWeight decay, also known as L2 regularization, is a technique to prevent overfitting by adding a penalty term to the loss function that is proportional to the squared magnitudes of the weights. This regularization term discourages the model from learning very large weights and encourages a smoother and more generalized solution.\nEarly Stopping:\nEarly stopping is a regularization technique used during the training of a machine learning model, typically in the context of iterative optimization algorithms like gradient descent. The idea behind early stopping is to monitor the model’s performance on a validation set during training and stop the training process when the performance on the validation set starts to degrade, even if the performance on the training set continues to improve.\n\nMechanism: Monitor a performance metric (e.g., validation loss) on a separate validation set during training.\nDecision Criteria: Stop training when the performance on the validation set starts to worsen or fails to improve for a certain number of consecutive epochs.\nPurpose: Prevent overfitting by terminating training before the model starts to memorize noise in the training data.\n\nWeight Decay (L2 Regularization):\nWeight decay, also known as L2 regularization, is a technique to prevent overfitting by adding a penalty term to the loss function that is proportional to the squared magnitudes of the weights. This regularization term discourages the model from learning very large weights and encourages a smoother and more generalized solution.\n\nMechanism: Add a term to the loss function that penalizes large weights by adding the sum of squared weights multiplied by a regularization strength.\nDecision Criteria: No specific stopping criterion; regularization is applied throughout the training process.\nPurpose: Encourage the model to have smaller and more evenly distributed weights, preventing overfitting.\n\nKey Differences:\n\nFocus:\n\nEarly stopping focuses on monitoring the model’s performance during training and stopping when the validation performance indicates potential overfitting.\nWeight decay focuses on adjusting the optimization objective by penalizing large weights, aiming to prevent overfitting from the beginning of training.\n\nDecision Criteria:\n\nEarly stopping makes decisions based on the validation performance, and the training stops when the validation performance degrades.\nWeight decay does not have a specific stopping criterion; it is a continuous regularization technique applied throughout training.\n\nImplementation:\n\nEarly stopping involves monitoring and interrupting the training loop.\nWeight decay involves adding a regularization term to the loss function during each iteration of the optimization algorithm.\n\n\nIn practice, these techniques can be used together to enhance the regularization effect and improve the generalization performance of a machine learning model."
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-the-problem-it-addresses",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-the-problem-it-addresses",
    "title": "Deep Learning Optimizations II",
    "section": "28 4) Dropout: the problem it addresses",
    "text": "28 4) Dropout: the problem it addresses\n\n\n\n\nSlide 31"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-why-does-it-work",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-why-does-it-work",
    "title": "Deep Learning Optimizations II",
    "section": "29 4) Dropout: why does it work?",
    "text": "29 4) Dropout: why does it work?\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-why-does-it-work-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-why-does-it-work-1",
    "title": "Deep Learning Optimizations II",
    "section": "30 4) Dropout: why does it work?",
    "text": "30 4) Dropout: why does it work?\n\n\n\n\nSlide 33"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-how-is-it-implemented",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-how-is-it-implemented",
    "title": "Deep Learning Optimizations II",
    "section": "31 4) Dropout: how is it implemented?",
    "text": "31 4) Dropout: how is it implemented?\n\n\n\n\nSlide 34\n\n\n\n\nYou switch the activations to 0, and now say with Bernulli you have 50% neurons working\nDuring testing you are not learning so you use all the neurons\n\nNow with Dropout you cannot have neurons that are inactive, because you drop all other neurons so now they need to work\nDecreases overfitting"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout",
    "title": "Deep Learning Optimizations II",
    "section": "32 Dropout",
    "text": "32 Dropout\n\n\n\n\nSlide 35"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-1",
    "title": "Deep Learning Optimizations II",
    "section": "33 Dropout",
    "text": "33 Dropout\n\n\n\n\nSlide 36"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-2",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-2",
    "title": "Deep Learning Optimizations II",
    "section": "34 Dropout",
    "text": "34 Dropout\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-3",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-3",
    "title": "Deep Learning Optimizations II",
    "section": "35 Dropout",
    "text": "35 Dropout\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-4",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-4",
    "title": "Deep Learning Optimizations II",
    "section": "36 Dropout",
    "text": "36 Dropout\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-5",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-5",
    "title": "Deep Learning Optimizations II",
    "section": "37 Dropout",
    "text": "37 Dropout\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-us.-bagging",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-us.-bagging",
    "title": "Deep Learning Optimizations II",
    "section": "38 Dropout US. Bagging",
    "text": "38 Dropout US. Bagging\n\n\n\n\nSlide 41\n\n\n\n\nBagging, also known as bootstrap aggregation, is the ensemble learning method that is commonly used to reduce variance within a noisy dataset. It is train in parallel\n\n\n\n\nBagging:\n\nHas its corresponding training set that is different from the whole training set\nUses all neurons\n\nDropout:\n\nIt does not employ all neurons\nThey are not trained they only get one SGD because you have many infinitely subnetworks, so if you apply dropout it is very unlikely that you trian the same subentwork many times"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-beyond-bagging",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-beyond-bagging",
    "title": "Deep Learning Optimizations II",
    "section": "39 Dropout beyond Bagging",
    "text": "39 Dropout beyond Bagging\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation",
    "title": "Deep Learning Optimizations II",
    "section": "40 5) Data augmentation",
    "text": "40 5) Data augmentation\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation-1",
    "title": "Deep Learning Optimizations II",
    "section": "41 Data augmentation",
    "text": "41 Data augmentation\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#one-note-about-backtranslation-though",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#one-note-about-backtranslation-though",
    "title": "Deep Learning Optimizations II",
    "section": "42 One note about backtranslation though:",
    "text": "42 One note about backtranslation though:\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#common-computer-vision-augmentations-visualised",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#common-computer-vision-augmentations-visualised",
    "title": "Deep Learning Optimizations II",
    "section": "43 Common computer vision augmentations visualised",
    "text": "43 Common computer vision augmentations visualised\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation-2",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation-2",
    "title": "Deep Learning Optimizations II",
    "section": "44 Data augmentation",
    "text": "44 Data augmentation\n\n\n\n\nSlide 47"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation-3",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-augmentation-3",
    "title": "Deep Learning Optimizations II",
    "section": "45 Data augmentation",
    "text": "45 Data augmentation\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#other-regularizations",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#other-regularizations",
    "title": "Deep Learning Optimizations II",
    "section": "46 Other regularizations",
    "text": "46 Other regularizations\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#various-ways-to-regularise",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#various-ways-to-regularise",
    "title": "Deep Learning Optimizations II",
    "section": "47 Various ways to regularise",
    "text": "47 Various ways to regularise\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#normalization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#normalization",
    "title": "Deep Learning Optimizations II",
    "section": "48 Normalization",
    "text": "48 Normalization\n\n\n\n\nSlide 51\n\n\n\n\nPutting data into a common shape without distorting tis shape"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-preprocessing",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#data-preprocessing",
    "title": "Deep Learning Optimizations II",
    "section": "49 Data preprocessing",
    "text": "49 Data preprocessing\n\n\n\n\nSlide 52\n\n\n\n\nHere basically if we have in same scale then our weights will not be elongated like in the elipse, now they would be able to take the same step size in the correct direction"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#normalizing-input-data",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#normalizing-input-data",
    "title": "Deep Learning Optimizations II",
    "section": "50 Normalizing Input Data",
    "text": "50 Normalizing Input Data\n\n\n\n\nSlide 53\n\n\n\n\nThis we apply in the input stage:\nNormalization is a linear operator so you can put this back into the NN after you have trained if you want to"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#normalizing-intermediate-layers",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#normalizing-intermediate-layers",
    "title": "Deep Learning Optimizations II",
    "section": "51 Normalizing intermediate layers",
    "text": "51 Normalizing intermediate layers\n\n\n\n\nSlide 54\n\n\n\n\nHere we talked about the normalization within the NN"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization",
    "title": "Deep Learning Optimizations II",
    "section": "52 Batch normalization",
    "text": "52 Batch normalization\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization-1",
    "title": "Deep Learning Optimizations II",
    "section": "53 Batch normalization",
    "text": "53 Batch normalization\n\n\n\n\nSlide 56\n\n\n\n\n\n\n\n\n\n\nHow does batch normalization works?\n\n\n\n\n\nBatch Normalization (BatchNorm) is a technique used in neural networks to improve the training stability and speed by normalizing the inputs of each layer. It was introduced by Sergey Ioffe and Christian Szegedy in their paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.”\nHere’s a high-level overview of how Batch Normalization works:\n\n53.1 Steps of Batch Normalization:\n\nNormalization:\n\nFor each mini-batch during training, normalize the input by subtracting the mean and dividing by the standard deviation. The normalization is applied independently to each feature (dimension) in the input.\n\n\\(\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\)\nWhere:\n\n\\(\\hat{x}^{(k)}\\) is the normalized output for the k-th feature.\n\\(x^{(k)}\\) is the input for the k-th feature.\n\\(\\mu\\) is the mean of the mini-batch.\n\\(\\sigma^2\\) is the variance of the mini-batch.\n\\(\\epsilon\\) is a small constant added for numerical stability.\n\nScale and Shift:\n\nIntroduce learnable parameters (scale and shift) for each feature to allow the model to adapt during training.\n\n\\(y^{(k)} = \\gamma \\hat{x}^{(k)} + \\beta\\)\nWhere:\n\n\\(y^{(k)}\\) is the final output for the k-th feature.\n\\(\\gamma\\) is a learnable scale parameter.\n\\(\\beta\\) is a learnable shift parameter.\n\nTraining and Inference:\n\nDuring training, the mean and variance are computed for each mini-batch and used for normalization.\nDuring inference, running averages of mean and variance from the training phase are typically used for normalization to ensure consistency.\n\n\n\n\n53.2 Benefits of Batch Normalization:\n\nImproved Training Stability:\n\nHelps mitigate the internal covariate shift problem, leading to more stable and faster convergence during training.\n\nReduced Sensitivity to Initialization:\n\nReduces the sensitivity of the model to the choice of initial weights.\n\nAllows Higher Learning Rates:\n\nEnables the use of higher learning rates, which can accelerate training.\n\nActs as Regularization:\n\nIntroduces a slight regularization effect, reducing the need for other regularization techniques.\n\nApplicability to Various Architectures:\n\nCan be applied to various types of neural network architectures, including fully connected layers, convolutional layers, and recurrent layers.\n\n\nBatch Normalization has become a standard component in many deep learning architectures due to its effectiveness in improving training stability and convergence speed."
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization-the-algorithm",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization-the-algorithm",
    "title": "Deep Learning Optimizations II",
    "section": "54 Batch normalization — The algorithm",
    "text": "54 Batch normalization — The algorithm\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#how-does-batch-normalization-help-optimization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#how-does-batch-normalization-help-optimization",
    "title": "Deep Learning Optimizations II",
    "section": "55 How does batch normalization help optimization?",
    "text": "55 How does batch normalization help optimization?\n\n\n\n\nSlide 58\n\n\n\n\nBecause some layers will push ouputs in one direction, then other layers will not use this. With batch norm we centered data so that all layers train around these centered inputs"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#how-does-batch-normalization-help-optimization-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#how-does-batch-normalization-help-optimization-1",
    "title": "Deep Learning Optimizations II",
    "section": "56 How does batch normalization help optimization?",
    "text": "56 How does batch normalization help optimization?\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#benefits-of-batch-normalization-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#benefits-of-batch-normalization-1",
    "title": "Deep Learning Optimizations II",
    "section": "57 Benefits of Batch normalization",
    "text": "57 Benefits of Batch normalization\n\n\n\n\nSlide 60\n\n\n\n\n\nTrain faster because all layers train similarly quickly\nAllows you to have high learning rates because you wont have vanishing or exploding gradients because everything is 0-1 distributed\nMakes weights easier to initialize, because you know everything will be between 0-1\nMake activations function sensible because all the activation functions have something special about zero\nHave added noise that comes from estimating the batch statistics, any noise that may help is regularization. Here the noise reduces overfitting and that acts as a regularization so that your model does not overfit.\nPut it simple: Noise Disrupts Patterns"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#quiz-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#quiz-1",
    "title": "Deep Learning Optimizations II",
    "section": "58 Quiz",
    "text": "58 Quiz\n\n\n\n\nSlide 61\n\n\n\n\nThere is no answer here"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization-at-test-time",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-normalization-at-test-time",
    "title": "Deep Learning Optimizations II",
    "section": "59 Batch normalization at test time",
    "text": "59 Batch normalization at test time\n\n\n\n\nSlide 62\n\n\n\n\nThe important thing is that when you go to test time, you dont have batches (you dont want anything that is depended on how you construct the batch) because that means if you take another batch is not the same which then is not reproducible.\nSo what we usually do is keep a moving average of the mean and variance during training, and then at test time you plug them\nBasically you extract the mean and the variance form the training and use it in test data"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#disadvantages-of-batch-normalization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#disadvantages-of-batch-normalization",
    "title": "Deep Learning Optimizations II",
    "section": "60 Disadvantages of batch normalization",
    "text": "60 Disadvantages of batch normalization\n\n\n\n\nSlide 63\n\n\n\n\n\nIt requires you to have large batch sizes because otherwhise the estimate of mean and variance is too noisy\nProblematic if you have discrepancy of the training and tst data\nNow the loss you get from training sample A, depends on what other training sample are present in the batch via this normalization of the mean and variance\nOne disadvantage is that is usually the reason for bugs, because if you keep estimating the mean and variances for the test data but now it is not reproducible because it will depend on the batch itself"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#disadvantages-of-batch-normalization-with-distributed-training",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#disadvantages-of-batch-normalization-with-distributed-training",
    "title": "Deep Learning Optimizations II",
    "section": "61 Disadvantages of batch normalization with distributed training",
    "text": "61 Disadvantages of batch normalization with distributed training\n\n\n\n\nSlide 64\n\n\n\n\n\nDifferent values across GPU\nIf you batch size is small in a single GPU, but maybe you have 10 GPUs running and for 10 GPUs your batch size is bigger, it will be a stupid idea to estimate 10 very noisy estimate of the mean and variance but instead you should compute across the GPU"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#layer-normalization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#layer-normalization",
    "title": "Deep Learning Optimizations II",
    "section": "62 Layer normalization",
    "text": "62 Layer normalization\n\n\n\n\nSlide 65\n\n\n\n\nHere mean and variance are not computed across batch but across all channels and spatial dimensions.\nSo now the statistics are independent of the batch size because now they depend on the feature dimensions see example below:\n\n\n\n\n\n\nExample Layer normalization\n\n\n\n\n\nLayer Normalization is a normalization technique similar to Batch Normalization but operates on a per-sample basis rather than per-minibatch. It normalizes the inputs of a layer across the features (dimensions) for each individual sample. Here’s an example of how Layer Normalization is typically applied:\n\nimport torch\nimport torch.nn as nn\n\n# Assuming input has shape (batch_size, num_features)\ninput_data = torch.randn(32, 64)\n\n# Layer Normalization\nlayer_norm = nn.LayerNorm(normalized_shape=64)\noutput = layer_norm(input_data)\n\n# Display input and output shapes\nprint(\"Input shape:\", input_data.shape)\nprint(\"Output shape:\", output.shape)\n\nInput shape: torch.Size([32, 64])\nOutput shape: torch.Size([32, 64])\n\n\nIn this example:\n\ninput_data is a random tensor with shape (32, 64), representing a batch of 32 samples, each with 64 features.\nnn.LayerNorm is the Layer Normalization layer provided by PyTorch. The normalized_shape parameter specifies the number of features in the input tensor.\nThe output tensor is the result of applying Layer Normalization to the input data.\n\nLayer Normalization normalizes the values along the feature dimension independently for each sample. This means that each feature in a sample is normalized based on its mean and standard deviation across the entire sample, rather than across a batch as in Batch Normalization.\nLayer Normalization is useful when the batch size is small or when working with sequences of varying lengths, as it normalizes each sample independently. It has been widely used in natural language processing tasks and recurrent neural networks.\n\n\n\nBasically, we have that our mean and std will be computed across dimensions, so on the columns, not on each input. So then each sample (so each row) we make it with this mean and std to be distributed between 0 and 1.\nThis is great for RNN, or other stuff that requires small batch sizes.\nHere the same operations happens at training and test time.\nSo now instead of normalizing across data samples now we basically i.e. if the input is an image that each color should be roughly be ocurring the same amount of spread across all the image, because now we normalize it across channel dimensions"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#layer-normalization-ln",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#layer-normalization-ln",
    "title": "Deep Learning Optimizations II",
    "section": "63 Layer normalization (LN)",
    "text": "63 Layer normalization (LN)\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#instance-normalization-in",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#instance-normalization-in",
    "title": "Deep Learning Optimizations II",
    "section": "64 Instance normalization (IN)",
    "text": "64 Instance normalization (IN)\n\n\n\n\nSlide 67\n\n\n\n\nInstance normalization now you do layer normalization but per channel and per training example.\nSo now the network should be agnostic to the constract of the original iamge and of the constrast whithin the channels\nNot used that often\nHere we compute the mean and var per sample but not per channel.\n\n\n\n\n\n\nHow Instance Normalization works\n\n\n\n\n\nInstance Normalization is a normalization technique similar to Batch Normalization and Layer Normalization but operates on a per-instance basis. It normalizes the activations of each individual sample independently. Here’s an explanation of how Instance Normalization works:\n\n64.1 Instance Normalization Steps:\n\nInput Tensor:\n\nAssume you have an input tensor \\(X\\) with shape \\((N, C, H, W)\\), where:\n\n\\(N\\) is the batch size.\n\\(C\\) is the number of channels.\n\\(H\\) is the height of the feature map.\n\\(W\\) is the width of the feature map.\n\n\nCalculate Mean and Variance:\n\nFor each instance (sample) in the batch, calculate the mean \\(\\mu\\) and variance \\(\\sigma^2\\) along each channel independently. This is done for each channel and each instance separately.\n\n\\[\\mu_c = \\frac{1}{H \\cdot W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} X_{n,c,i,j}\\]\n\\[\\sigma^2_c = \\frac{1}{H \\cdot W} \\sum_{i=1}^{H} \\sum_{j=1}^{W} (X_{n,c,i,j} - \\mu_c)^2\\]\nNormalize:\n\nNormalize each channel of each instance independently using the calculated mean and standard deviation:\n\n\\[\\hat{X}_{n,c,i,j} = \\frac{X_{n,c,i,j} - \\mu_c}{\\sqrt{\\sigma^2_c + \\epsilon}}\\]\nWhere:\n\n\\(\\hat{X}_{n,c,i,j}\\) is the normalized value.\n\\(X_{n,c,i,j}\\) is the original input value.\n\\(\\mu_c\\) is the mean of the channel \\(c\\) for the instance \\(n\\).\n\\(\\sigma^2_c\\) is the variance of the channel \\(c\\) for the instance \\(n\\).\n\\(\\epsilon\\) is a small constant added for numerical stability.\n\nScale and Shift:\n\nIntroduce learnable scale (\\(\\gamma\\)) and shift (\\(\\beta\\)) parameters for each channel:\n\n\\[Y_{n,c,i,j} = \\gamma_c \\hat{X}_{n,c,i,j} + \\beta_c\\]\nWhere:\n\n\\(Y_{n,c,i,j}\\) is the final normalized output.\n\\(\\gamma_c\\) is a learnable scale parameter for channel \\(c\\).\n\\(\\beta_c\\) is a learnable shift parameter for channel \\(c\\).\n\n\n\n\n64.2 Benefits of Instance Normalization:\n\nNormalization Across Samples:\n\nInstance Normalization normalizes each instance independently, making it suitable for scenarios where batch sizes may vary or are small.\n\nReduces Covariate Shift:\n\nSimilar to Batch Normalization, Instance Normalization helps reduce internal covariate shift, leading to more stable training.\n\nApplicability to Style Transfer:\n\nInstance Normalization has found applications in style transfer tasks in computer vision.\n\n\nInstance Normalization is often used in computer vision tasks, especially in scenarios where the batch size may be small or when normalization across instances is desired."
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#group-normalization-gn",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#group-normalization-gn",
    "title": "Deep Learning Optimizations II",
    "section": "65 Group normalization (GN)",
    "text": "65 Group normalization (GN)\n\n\n\n\nSlide 68\n\n\n\n\nWe are gonna group different certain channels toguether. So we are not normalizing per channel but per groups, for instance 5 channels.\n\nIf you have only one group then you recover layer nomalization because layer normal normalizes across all channels and not per channel\nIf you have more number of channel group toguether i.e 3 channels group together then you do Instance normalization. meaning you compute the mean per sample across each channel\n\n\nIn Grouped Convs, you are basically separating the hidden layers or the hidden channels in hidden groups which makes computations more easier\nThis is better than batch normalization for small batches &lt;32"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#a-comparison-of-different-normalizations",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#a-comparison-of-different-normalizations",
    "title": "Deep Learning Optimizations II",
    "section": "66 A comparison of different normalizations",
    "text": "66 A comparison of different normalizations\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#weight-normalization",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#weight-normalization",
    "title": "Deep Learning Optimizations II",
    "section": "67 Weight normalization",
    "text": "67 Weight normalization\n\n\n\n\nSlide 70\n\n\n\n\nYou can thing the weights like a vector, you can have its magnitude and its direction. Now with this g can learn this parameter which tells you how long you want to go in that direction\n\n\n\n\nSlide 72\n\n\n\n\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#learning-rate",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#learning-rate",
    "title": "Deep Learning Optimizations II",
    "section": "68 Learning rate",
    "text": "68 Learning rate\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#convergence",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#convergence",
    "title": "Deep Learning Optimizations II",
    "section": "69 Convergence",
    "text": "69 Convergence\n\n\n\n\nSlide 75\n\n\n\n\nTo achieve convergence you need tehse two equations:\n\nall learning over time should be infinitely to allow for exploration\nThe quadratic term should be less than infite to converge"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#learning-rate-schedules",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#learning-rate-schedules",
    "title": "Deep Learning Optimizations II",
    "section": "70 Learning rate schedules",
    "text": "70 Learning rate schedules\n\n\n\n\nSlide 76\n\n\n\n\nYou also make a warmpup learning rate so you start by going up in a linear fashion"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#in-practice",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#in-practice",
    "title": "Deep Learning Optimizations II",
    "section": "71 In practice",
    "text": "71 In practice\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#quiz-2",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#quiz-2",
    "title": "Deep Learning Optimizations II",
    "section": "72 Quiz",
    "text": "72 Quiz\n\n\n\n\nSlide 78\n\n\n\n\nWe do all of the above,\n\nIt is good that if the loss explode you check the individual values of the gradients,\nor was the batch size was so small that something could have affect it\nor was the learning rate too high that even a small batch it was not too but but the large learning rate was too big tha then it end up in a completely broken spot in the NNs"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-rate",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#dropout-rate",
    "title": "Deep Learning Optimizations II",
    "section": "73 Dropout rate",
    "text": "73 Dropout rate\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-size",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#batch-size",
    "title": "Deep Learning Optimizations II",
    "section": "74 Batch size",
    "text": "74 Batch size\n\n\n\n\nSlide 80"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#designing-cnns-to-become-even-better.-dont-try-this-at-home",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#designing-cnns-to-become-even-better.-dont-try-this-at-home",
    "title": "Deep Learning Optimizations II",
    "section": "75 Designing CNNs to become even better. (Don’t try this at home)",
    "text": "75 Designing CNNs to become even better. (Don’t try this at home)\n\n\n\n\nSlide 81\n\n\n\n\nArchitecture, which model do you use?"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#number-of-layers-and-neurons",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#number-of-layers-and-neurons",
    "title": "Deep Learning Optimizations II",
    "section": "76 Number of layers and neurons",
    "text": "76 Number of layers and neurons\n\n\n\n\nSlide 82\n\n\n\n\nProgress is not only in the architecture side, for example if you want to develop better algorithms, you want to have just a neural network which can identify between cats and dogs, there is multiple ways:\n\nCome up with a new architecture\nCome up with better gradients and better weights of the NNs"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#babysitting-deep-nets",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#babysitting-deep-nets",
    "title": "Deep Learning Optimizations II",
    "section": "77 Babysitting Deep Nets",
    "text": "77 Babysitting Deep Nets\n\n\n\n\nSlide 83\n\n\n\n\nfor classifying 8 classes, the loss should be -log(1/8) and you check whether is true, if it gets worst performance that randomly guessing so something is wrong"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#logging-tools",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#logging-tools",
    "title": "Deep Learning Optimizations II",
    "section": "78 Logging tools",
    "text": "78 Logging tools\n\n\n\n\nSlide 84"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#babysitting-deep-nets-1",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#babysitting-deep-nets-1",
    "title": "Deep Learning Optimizations II",
    "section": "79 Babysitting Deep Nets",
    "text": "79 Babysitting Deep Nets\n\n\n\n\nSlide 85\n\n\n\n\nLink1"
  },
  {
    "objectID": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#reading-material",
    "href": "blog/2023-11-20_deep-learning-optimizations-ii/index.html#reading-material",
    "title": "Deep Learning Optimizations II",
    "section": "80 Reading material",
    "text": "80 Reading material\n\n\n\n\nSlide 86"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html",
    "title": "Classification and Decision Theory",
    "section": "",
    "text": "Notes for the course at UvA: Machine Learning 1\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Classification\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Classification\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 25, 2023\nThis section focus on third week of the course. For Regression continue to this post."
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#classification-through-decision-regions",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#classification-through-decision-regions",
    "title": "Classification and Decision Theory",
    "section": "1 Classification through decision regions",
    "text": "1 Classification through decision regions\n\nHere the targets can take only discrete values. In Linear Regression the targets were continuous\n\n\\[\n\\begin{align}\n\\underline{x} &\\in \\mathbb{R}^{Dx1}, \\text{ $D$ Dimensional Space}\\\\\n&= [x_1, .., x_D]^T \\nonumber\n\\end{align}\n\\]\n\nFor instance when \\(D=2\\) we can think of \\(x_1\\) as the amount of black pixels in the image, and \\(x_2\\) as the white pixels. Then I can clasify one image into this 2-D dimensional space. So in the xy-plane one image has \\((x1,x2)\\) coordinates\nWe dive \\(\\underline{x}\\) into \\(K\\) Decision Regions \\(R_k\\).\nFor each Decision Region \\(R_k\\) we assign it to a class \\(C_k\\).\nThe target \\(\\underline{t} \\in \\{C_1,...,C_k\\}\\) meaning the target can be classified either \\(C_1\\) or the target can be classified as \\(C_2\\)"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#linear-classification",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#linear-classification",
    "title": "Classification and Decision Theory",
    "section": "2 Linear Classification",
    "text": "2 Linear Classification\n\nNote: do not confuse \\(D\\) the amount of data points with this new \\(D\\) where we talk about the dimensionality of how each data point is represented (the coordinates)\n\n\nThe classification is done by only linear decision boundaries\n\nFor \\(D\\)-dimensional input space: \\(\\underline{x}\\in\\mathbb{R}^{D}\\). The decision surface is a \\(D-1\\) dimensional hyperplane. For instance:\n\nThe decision boundaries can take a form of a line, i.e when \\(\\underline{x}\\in\\mathbb{R}^{D=2x1}\\) meaning the dots are drawn in the \\(x,y\\) coordinates\nThe decision boundaries can take a form of a plane, i.e when \\(\\underline{x}\\in\\mathbb{R}^{D=3x1}\\) meaning the dots are drawn in the \\(x,y,z\\) coordinates\n\nLinear Classifiers have however some constraints: one-versus-one, or one-vs-rest cannot classified in one specific region when majority vote its applied. There is class of decisions"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#decision-theory",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#decision-theory",
    "title": "Classification and Decision Theory",
    "section": "3 Decision Theory",
    "text": "3 Decision Theory\nHere we talked about when we consider a classifier (a model) a good classifier.\n\nWe talk about the Bayes Error Rate\nHow to minimize the misclassification rate\n\nModel that you start with:\n\nClass-conditional densities: \\(p(\\underline{x}|C_k)\\) aka (Likelihood)\nPrior class probabilities: \\(p(C_k)\\)\n\nFrom these two you can derive:\n\nThe Joint distribution \\(p(\\underline{x}, C_k)\\)\n\n\\[\n\\begin{align}\np(\\underline{x}, C_k)=p(\\underline{x}|C_k)p(C_k)\n\\end{align}\n\\]\n\nThe Posterior \\(p(C_k|\\underline{x})\\).\n\n\\[\n\\begin{align}\np(C_k|\\underline{x}) = \\frac{p(\\underline{x}| C_k)p(C_k)}{p(\\underline{x})}\n\\end{align}\n\\]\n\nDecision Theory tell us that the best prediction for input \\(\\underline{x}\\) is to choose the class with highest joint \\(p(\\underline{x}, C_k)\\)\nOr equivalently: choose class with the highest posterior \\(p(C_k|\\underline{x})\\)\nDecision boundary between \\(C_k\\) and \\(C_j\\) are at \\(p(C_k | \\underline{x})=p(C_j | \\underline{x})\\)"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#from-bayes-rule-to-bayes-classifiers",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#from-bayes-rule-to-bayes-classifiers",
    "title": "Classification and Decision Theory",
    "section": "4 From Bayes Rule to Bayes Classifiers",
    "text": "4 From Bayes Rule to Bayes Classifiers\n\\[\n\\begin{align}\np(C|X) = \\frac{P(X|C)P(C)}{P(X)} \\\\\n\\end{align}\n\\]\nWhere:\n\n\\(P(C|X)\\) is the Posterior (Given the data what is the prob of being class C_k)\n\\(P(X|C)\\) is the Likelihood (How the data is distributed given C)\n\\(P(C)\\) is the Prior\n\\(P(X)\\) is the Evidence/ Marginal likelihood\n\nThe evidence \\(P(X)\\) can also be decomposed in:\n\\[\n\\begin{align}\nP(X) &= \\sum_{j}{}P(X,C_j)\\\\\n     &= \\sum_{j}{}P(X|C_j)P(C_j) \\\\\n\\end{align}\n\\]\nUnlike regression, I will have one likelihood per each class, mainly:\n\\[\n\\begin{align}\np(X|C_0) \\quad \\text{and} \\quad p(X|C_1) \\\\\n\\end{align}\n\\]\nFor instance for \\(K=2\\) two classes. This means per each class we are going to have our own model\nThe decicion boundary then becomes equal when both likelihoods are equal:\n\\[\n\\begin{align}\np(C_0|X) &= p(C_1|X) \\quad \\text{Decision Boundary}\\\\\n% P(X, C_0) &= p(X, C_1) \\nonumber\n\\end{align}\n\\]\nHere:\n\n\\(P(C_0|X)\\) is the Posterior probability"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#types-of-classification",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#types-of-classification",
    "title": "Classification and Decision Theory",
    "section": "5 Types of Classification",
    "text": "5 Types of Classification\n\nDecision Trees\nLogistic Regressions\nBayes Classifiers (Generative): first fit P(x|C) and then use Bayes Rule to flip it and tell which type of class correspond \\(x\\)\n\nNaive Bayes: you put an x and it gives you to which class \\(K\\) correponds\nGaussian Likelihood: here \\(p(x|C_k) = N(\\mu_k, \\Sigma_k)\\)\nQDA: Quadratic discriminant analysis\n\nShared Parameters\nLDA: Linear Discriminant Analysis\n\n\n\n\n5.1 Gaussian Generative Classifiers\nHere we assume arbitrary covariance matrices for each class\n\\[\n\\begin{align}\np(x|C_k) = N(\\mu_k, \\Sigma_k)\n\\end{align}\n\\]\n\nEach class \\(K\\) is going to have its normal distribution. And each of these distributions would be completely different from the others.\n\n\nQuadratic Discriminant Analysis\nHere the decision boundary would be quadratic.\nHere the covariances would different for each class, like so:\n\\[\n\\begin{align}\n1&=\\frac{p(x, C_2)}{p(x, C_1)} \\\\\n&=\\frac{p(x| C_2)p(C_2)}{p(x| C_1)p(C_1)} \\nonumber\n\\end{align}\n\\]\nTaking the log at both sides\n\\[\n\\begin{align}\n0 &= log N(\\mu_2, \\Sigma_2) - log N(\\mu_1, \\Sigma_1) + log \\frac{p(C_2)}{p(C_1)}\n\\end{align}\n\\]\nIf you solve the equation above then you end up with quadratic terms.\nThis tell us that now we can handle non-linear separate cases now the data can needs to be quadratic separable.\n\n\n5.1.1 Shared Parameters\n\n\nLinear Discriminant Analysis\nHere the decision boundary would be linear.\nHere the covariances would be the same: 1 covariance matrix like so:\n\\[\n\\begin{align}\n0 &= log N(\\mu_2, \\Sigma) - log N(\\mu_1, \\Sigma) + log \\frac{p(C_2)}{p(C_1)}\n\\end{align}\n\\]\nHere we do not impose \\(diag\\{\\Sigma_k\\}\\), we can have Naive Bayes approach here. The latter meaning we can indeeed if we want have the covariance matrices to be \\(diag\\{\\Sigma_k\\}\\). In this called we called. Naive Bayes applied to LDA.\n\nNaive Bayes applied to LDA: same/shared parameters for \\(\\Sigma_k\\) and \\(diag\\{\\Sigma_k\\}\\)\n\n\n\n\nUnderstanding Covariance and Variance\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nimport matplotlib.colorbar as cbar\n\n# Define the mean and covariance matrix for the original case\nmean = [0, 0]\ncovariance_matrix = [[2, 1], [1, 2]]  # Example covariance matrix\n\n# Create a grid of points\nx, y = np.meshgrid(np.linspace(-5, 5, 100), np.linspace(-5, 5, 100))\npos = np.dstack((x, y))\n\n# Create a multivariate Gaussian distribution for the original case\nrv = multivariate_normal(mean, covariance_matrix)\n\n# Calculate the probability density at each point in the grid for the original case\npdf = rv.pdf(pos)\n\n# Calculate eigenvalues and eigenvectors for the original case\neigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n# Define the scale factor for the arrows\nscale_factor = 2.0\n\n# Create the first subplot for the original case\nplt.subplot(1, 2, 1)\nplt.contourf(x, y, pdf, cmap='viridis')\nfor i in range(2):\n    plt.arrow(\n        mean[0],\n        mean[1],\n        eigenvectors[i, 0] * np.sqrt(eigenvalues[i]) * scale_factor,\n        eigenvectors[i, 1] * np.sqrt(eigenvalues[i]) * scale_factor,\n        head_width=0.2,\n        head_length=0.2,\n        fc='r',\n        ec='r',\n    )\nplt.annotate(f'Var(X) = {eigenvalues[0]:.2f}', xy=(-3, 3), color='white')\nplt.annotate(f'Var(Y) = {eigenvalues[1]:.2f}', xy=(-3, 2), color='white')\nplt.annotate(f'Cov(X, Y) = {covariance_matrix[0][1]:.2f}', xy=(-3, 1), color='white')\nplt.title('Original Case (Covariance ≠ 0)')\n\n# Define the mean and covariance matrix for the equal variance case\nequal_variance_cov_matrix = np.diag([2, 2])  # Equal variance along both dimensions\n\n# Create a multivariate Gaussian distribution for the equal variance case\nrv_equal_variance = multivariate_normal(mean, equal_variance_cov_matrix)\n\n# Calculate the probability density at each point in the grid for the equal variance case\npdf_equal_variance = rv_equal_variance.pdf(pos)\n\n# Create the second subplot for the equal variance case\nplt.subplot(1, 2, 2)\nplt.contourf(x, y, pdf_equal_variance, cmap='viridis')\nplt.annotate(f'Var(X) = {equal_variance_cov_matrix[0, 0]:.2f}', xy=(-3, 3), color='white')\nplt.annotate(f'Var(Y) = {equal_variance_cov_matrix[1, 1]:.2f}', xy=(-3, 2), color='white')\nplt.title('Equal Variance Case (Covariance = 0)')\n\n# Set common labels\nfor ax in plt.gcf().axes:\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n\n# Ensure equal aspect ratio for both subplots\nfor ax in plt.gcf().axes:\n    ax.set_aspect('equal', adjustable='box')\n\n# Create a new axes for the legend with adjusted width\ncax = plt.gcf().add_axes([0.96, 0.3, 0.02, 0.4])  # Adjust the width and position as needed\n\n# Plot vertical colorbar for the legend\ncbar.ColorbarBase(cax, cmap='viridis', orientation='vertical', label='Probability Density')\n\n# Adjust the overall layout\nplt.subplots_adjust(wspace=0.3)\n\nplt.show()\n\n\n\n\n\nThis plot visually illustrates how variance represents the spread along each dimension, and the arrows depict how the covariance matrix encodes the relationships between the dimensions in a Gaussian distribution.\n\nOriginal Case (Covariance ≠ 0):\n\n\nIn the first plot (Original Case), the color yellow represents regions where the probability density is higher. In a Gaussian distribution, the probability density is highest at the mean (center) of the distribution and decreases as you move away from the mean. The color yellow typically corresponds to higher probability values in this context.\n\n\nEqual Variance Case (Covariance = 0):\n\n\nIn the second plot (Equal Variance Case), the color yellow also represents regions of higher probability density. Even though the covariance is zero, meaning there is no linear relationship between the X and Y dimensions, the multivariate Gaussian distribution still has a peak at the mean (center) in each dimension. The color yellow again corresponds to higher probability values in this context.\n\n\n\n\n\nQDA: they have separate Covariances\nLDA: they share a non-zero covariance\nHere the shared variances mean for example 3 in the y-direction and 1 in the x-direction, they however contain a non-zero covariance\nHere the shared variances mean for example 1 in the y-direction and 1 in the x-direction, they however contain a zero covariance."
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#probabilistic-generative-models-k2",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#probabilistic-generative-models-k2",
    "title": "Classification and Decision Theory",
    "section": "6 Probabilistic Generative Models \\(K=2\\)",
    "text": "6 Probabilistic Generative Models \\(K=2\\)\n\\[\n\\begin{align}\nP(C_1|x) &= \\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)p(x|C_2)p(C_2)}\\\\\n&= \\frac{1}{1+e^-a}\\nonumber\\\\\n\\end{align}\n\\]\nWhere \\(a\\) is the log odds:\n\\[\n\\begin{align}\na &= \\ln\\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}\\label{log_odds}\\\\\n\\end{align}\n\\]\nThis can be expressed as the Sigmoid Function:\n\\[\n\\begin{align}\n\\sigma &= \\frac{1}{1+e^{(-a)}}\\\\\n\\end{align}\n\\]\n\nWhen the log odds its possitive in the sigmud function it will converge to 1. This means I am certain it will be class \\(C_1\\)\nIf the log odds is equal meaning not clue which class to assign. The probability of classifing the target to either class is 0.5.\n\nFor \\(a\\) to be equal to zero. I need \\(a=\\ln(1)\\). This 1 means \\(p(x|C_1)p(C_1)=p(x|C_2)p(C_2)\\)\n\nWhen the log odds its negative, the sigmoid function will go to zero. This means I am certain it will be class \\(C_2\\)"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#probabilistic-generative-models-generalk",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#probabilistic-generative-models-generalk",
    "title": "Classification and Decision Theory",
    "section": "7 Probabilistic Generative Models, general:\\(K\\)",
    "text": "7 Probabilistic Generative Models, general:\\(K\\)\n\n\n\n\n\n\nRemember\n\n\n\nYou are given:\n\nClass-conditional densities: \\(p(x|C_k)\\) aka Likelihood\nPrior class probabilities: \\(p(Ck)\\)\n\nWith these two guys you can get:\n\nJoint Distribution: \\(p(x,C_k) = p(x|C_k)p(C_k)\\) the numerator\nThe Posterior \\(p(C_k|x)=\\frac{p(x|C_k)p(C_k)}{p(x)}\\)\n\nGoal:\n\nFind \\(p(C_k|x)\\) so that you can determine the class of \\(x\\) by knowing the Decision boundaries\n\n\n\n\n\\[\n\\begin{align}\nP(C_k|x) &= \\frac{p(x|C_k)p(C_k)}{\\sum_{j=1}^{K}p(x|C_j)p(C_j)}\\\\\n&= \\frac{e^{a_k}}{\\sum_{j=1}^{K}e^{a_j}}\\label{softmax}\\\\\na_k &= \\ln(p(x|C_k)p(C_k)) \\nonumber\n\\end{align}\n\\]\nWhere \\(\\ref{softmax}\\) is called the Softmax:\n\nif \\(a_k&gt;&gt;a_j\\) for all \\(j \\neq k\\) then \\(p(C_k|x)=1\\) and \\(p(C_j|x)=0\\)\nThe Softmax reduces to the Simoid function when \\(K=2\\)\n\n\n7.0.1 How to parametrize Class Conditional Densities (aka The Likelihood)?\nWith Gaussians!\n\\[\n\\begin{align}\np(x|C_k)&=\\mathcal{N}(x|\\mu_k , \\Sigma_k ) \\\\\n        &= \\frac{1}{(2 \\pi)^{D/2}} \\frac{1}{|\\Sigma_k|^{1/2}} \\exp^{\\{-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^-1 (x-\\mu_k)\\}} \\label{gaussian_lda}\\\\\n\\end{align}\n\\]\n\nWhere the Gaussian is going to be multivariate and it will be \\(D\\)-dimensional due to the input being \\(x \\in \\mathbb{R}^{D}\\)\n\nThis means for each \\(\\mu_k\\) and \\(\\Sigma_k\\) I would have a different Gaussian distribution\n\n\\(\\Sigma_k\\) determines the shape of my distribution. Like in the python plot above in the left graph\n\nWhen we assume each of these Gaussian share the same the Covariance matrix: \\(\\Sigma_k\\) then we are talking about LDA\nTo determine the decision boundary we have that:\n\\[\n\\begin{align}\np(C_1|X) &= \\frac{1}{1+e^(-a)} = \\sigma(a)\\\\\n\\end{align}\n\\]\nWhere \\(a\\) is defined was defined as the log odds. So replacing \\(p(x|C_k)\\) so replacing \\(\\ref{gaussian_lda}\\) (the Gaussians) in the log odds \\(\\ref{log_odds}\\) give us The Generalized Linear Model:\n\\[\n\\begin{align}\np(C_1|x) &= \\sigma(\\underline{w}^T\\underline{x}+w_o)\\\\\n\\end{align}\n\\]\nAnd now recall that the decision boundary happens when \\(p(C_1|x) = p(C_2|x)\\).\n\nThis all means if we want to make decisions based on the posterior distributions, then \\(a=0\\) meaning the prob/prob is 1 or the \\(\\sigma(a) = \\frac{1}{2}\\)"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#lda-maximum-likelihood-for-k2",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#lda-maximum-likelihood-for-k2",
    "title": "Classification and Decision Theory",
    "section": "8 LDA: Maximum Likelihood for K=2",
    "text": "8 LDA: Maximum Likelihood for K=2\n\nGoal: recover the Gaussian distributions (the join distribution = p(X, C_k)) that have generated the data. To accomplished that we need to take the MLE over the the Gaussian conditional densities aka the likelihood and solve for \\(u_k\\), \\(\\Sigma\\) and priors \\(p(C_k)\\)\n\n\n\n\n\n\n\nIsometric Covariance definition\n\n\n\n\n\nIt is a special case of a covariance matrix where all off-diagonal elements are zero, and the diagonal elements are equal, representing a constant variance or dispersion in all dimensions.\nMathematically, an isometric covariance matrix Σ can be represented as:\n\nΣ = σ² * I\n\nWhere:\n\nΣ is the covariance matrix.\nσ² is the common variance or dispersion parameter.\nI is the identity matrix, which has ones on the diagonal and zeros elsewhere.\n\nIn this form, each element along the diagonal of the covariance matrix Σ is equal to σ², and all off-diagonal elements are zero. This implies that the variables in a multivariate distribution with an isometric covariance matrix have equal variances and are uncorrelated with each other.\n\n\n\nGiven:\n\nGaussian conditional densities aka Likelihoods:\n\n\\[\n\\begin{align}\np(x|C_k) = \\frac{1}{(2 \\pi)^{D/2}} \\frac{1}{|\\Sigma_k|^{1/2}} \\exp^{\\{-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^-1 (x-\\mu_k)\\}} \\nonumber\\\\\n\\end{align}\n\\]\n\nPrior \\(p(C_k)\\)\nBecause we have \\(k=2\\) then we can assign \\(p(C_1) = q\\) and \\(p(C_2) = 1-q\\)\n\nWith 1. and 2. we can solve for the Joint distributions:\nFor \\(x_n\\) with \\(t_n =1\\): \\[\n\\begin{align}\np(x_n, C_1) &= p(x_n|C_1)p(C_1) = q \\, N(x_n|\\mu_1,\\Sigma) \\\\\n\\end{align}\n\\]\nFor \\(x_n\\) with \\(t_n =0\\): \\[\n\\begin{align}\np(x_n, C_2) &= p(x_n|C_2)p(C_2) = (1-q) \\, N(x_n|\\mu_2, \\Sigma)\\\\\n\\end{align}\n\\]\n\n\\(t_n\\) is binary if I do \\(\\sum t_n\\) here I am counting the number of times \\(t_n\\) is equals to 1.\n\n\n8.1 Deriving \\(q_{ML}\\)\n\\[\n\\begin{align}\nq_{ML} &= \\frac{1}{N} \\sum_{n=1}^{N} t_n = \\frac{N_1}{N}\\\\\n\\end{align}\n\\]\n\n\\(q\\) is the prior probability of observing class \\(K=1\\). The result above means the total number of observations that I have observed \\(t_n=1\\)\n\n\n\n8.2 Deriving \\(\\mu{ML}\\)\n\\[\n\\begin{align}\n\\mu_{1,ML} &= \\frac{1}{N_1} \\sum_{n=1}^{N} t_n \\, x_n\\\\\n\\end{align}\n\\]\n\n\\(\\mu_{1,Ml}\\) is the sample mean of all my observations where \\(x_n\\) belongs to class \\(K=1\\). Here $t_n =1 $ when the class is 1.\n\n\\[\n\\begin{align}\n\\mu_{2,ML} &= \\frac{1}{N_2} \\sum_{n=1}^{N} (1-t_n) \\, x_n\\\\\n\\end{align}\n\\]\n\n\\(\\mu_{2,Ml}\\) is the sample mean of all my observations where \\(x_n\\) belongs to class \\(K=2\\)\n\n\n\n8.3 Covariance for discrete Random Variables\nFor class \\(i\\) the covariance matrix can be calculated as: \\[\n\\begin{align}\n\\Sigma_i &= \\frac{1}{N_i}\\sum_{n=1}^{N_i}(x_n-\\mu_i)(x_n-\\mu_i)^T \\\\\n\\end{align}\n\\]\nWhere:\n\n\\(N_i\\) ​is the number of data points in class \\(i\\)\n\\(x_n\\) is a data point in class \\(i\\)\n\\(\\mu_i\\) is the mean vector of class \\(i\\)\n\n\n\n\n\n\n\nHow to classify a new data point?\n\n\n\n\n\n\nGaussian Classifier: Once you have the covariance matrices for each class, you can use them to build a Gaussian classifier. The Gaussian classifier estimates the likelihood of a given data point belonging to each class based on the probability density function of a multivariate Gaussian distribution with the class’s mean and covariance matrix.\nClassification: When you receive a new data point, you calculate the likelihood of it belonging to each class using the Gaussian distribution parameters (mean and covariance matrix) for each class. You can then assign the data point to the class with the highest likelihood.\n\n\n\n\nThe whole point of LDA was explained at: here\n\n\n8.4 Disadvantages of LDA\n\nSensitive to outliers. Meaning if I have a point really far from \\(\\mu_1\\) then it induces a large shift to the actual \\(\\mu_1\\)\nRelies in handcrafted features, if I go to high dimensional spaces I need to make choices and complicates things\nThe same as regression, here in clarification with LDA the MLE MAximum Likelihood are prone to overfilling. The latter because any regularization has been applied\n\nSo far:"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#probabilistic-generative-models-discrete",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#probabilistic-generative-models-discrete",
    "title": "Classification and Decision Theory",
    "section": "9 Probabilistic Generative Models: Discrete",
    "text": "9 Probabilistic Generative Models: Discrete\nIn this section we parametrize the class-conditional density with other distributions\n\nSo far we parametrize the Class conditional distributions with Gaussians. We can use however different ones. This is necessary when ie. the data it is not continuous and for instance its discrete.\nIn the Continuous space the number of parameters does not scales as much as in the discrete where then we have for i.e a binary classification to \\(2^D\\) parameters. The reasons is that we cannot fit anymore a Gaussains distribution to the discrete variables\n\nTo contrast the huge num. of parameters then we are going to make a model assumption that is:\n\nNaive Bayes assumption: feature value are treated as independent when conditioned on class \\(C_k\\).\n\nThe above means that we are going to model each feature value with its own distribution. This in turn means that to model \\(p(x|C_k, \\lambda_1,...,\\lambda_D)\\) we will have \\(D\\) number of parameters. The following equation accounts for that using the product symbol.\nGiven:\n\nClass-conditional probabilities: \\[\n\\begin{align}\np(x|C_k) &= \\prod_{i=1}^{D} \\, p(x_i|C_k) \\\\\n&= \\prod_{i=1}^{D} \\,  \\pi_{k_i}^{x_i}(1- \\pi_{k_i}^{x_i})^{1-x_i} \\nonumber\n\\end{align}\n\\]\n\nHere:\n\n\\(x_i\\) takes the value \\(0\\) or \\(1\\) given my class \\(C_k\\)\n\nThe above equation was modeled using the bernoulli equation\n\n\n\n\n\n\nEffect of changing parameters in Bernoulli distribution\n\n\n\n\n\nThe Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes, often referred to as “success” and “failure.” It is named after Swiss mathematician Jacob Bernoulli. The Bernoulli distribution is used to represent situations where an event can result in one of two possible outcomes, typically denoted as 1 (success) or 0 (failure).\nHere are the key characteristics and properties of the Bernoulli distribution:\n\nParameters: The Bernoulli distribution has a single parameter, denoted as “p,” which represents the probability of success (or the probability of the outcome being 1). The parameter “q” represents the probability of failure (q = 1 - p).\nProbability Mass Function (PMF): The probability mass function of the Bernoulli distribution is defined as follows:\nP(X = x) = p^x * q^(1-x)\nWhere:\n\nP(X = x) is the probability of the random variable X taking the value x (either 0 or 1).\np is the probability of success (X = 1).\nq is the probability of failure (X = 0).\n\nMean and Variance:\n\nThe mean (expected value) of a Bernoulli random variable is E(X) = p.\nThe variance of a Bernoulli random variable is Var(X) = p(1-p).\n\nSupport: The Bernoulli distribution is defined over the set of values {0, 1}.\n\nApplications of the Bernoulli distribution: - Modeling coin flips (heads or tails). - Modeling success/failure outcomes, such as whether a product is defective (failure) or non-defective (success). - Modeling binary decisions, such as whether a customer makes a purchase (success) or does not make a purchase (failure).\nExample Scenario: Suppose you have a coin that is not fair; it is biased. When you flip this biased coin, it does not have an equal chance of landing on heads (H) or tails (T). Instead, it has a higher probability of landing on heads.\nProbability of Success (Heads): In this example, we have a probability of success (getting heads) denoted as “p.” Let’s say that “p” is equal to 0.6. This means that when you flip the coin, there is a 60% chance of getting heads (H) and a 40% chance of getting tails (T).\nUsing the Bernoulli Distribution: To model this coin-flipping scenario, you can use a Bernoulli distribution. In this context:\n\nThe outcome “1” can represent success (getting heads).\nThe outcome “0” can represent failure (getting tails).\n\nThe Bernoulli distribution allows you to calculate the probability of success (1) or failure (0) for each coin flip.\nBernoulli Distribution Parameters: - Parameter “p” is the probability of success (1), which is 0.6 in this case. - Parameter “q” is the probability of failure (0), which is 1 - p, or 0.4 in this case.\nUsing the Bernoulli PMF: The Bernoulli probability mass function (PMF) allows you to calculate the probability of each outcome:\n\nP(X = 1) represents the probability of getting heads (success), which is equal to “p” or 0.6.\nP(X = 0) represents the probability of getting tails (failure), which is equal to “q” or 0.4.\n\nInterpreting the Results: When you flip this biased coin multiple times, the Bernoulli distribution helps you understand the likelihood of getting heads (success) or tails (failure) for each individual flip. For example:\n\nIf you flip the coin 10 times, you would expect to get heads approximately 6 times (0.6 * 10) on average.\nHowever, the actual outcomes in any given sequence of 10 flips may vary around this expected value due to randomness.\n\nThe Bernoulli distribution provides a mathematical framework to model and analyze such binary outcomes in scenarios like coin flipping, where there are two possible results with different probabilities of occurrence.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n# Define the probability of success (getting heads)\np = 0.6\n\n# Number of coin flips\nnum_flips = 10\n\n# Simulate the outcomes of flipping the biased coin 10 times\noutcomes = bernoulli.rvs(p, size=num_flips)\n\n# Calculate the PMF of the Bernoulli distribution\nx = [0, 1]\npmf = bernoulli.pmf(x, p)\n\n# Create a bar plot to visualize the PMF\nplt.figure()\n\nplt.subplot(1, 2, 1)\nplt.bar(x, pmf, align='center', alpha=0.7)\nplt.xticks(x)\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\nplt.title(f'Bernoulli Distribution PMF (p={p})')\n\nplt.subplot(1, 2, 2)\nplt.bar(range(num_flips), outcomes, align='center', alpha=0.7)\nplt.xticks(range(num_flips))\nplt.xlabel('Coin Flip')\nplt.ylabel('Outcome (0 or 1)')\nplt.title(f'Outcomes of {num_flips} Coin Flips (p={p})')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\\(\\pi_{k_i}=p(x_i=1|C_k)\\)\n\nNow the number of parameters per class is \\(D\\)\n\nWith this parametrization we can calculate the posterior probability: \\(p(C_k|x)\\) where we can compute it with our recently parametrized-bernouli like class density aka the likelihood. Here modeling the prior follows the same as how we did it with the class-density akak Likelihood, With these two we can get the joint aka the evidence evidence.\nRemember the evidence is the marginalization (sumation) over the joint (\\(p(x,C_k)\\)).\n\\[\n\\begin{align}\np(x) = \\sum_{k=1}^{K}p(x,C_k) = \\sum_{k=1}^{K}p(x|C_k)p(C_k)\n\\end{align}\n\\]"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#discriminative-linear-models",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#discriminative-linear-models",
    "title": "Classification and Decision Theory",
    "section": "10 Discriminative Linear Models",
    "text": "10 Discriminative Linear Models\nWe go away momentarily from the prob view and try model the discriminant function without describing first a prob model.\nWe will do the following. Given input \\(x \\in \\mathbb{R}^{Dx1}\\) and targets \\(t \\in {C1, C2}={-1,1}\\)\n\\[\n\\begin{align}\ny(x,\\overline{w})&=f(\\overline{w}^t \\boldsymbol{\\phi}) \\\\\n\\boldsymbol{\\phi} &= (\\phi_{0}(x),\\phi_{1}(x),...,\\phi_{M-1}(x))^T\n\\end{align}\n\\]\nThis model is linear with respect to \\(w\\)"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#least-squares-for-classification",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#least-squares-for-classification",
    "title": "Classification and Decision Theory",
    "section": "11 Least Squares for Classification",
    "text": "11 Least Squares for Classification\nWe are gonna have for each \\(K\\) a linear model (discriminant function)\n\\[\n\\begin{align}\ny_k = (\\textbf{x}) = \\textbf{w}_{k}^{T}\\textbf{x} + w_{k0}\n\\end{align}\n\\]\nWhere:\n\nInputs: \\(\\textbf{x} \\in \\mathbb{R}^{Dx1}\\), \\(D\\)-Dimensional feature vector (data points that describe each vector)\ni.e for \\(D=10\\) we have \\(\\textbf{x}_n = (x1, x2, ...,x_D)^T = (0,1,0,...0)\\)\nFor instance in the document classifier form the practicals, we have that each word from the document will add up to form a \\(D\\) vector. Each element in this vector correspond \\(0\\) or \\(1\\) indicating whether a word belongs or not to the document.\n\\(\\textbf{w}_k^T \\in \\mathbb{R}^{1xD}\\) is the weight vector for class \\(k\\)\n\\(w_{k0}\\) is the bias term.\n\nWe can generalize this not only for one \\(y_k\\) but for all \\(K\\text{s}\\) like so:\n\\[\n\\begin{align}\n\\textbf{y} =  \\mathbf{\\widetilde W}^T \\mathbf{\\tilde x}\n\\end{align}\n\\]\nWhere:\n\nMatrix \\(\\mathbf{\\widetilde W} \\in \\mathbb{R}^{MxK}\\): has in the kth column \\(\\mathbf{\\tilde w}_k = (w_{k0}, \\textbf{w})^T \\in \\mathbb{R}^{(D+1)x1} \\in \\mathbb{R}^{Mx1}\\)\nMatrix \\(\\mathbf{\\widetilde W}^T \\in \\mathbb{R}^{KxM}\\)\n\\(\\mathbf{\\tilde{x}} = (1, \\textbf{w})^T \\in \\mathbb{R}^{(D+1)x1}\\in \\mathbb{R}^{Mx1}\\)\nVector: \\(\\textbf{y(x)} \\in \\mathbb{R}^{Kx1}\\)\n\nRemember at the end \\(y_k\\) is just a number (you can think of a number that tells you how far \\(x\\) is from the decision surface)so we are going to assign \\(\\textbf{x}\\) to class \\(C_k\\) if: \\[\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\begin{align}\nk = \\argmax_{j} \\, y_j(\\textbf{x})\\\\\n\\end{align}\n\\]\nNow, for this to work we need target values so that we can minimize our error function:\n\\[\n\\begin{align}\nE_D(\\mathbf{\\widetilde W})=\\frac{1}{2}Tr[(\\mathbf{\\widetilde X}\\mathbf{\\widetilde W} - \\mathbf{\\widetilde T})^T(\\mathbf{\\widetilde X}\\mathbf{\\widetilde W} - \\mathbf{\\widetilde T})]\n\\end{align}\n\\]\n\n\n\n\n\n\nRemember: What was Linear Regresion & Do we get the target values?\n\n\n\n\n\nIn simple linear regression, there is one target value, whereas in multiple linear regression, there can be multiple target values.\nIn a typical linear regression problem, you are given the input features (independent variables) and the corresponding target values (dependent variables) as training data. The goal of linear regression is to learn a linear relationship between the input features and the target values\nInput Features (Independent Variables): These are the variables that you use to make predictions. Each data point in the dataset has a set of input features.\nTarget Values (Dependent Variables): These are the values you are trying to predict or estimate based on the input features. Each data point in the dataset has a corresponding target value.\nThe linear regression model is trained using this dataset to find the coefficients (weights) for the input features that minimize the mean squared error between the predicted values and the actual target values. Once the model is trained, you can use it to make predictions on new or unseen data.\n\n\n\nIn the above equation \\(E_D\\):\n\n\\(X \\in \\mathbb{R}^{Nx(D+1)} \\in \\mathbb{R}^{NxM}\\), where each row is a different observation (a different number of document like in the practicals) represented by the vector \\(\\mathbf{\\tilde{x}}_n\\)\nTarget: \\(T \\in \\mathbb{R}^{NxK}\\), where each row is a different one-hot encoded vector that is trying to predict what is the class that \\(\\mathbf{\\tilde{x}}_n\\) belongs to. The one-hot enconding means that for that particular kth value is 1 meaning it belongs to the kth class and for the rest is zero.\n\\(t \\in \\mathbb{R}^{Kx1}\\) where \\(t \\in \\{C_1, C_2, ..., C_k\\}\\) and \\(K\\) classes\ni.e for \\(K=4\\) for binary classification we have we have \\(t = (0, 0, 0, 1)^T\\). The latter would be the one-hot encoding for \\(K=4\\) documents\ni.e if \\(k=5\\) my one-hot encoding when it is predicting for class k=3 would be. \\(t_n=(0,0,1,0,0)^T\\)\n\n\\(Tr\\): Sum of the diagonal matrix\n\n\nGoal: Now that we have defined our error we want to minimize it as a function of \\(W\\) so that when we new values for \\(x\\) come, then we multiply with our computed \\(W\\) and finally get our predicted \\(y(x)\\).\n\nSolution:\n\\[\n\\begin{align}\n\\mathbf{\\widetilde W}_{LS} = (\\mathbf{\\widetilde X}^T\\mathbf{\\widetilde X})^{-1}\\mathbf{\\widetilde X}^T\\textbf{T} = \\mathbf{\\widetilde X}^\\dagger\\textbf{T}\n\\end{align}\n\\]\nFinally to predict the label for \\(\\mathbf{\\tilde{x}}\\) we use our discriminant function: \\[\n\\begin{align}\n\\textbf{y}_{LS}(\\textbf{x})&=\\mathbf{\\widetilde W}_{LS}^T\\mathbf{\\tilde{x}}\\\\\n& \\in \\mathbb{R}^{KxM} \\, \\in \\mathbb{R}^{Mx1} \\nonumber\\\\\n&\\in \\mathbb{R}^{Kx1}\\nonumber\n\\end{align}\n\\]\nSo one number per each class. We get a vector with dimensions \\(K\\) because this vector was one-hot encoded so that means we have to look at the value that contains \\(1\\) and that \\(k_{th}\\) element would be our class \\(C_k\\)\n\nDiscriminant functions are used to classify data points into different classes based on the values of the discriminant function\n\n\n11.1 Why Linear Regresion for Classification is not a good idea?\n\nThe decision boundaries are sensitive to outliers. Our Linear Regresion wants our distance to be as close to \\(y(x)=1\\) if the target value is also \\(1\\), but if there is outliers then these points will influence the decision boundary skewing it.\nFor \\(k&gt;2\\) some decision regions can become very small or are even completely ignored\nThe components of the \\(\\textbf{y}_{LS}(\\textbf{x})\\) are not real probabilities"
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#multi-class-logistic-regression",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#multi-class-logistic-regression",
    "title": "Classification and Decision Theory",
    "section": "12 Multi-class Logistic Regression",
    "text": "12 Multi-class Logistic Regression\nConsider logistic regresion for \\(K\\) classes with \\(N\\) training vectors \\(\\{x_n\\}_{n=1}^{N}\\). Each of these vectors is mapped to a different feature vector.\n\\[\n\\begin{align}\n\\phi_n = \\phi(x_n) = (\\phi_0(x_n), \\phi_1(x_n), ..., \\phi_{M-1}(x_n))^T\n\\end{align}\n\\]\n\nEach vector \\(x_n\\) has a target vector \\(t_n\\) of size \\(K\\)\n\n\\[\n\\begin{align}\nt_n = (t_{n1}, t_{n2}, ..., t_{nK})^T\n\\end{align}\n\\]\nWhere, \\(t_{nk} = 1\\) if \\(x_n \\in C_k\\), \\(0\\) otherwise.\n\nThe input data can be collected in a matrix \\(X\\) such that the n-th row is given by \\(\\textbf{x}_{n}^{T}\\). The targets can also be collected in a matrix \\(T\\) where each n-th row is given by \\(\\textbf{t}_{n}^{T}\\)\n\nWe have:\n\\[\n\\begin{align}\ny_k(\\phi) = p(C_k|\\phi(x), \\textbf{w}_1,...,\\textbf{w}_K) = \\frac{exp(a_k)}{\\sum_{j=1}^{K}exp(a_j)}\n\\end{align}\n\\]\nWhere we have \\(a_k = \\textbf{w}_{k}^{T}\\phi\\)\nDimensions:\n\\[\n\\begin{align}\n\\textbf{y(x)} &= (y_1(x),...,y_k(x) )^T &\\in \\mathbb{R}^{Kx1} \\nonumber\\\\\n\\phi &= \\phi(x) = (\\phi_0(x), ..., \\phi_{M-1}(x))^T &\\in \\mathbb{R}^{Mx1}\\nonumber\\\\\n\\mathbf{\\phi}_n &= \\phi(x_n) = (\\phi_0(x_n), ..., \\phi_{M-1}(x_n))^T\\nonumber\\\\\n\\Phi &= (\\phi_1, \\phi_2, ..., \\phi_{N})^T &\\in \\mathbb{R}^{NxM}\\nonumber\\\\\n\\mathbf{w}_k &= (w_{k0},...,w_{k(M-1)})^T \\in \\mathbb{R}^{(D+1)x1} &\\in \\mathbb{R}^{Mx1}\\nonumber\\\\\nX &= (\\textbf{x}_1,...,\\textbf{x}_N)^T \\in \\mathbb{R}^{Nx(D+1)} &\\in \\mathbb{R}^{NxM}\\nonumber\\\\\n\\textbf{t}_n &= (t_{n1}, ...,t_{nK})^T  &\\in \\mathbb{R}^{1xK}\\nonumber\\\\\nT &= (\\textbf{t}_1,...,\\textbf{t}_N)^T &\\in \\mathbb{R}^{NxK}\\nonumber\\\\\n\\end{align}\n\\]\n\n12.1 Multivariate Gaussian Distribution\nThe PDF: \\[\n\\begin{align}\nf(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{\\frac{n}{2}} |\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}} \\exp\\left(-\\frac{1}{2} (\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)\\\\\n\\end{align}\n\\]\nWhere:\n\n\\(\\mathbf{x}\\) represents the vector of random variables (n-dimensional).\n\\(\\boldsymbol{\\mu}\\) is the mean vector, which is also an n-dimensional vector.\n\\(\\boldsymbol{\\Sigma}\\) is the covariance matrix, which is an n x n symmetric positive-definite matrix.\n\\(|\\boldsymbol{\\Sigma}|\\) represents the determinant of the covariance matrix.\n\\((\\mathbf{x} - \\boldsymbol{\\mu})^T\\) represents the transpose of the vector \\((\\mathbf{x} - \\boldsymbol{\\mu}\\)).\n\\(\\boldsymbol{\\Sigma}^{-1}\\) is the inverse of the covariance matrix."
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#fixed-bssis-functions",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#fixed-bssis-functions",
    "title": "Classification and Decision Theory",
    "section": "13 Fixed Bssis Functions",
    "text": "13 Fixed Bssis Functions\nIn the context of basis functions, the statement “Easy way to define nonlinear models (wrt the original features) via linear models (in the parameters)” means that you can represent complex, nonlinear relationships between your input features and the output variable by using a linear model in terms of transformed or “basis” functions. Let’s break down what this statement implies:\n\nNonlinear Models: Nonlinear models are those that cannot be expressed as simple linear relationships between the input features and the output. In many real-world problems, the relationships between variables are not linear, which makes modeling them directly with linear models (like linear regression) challenging.\nOriginal Features: These are the raw input features of your data. For example, if you’re working with a dataset of house prices, the original features might include the number of bedrooms, square footage, and location.\nBasis Functions: Basis functions are mathematical functions that transform the original features into a new set of features. These new features are designed to capture the underlying nonlinear relationships in the data. Common basis functions include polynomial functions (e.g., squaring a feature to capture quadratic relationships) and radial basis functions (used in radial basis function networks).\nLinear Models (in the Parameters): Despite the use of basis functions to transform the input features, the model’s structure is still linear in terms of its parameters. This means that you can express the output variable as a linear combination of the transformed features, where the coefficients of this linear combination are the parameters of the model. For example, you might have a model like:\ny = w0 + w1 * basis_function1(x) + w2 * basis_function2(x) + ... + wn * basis_functionn(x)\nHere, w0, w1, w2, ..., wn are the model parameters, and basis_function1(x), basis_function2(x), ... are the transformed features created by the basis functions.\n\nSo, the statement is highlighting that by using basis functions to transform the original features, you can still use a linear model in terms of its parameters to capture complex, nonlinear relationships in the data. This approach makes it easier to model nonlinear data patterns while benefiting from the simplicity and interpretability of linear models when it comes to parameter estimation and inference."
  },
  {
    "objectID": "blog/2023-09-25_classification-and-decision-theory/index.html#faq",
    "href": "blog/2023-09-25_classification-and-decision-theory/index.html#faq",
    "title": "Classification and Decision Theory",
    "section": "14 FAQ",
    "text": "14 FAQ\n\n\n\n\n\n\n1. Why if correlated features are treated independently, the evidence for a class will be overcounted?\n\n\n\n\n\nI apologize for any confusion. Let me break down that fragment for better clarity:\n\nEvidence for a Class: When we talk about “evidence for a class” in a classification problem, we’re referring to the information or characteristics of a data point’s features that suggest or indicate which class that data point should belong to. This evidence is essentially the input data that the classification algorithm uses to make predictions.\nCorrelated Features: Correlated features are features that have some degree of statistical relationship or similarity between them. In the context of a classification problem, correlated features might provide similar or redundant information about the data.\nRedundant or Overlapping Information: When features are correlated, it means that some of the information they provide is redundant or overlapping. In other words, these features may convey the same or very similar details about the data point. For example, if you have two highly correlated features, knowing the value of one feature might give you a good idea about the value of the other.\nTreating Features Independently: In some classification algorithms, especially simple ones like Naive Bayes, each feature is treated as if it is completely independent of the others. This assumption simplifies the modeling process but can be problematic when features are correlated.\nCounting Shared Information Multiple Times: When you treat correlated features independently, you essentially consider the shared or overlapping information multiple times. For example, if two features are highly correlated and you treat them independently, you might effectively count the same information twice, once for each correlated feature. This can lead to an overestimation of the importance of that shared information.\n\nTo illustrate this concept, consider a classification task where you’re trying to predict whether an email is spam (class 1) or not spam (class 0) based on two features: the number of times the word “money” appears in the email and the number of times the word “cash” appears. If these two features are highly correlated (i.e., they tend to occur together), treating them independently might lead to double-counting the evidence related to financial terms, which could skew the classification result. Therefore, it’s important to handle correlated features appropriately in order to make accurate predictions.\n\n\n\n\n\n\n\n\n\n2. What is the Köppen climate classification system?\n\n\n\n\n\nIt consists of five primary climate groups, which are further subdivided into various climate types. Here’s a breakdown of the primary climate groups and their associated climate types:\n\nGroup A: Tropical Climates:\n\nAf: Tropical rainforest climate\nAm: Tropical monsoon climate\nAw: Tropical wet and dry or savanna climate\n\nGroup B: Dry Climates:\n\nBWh: Hot desert climate\nBWk: Cold desert climate\nBSh: Hot semi-arid climate\nBSk: Cold semi-arid climate\n\nGroup C: Temperate Climates:\n\nCfa: Humid subtropical climate\nCwa: Monsoon-influenced humid subtropical climate\nCfb: Oceanic or maritime climate\nCfc: Subpolar oceanic climate\nCsa: Mediterranean climate\nCsb: Mediterranean climate with dry summer\nCwa: Monsoon-influenced humid subtropical climate\nCwc: Cold subtropical highland climate\n\nGroup D: Continental Climates:\n\nDfa: Hot-summer humid continental climate\nDfb: Warm-summer humid continental climate\nDfc: Subarctic or boreal climate\nDwa: Hot-summer subarctic climate\nDwb: Warm-summer subarctic climate\n\nGroup E: Polar Climates:\n\nET: Tundra climate\nEF: Ice cap climate\n\n\nAdditionally, there is a Group H: Highland Climates category for high-altitude regions with their own unique climate characteristics.\nSo, there are a total of 12 primary climate types within the Köppen climate classification system, and each of these primary types can be further refined with additional letters and numbers to provide more specific details about temperature and precipitation patterns.\n\n\n\n\n\n\n\n\n\n3. What are Generative Models?\n\n\n\n\n\nIn machine learning, generative models are a class of models used to learn the underlying probability distribution of the data. These models are called “generative” because they can generate new data samples that resemble the training data. In other words, they capture the structure and patterns within the data and can be used to create synthetic data points that are statistically similar to the real data.\n\nHow NB is a generative model?\n\nIn generative models we model two things: 1. the class-conditional densities p(x|C_k) 2. the class priors p(C_k). With these two we can compute the join distribution.\nFurthermore, one can use Bayes Theorem to compute the posterior p(C_k|x). Looking at the equation above we see that we have defined the numerator as the joint distribution obtained from our generative model. For the denominator part, we have computed the evidence by marginalising over the k classes.\nHaving done this, (we have used Naive Bayes to model the distribution of our features values as independent to decrease the number of parameters) we have obtain the posterior probability which can be used to determine class membership for each new input x.\nAs referred in Bishop, approaches that explicit or implicitly model the distribution of input as well as the outputs are known as generative models. They called generative models because by sampling from them it is possible to generate synthetic data points in the input space.\n\nWhat does it mean by Generative models?\n\nIn Bishop we have:\n\nApproaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.\n\nThe statement you provided highlights a key characteristic of generative models and explains why they are called “generative.” Let’s break down the meaning of this statement:\n\nApproaches that explicitly or implicitly model the distribution of inputs as well as outputs: This part of the statement refers to machine learning models that are designed to not only capture the relationship between inputs and outputs but also to learn the underlying probability distribution of the inputs. In other words, these models aim to understand how the input data is generated probabilistically.\nKnown as generative models: These models are commonly referred to as “generative models” because they have the capability to generate synthetic data points that resemble the real data. This is achieved by sampling from the learned distribution of inputs. In essence, generative models can create new data instances that are statistically similar to the training data.\nBy sampling from them it is possible to generate synthetic data points in the input space: This part of the statement explains the practical significance of generative models. Once a generative model has learned the data distribution, you can use it to create new data points. These new data points are generated by sampling from the probability distribution of inputs that the model has learned during training.\n\nHere’s a simple example to illustrate this concept: Consider a generative model trained on a dataset of images of cats. The model learns not only to recognize cats but also the underlying distribution of features that define what a cat looks like. Once trained, you can sample from this model, and it will generate new images of cats that resemble those in the training data. These generated images are synthetic data points in the input space because they represent new, artificial cat images that the model has “generated” based on what it has learned about cats.\n\n\n\n\n\n\n\n\n\n4. Convex regions\n\n\n\n\n\nLet’s consider a 2D space, and we have a convex region “C1.” We’ll choose two points within this region, x0 and x1. Then, we’ll create a line segment by varying λ between 0 and 1 and plot the points on the line segment. If all points on the line segment remain within C1, it indicates that C1 is convex.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the convex region C1 (a circle for this example)\nradius = 2\ncenter = (0, 0)\n\n# Generate random points within C1\ntheta = np.linspace(0, 2 * np.pi, 100)\nx0 = center[0] + radius * np.cos(theta)\ny0 = center[1] + radius * np.sin(theta)\n\n# Choose two points x0 and x1 within C1\nx0_point = (1, 1)\nx1_point = (-1, 1)\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Plot C1 as a filled circle\nax.fill(x0, y0, 'b', alpha=0.5)\n\n# Plot the two chosen points x0 and x1\nax.plot(x0_point[0], x0_point[1], 'ro', label='x0')\nax.plot(x1_point[0], x1_point[1], 'go', label='x1')\n\n# Generate points along the line segment defined by x0 and x1\nlambdas = np.linspace(0, 1, 50)\nline_points = [(1 - l) * np.array(x0_point) + l * np.array(x1_point) for l in lambdas]\n\n# Check if all points on the line segment are within C1\nall_inside_c1 = all(np.linalg.norm(p - center) &lt;= radius for p in line_points)\n\n# Highlight the line segment\nif all_inside_c1:\n    ax.plot([p[0] for p in line_points], [p[1] for p in line_points], 'k-', label='Line Segment (Convex)')\nelse:\n    ax.plot([p[0] for p in line_points], [p[1] for p in line_points], 'r-', label='Line Segment (Not Convex)')\n\n# Set axis limits\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\n\n# Add labels and legend\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Convex Region and Line Segment')\nax.legend()\n\n# Show the plot\nplt.grid(True)\nplt.show()\n\n\n\n\n\nIn this example, we define a convex region “C1” as a circle. We choose two points, x0 and x1, within C1 and create a line segment by interpolating between them using λ values. If all points on the line segment remain within the circle (C1), it indicates that C1 is convex. The plot will show the circle, the two chosen points, and the line segment along with labels and a legend."
  },
  {
    "objectID": "blog/2023-07-24_the-right-tool/index.html",
    "href": "blog/2023-07-24_the-right-tool/index.html",
    "title": "Picking the right tool to show your Machine Learning project",
    "section": "",
    "text": "Picking the right tool to show your Machine Learning project\n        \n        \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                DevOps\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                            \n                            \n                                Shiny\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                DevOps\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                                            \n                            \n                               \n                                Shiny\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          July 24, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n\n\n\n\n\n\nDisclaimer\n\n\n\nFor sake of keeping my thoughts run smoothly this page may contain typos. Bear with Danilo.\n\n\n\n\nThe reason\nI have come across the question on where to host the models that I have trained and display its outputs in an interactive way.\nWhile doing some googling I came across different technologies which much of which have been recently launched, provided the pletora of machine learning application these days. So here a couple of things that I have learnt:\n\n\nStatic websites just got better!\nFor static websites, there is pretty much not too many available options. Statics sites like this one make use of javascript as the backbone to run expensive tasks and usually data science tools these days operate with Python, Julia or R.\nBeing said that, pyscript seems to pose a first steps on how to run python in HTML. Found this was a relief because then I discovered that Python code is able to be written in a portable binary-code format WASM, which in turn means you can write Python and run it at the server side. For a more thorugh explanation about this you can check this video explanation.\n\n\nIf Python available which technology to use?\nHere gets tricky, if you want to have a fast and easy to deployment usually there is no much customization, and by that I mean curky ads and logos like: made by.. not to much fan of those. Following a simple chart that I have found really usefull to solve my enquires, I am attaching the source link here for reference.\n Source: 1\n\n\nThe decision\nAll set and stone, I have only some requirements to jump into learning a new technology and those are\n\nScalable: I want to be able to not just use it for my blog but for future work as well\nOn the look: if it has not been used for the last couple of years then something new is on the look\nBalance-learning-curve: I have learnt Haskell and let me tell you, prototyping and designing code is another world in functional language. So I want be a eager learning while still discovering more tools next couple of months in the road.\n\nConsidering these facts: I am planing to stick with Shiny for the time being."
  },
  {
    "objectID": "blog/2023-06-05_how-hash-table-works/index.html",
    "href": "blog/2023-06-05_how-hash-table-works/index.html",
    "title": "How PCY Algorithm works",
    "section": "",
    "text": "An Effective Hash-Based Algorithm for Mining Association Rules.\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Data Mining\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Data Mining\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          June 5, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                    \n                 Code\n                        \n                                     See Article"
  },
  {
    "objectID": "blog/2023-06-05_how-hash-table-works/index.html#initializing-the-transaction-baskets",
    "href": "blog/2023-06-05_how-hash-table-works/index.html#initializing-the-transaction-baskets",
    "title": "How PCY Algorithm works",
    "section": "Initializing the transaction baskets",
    "text": "Initializing the transaction baskets\n\nimport numpy as np\nimport pandas as pd\nimport itertools\n\n\n# initialize lists\ndata = [[1,2,5], [2,3,5], [4,5], [1,6,7], [2,3,5,7], [1,2,7]]\npairs = [list(itertools.combinations(i, 2)) for i in data]\nmydict = {'Items':data, 'Pairs': pairs}\n\n# Create the pandas DataFrame with column name is provided explicitly\ndf = pd.DataFrame(mydict)\n\n# Print dataframe\ndisplay(df)\n\n\n\n\n\n\n\n\nItems\nPairs\n\n\n\n\n0\n[1, 2, 5]\n[(1, 2), (1, 5), (2, 5)]\n\n\n1\n[2, 3, 5]\n[(2, 3), (2, 5), (3, 5)]\n\n\n2\n[4, 5]\n[(4, 5)]\n\n\n3\n[1, 6, 7]\n[(1, 6), (1, 7), (6, 7)]\n\n\n4\n[2, 3, 5, 7]\n[(2, 3), (2, 5), (2, 7), (3, 5), (3, 7), (5, 7)]\n\n\n5\n[1, 2, 7]\n[(1, 2), (1, 7), (2, 7)]"
  },
  {
    "objectID": "blog/2023-06-05_how-hash-table-works/index.html#calculating-the-supports",
    "href": "blog/2023-06-05_how-hash-table-works/index.html#calculating-the-supports",
    "title": "How PCY Algorithm works",
    "section": "Calculating the supports",
    "text": "Calculating the supports\n\nprint(data)\n\n[[1, 2, 5], [2, 3, 5], [4, 5], [1, 6, 7], [2, 3, 5, 7], [1, 2, 7]]\n\n\n\nsupports = {}\nfor row in data:\n    for e in row:\n        if e not in supports:\n            supports[e] = 0\n\n# Sorting dictionary\nsorted_dict = supports.keys()\nsuppports = sorted(sorted_dict)\nprint(supports)\n\n{1: 0, 2: 0, 5: 0, 3: 0, 4: 0, 6: 0, 7: 0}\n\n\n\nfor row in data:\n    for e in row:\n        supports[e] += 1\n\nprint(supports)\n\n{1: 3, 2: 4, 5: 4, 3: 2, 4: 1, 6: 1, 7: 3}\n\n\n\nitem = [ key for key, value in supports.items()]\nsupp = [ value for key, value in supports.items()]\nunique_items_count = {'Itemset': item, 'Sup': supp}\n\ndf_item_sup = pd.DataFrame(unique_items_count)\ndisplay(df_item_sup)\n\n\n\n\n\n\nTable 1: Support\n\n\n\nItemset\nSup\n\n\n\n\n0\n1\n3\n\n\n1\n2\n4\n\n\n2\n5\n4\n\n\n3\n3\n2\n\n\n4\n4\n1\n\n\n5\n6\n1\n\n\n6\n7\n3\n\n\n\n\n\n\n\n\n\nPass 1\n\ndef hash_f_pair(i,j):\n    return (i*j) % 7\n\n\nunique_items = []\nfor row in pairs:\n    for e in row:\n        if e not in unique_items:\n            unique_items.append(e)\nprint(unique_items)\n\n[(1, 2), (1, 5), (2, 5), (2, 3), (3, 5), (4, 5), (1, 6), (1, 7), (6, 7), (2, 7), (3, 7), (5, 7)]\n\n\n\nunique_items[0][1]\n\n2\n\n\n\nhash_value_pairs = []\nfor e in unique_items:\n        hash_value_pairs.append(hash_f_pair(e[0],e[1]))\nprint(hash_value_pairs)\n\n[2, 5, 3, 6, 1, 6, 6, 0, 0, 0, 0, 0]\n\n\n\nmy_dict2 = {}\nfor pair in unique_items:\n    my_dict2[str(pair)] = 0\nprint(my_dict2)\n\n{'(1, 2)': 0, '(1, 5)': 0, '(2, 5)': 0, '(2, 3)': 0, '(3, 5)': 0, '(4, 5)': 0, '(1, 6)': 0, '(1, 7)': 0, '(6, 7)': 0, '(2, 7)': 0, '(3, 7)': 0, '(5, 7)': 0}\n\n\n\nfor row in pairs:\n    for e in row:\n        my_dict2[str(e)] +=1\n\nprint(my_dict2)\n\n{'(1, 2)': 2, '(1, 5)': 1, '(2, 5)': 3, '(2, 3)': 2, '(3, 5)': 2, '(4, 5)': 1, '(1, 6)': 1, '(1, 7)': 2, '(6, 7)': 1, '(2, 7)': 2, '(3, 7)': 1, '(5, 7)': 1}\n\n\n\ncounts = [ value for key, value in my_dict2.items()]\nprint(counts)\n# df_counts = pd.DataFrame()\n\n[2, 1, 3, 2, 2, 1, 1, 2, 1, 2, 1, 1]\n\n\n\nunique_items_count = {'Pairs': unique_items, 'Count': counts, 'Hash': hash_value_pairs}\nprint(unique_items_count)\n\n{'Pairs': [(1, 2), (1, 5), (2, 5), (2, 3), (3, 5), (4, 5), (1, 6), (1, 7), (6, 7), (2, 7), (3, 7), (5, 7)], 'Count': [2, 1, 3, 2, 2, 1, 1, 2, 1, 2, 1, 1], 'Hash': [2, 5, 3, 6, 1, 6, 6, 0, 0, 0, 0, 0]}\n\n\n\ndf_counts = pd.DataFrame(unique_items_count)\ndisplay(df_counts)\n\n\n\n\n\n\nTable 2: Hash Pair Table\n\n\n\nPairs\nCount\nHash\n\n\n\n\n0\n(1, 2)\n2\n2\n\n\n1\n(1, 5)\n1\n5\n\n\n2\n(2, 5)\n3\n3\n\n\n3\n(2, 3)\n2\n6\n\n\n4\n(3, 5)\n2\n1\n\n\n5\n(4, 5)\n1\n6\n\n\n6\n(1, 6)\n1\n6\n\n\n7\n(1, 7)\n2\n0\n\n\n8\n(6, 7)\n1\n0\n\n\n9\n(2, 7)\n2\n0\n\n\n10\n(3, 7)\n1\n0\n\n\n11\n(5, 7)\n1\n0\n\n\n\n\n\n\n\n\nMinium support count is 2 so we eliminate from the Pair list the items that have less than 2 from Table 1. The resulting table is called the Candidate Pairs.\n\ndf_counts_after = df_counts.drop([5, 6, 8])\ndisplay(df_counts_after)\n\n\n\n\n\n\nTable 3: Hash Pair Table after Pass 1\n\n\n\nPairs\nCount\nHash\n\n\n\n\n0\n(1, 2)\n2\n2\n\n\n1\n(1, 5)\n1\n5\n\n\n2\n(2, 5)\n3\n3\n\n\n3\n(2, 3)\n2\n6\n\n\n4\n(3, 5)\n2\n1\n\n\n7\n(1, 7)\n2\n0\n\n\n9\n(2, 7)\n2\n0\n\n\n10\n(3, 7)\n1\n0\n\n\n11\n(5, 7)\n1\n0\n\n\n\n\n\n\n\n\n\n\nPass 2\nThe final step is to build a table from Table 2 that counts the number of ocurrences per hash value.\n\nhash_values = [x for x in range(max(hash_value_pairs)+1)]\nmy_dict3 = {}\nfor e in hash_values:\n    my_dict3[e] = 0\n\nfor i, hash_value in enumerate(df_counts[\"Hash\"]):\n    my_dict3[hash_value] += df_counts[\"Count\"][i]\nprint(my_dict3)\n\ndata_bucket = {\"Bucket\": hash_values, \"Count\": my_dict3.values()}\ndf_bucket_count = pd.DataFrame(data_bucket)\ndisplay(df_bucket_count)\n\n{0: 7, 1: 2, 2: 2, 3: 3, 4: 0, 5: 1, 6: 4}\n\n\n\n\n\n\n\n\n\nBucket\nCount\n\n\n\n\n0\n0\n7\n\n\n1\n1\n2\n\n\n2\n2\n2\n\n\n3\n3\n3\n\n\n4\n4\n0\n\n\n5\n5\n1\n\n\n6\n6\n4\n\n\n\n\n\n\n\nWith the result above we can look back to table Table 3 and see which hash values, have a value lower than the minimum support count. That is hash value 4 and 5. With this result we look at table Table 3 and delete those corresponding hash values. The final result is teh Final Candidates\n\ndf_counts_after_pass2 = df_counts_after.drop([1])\ndisplay(df_counts_after_pass2)\n\n\n\n\n\n\n\n\nPairs\nCount\nHash\n\n\n\n\n0\n(1, 2)\n2\n2\n\n\n2\n(2, 5)\n3\n3\n\n\n3\n(2, 3)\n2\n6\n\n\n4\n(3, 5)\n2\n1\n\n\n7\n(1, 7)\n2\n0\n\n\n9\n(2, 7)\n2\n0\n\n\n10\n(3, 7)\n1\n0\n\n\n11\n(5, 7)\n1\n0"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html",
    "href": "blog/2023-11-13_sequence-labelling/index.html",
    "title": "Sequence Labelling",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 13, 2023"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#representing-and-estimating-categorical-distributions-the-logistic-case",
    "href": "blog/2023-11-13_sequence-labelling/index.html#representing-and-estimating-categorical-distributions-the-logistic-case",
    "title": "Sequence Labelling",
    "section": "1 Representing and estimating Categorical distributions: the logistic case",
    "text": "1 Representing and estimating Categorical distributions: the logistic case\n\n\n\n\nW is condition on the Random variable H, which can be a history\n\nf = is a function of the history \\(h\\) and has some parameters \\(\\theta\\)\nf is not a real value but a collection of probability values\nW = word\nIt can take on \\(V\\) categories, so W can take on \\(V\\) possible assignments\nV = vocabulary size\nH = history h the history can take as many values as possible i.e (BOS, a), (BOS, an), (BOS, some)\n\nIf you take a single row then all these values would add up to 1\nA Tabular representation is table look up operation. The yellow row represents then one \\(\\textbf{f}=\\theta_{row=h}^{col=1...V}\\)\nThen we need to think the function\n\\[\n\\textbf{f}(h; \\theta)\n\\]\nas a mechanism to predict vectors from the conditional information"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#logistic-representation-of-categorical-cpds",
    "href": "blog/2023-11-13_sequence-labelling/index.html#logistic-representation-of-categorical-cpds",
    "title": "Sequence Labelling",
    "section": "2 Logistic Representation of Categorical CPDs",
    "text": "2 Logistic Representation of Categorical CPDs\nCPDs= Conditional Probability Distribution\nWhat we are predicting with this Categorical distribution is the probabilities per each W under the condition of an \\(h\\) so on a row. Essentially we are predicting every single entry in the tabular representation. I am interest then to map these to\n\n\n\n\nafter each conditional information has been mapped to a point, I will map those points to a different space. I want a vecgor with \\(V\\) probabilities in it, one per each possible outcomes of the random variable \\(W\\).\n\n\n\n\nHere we have \\(\\phi\\)(small,birs) \\(\\in \\mathbb{R}^{2}\\) so in 2D that means I need my \\(W_1\\) also to be in 2D so when I multiply with \\(\\phi\\)(small,birs), I get an scalar value. \\(b_1\\) is just an scalar.\nThese last row \\(\\textbf{S}\\) is called vector of ‘Logits’ these can be large or negative to turn this into a vector inside the probability simplex we can use Softmax. Note we go from the dimensional space to logits throught a lienar model. And then from logits to probabilities with a Softmax function\nThe \\(n\\)-dimensional probability simplex, denoted as \\(\\Delta^n\\), is defined as:\n\\[\n\\Delta^n = \\left\\{ (p_1, p_2, \\ldots, p_n) \\mid p_i \\geq 0, \\sum_{i=1}^n p_i = 1 \\right\\}\n\\]\nHere, \\(p_i\\) represents the probability of event \\(i\\), and the conditions \\(p_i \\geq 0\\) and \\(\\sum_{i=1}^n p_i = 1\\) ensure that the vector lies within the simplex.\n\n\n\n\nThe model can turn the h into a vector of \\(V\\) probabilities. so the \\(\\textbf{f}_1(h,\\theta), \\textbf{f}_2(h,\\theta)...\\)\n\n2.1 Why is this a great idea?\n\n\n\n\n\nThe model size is a function of how many \\(w\\)s we have. So the model is very compact, because it does not depend on how many instances of h, because we have this basis function \\(\\phi\\) that maps it to the proper dimensions so that we can multiply it with \\(W^T\\)\nHistories are conditional information are no longer treated as unrelated to one another, now in fact are related to each other trhough their features. Because of these relatedness linear models work, because they find patterns that we can code in a tabular reprsentation\n\n\nSuppose you condition on a sentence to draw a prob of a label/class.\nImagine you have 5 labels/classes, the thing that you condition can be represented by a feature function that gives you D-dimensional features so we are talking about \\(\\phi(h) \\in \\mathbb{R}^D\\). For each of those dimensions you would weight them towards a class, for instance if you have \\(K\\)-classes, so \\(KD\\). Note \\(K=V\\)\nEach feature gets a relevance an importance score towards a class and each class get a bias \\(b\\). You can think of \\(b\\) of how usefull this class is, or how generally present it is in the data regardless of context.\nSo per feature, per class you have a real number \\(KD\\) + \\(K\\) (from the bias) for each of the possible classes in my hypothetical example"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#estimation-example",
    "href": "blog/2023-11-13_sequence-labelling/index.html#estimation-example",
    "title": "Sequence Labelling",
    "section": "3 Estimation example",
    "text": "3 Estimation example\nHow do we estimate the W_s and the b_s\n\nWe get data as pair, where we have \\(h_n\\) as a conditional variable & \\(w_n\\) an assigment of the outcome variable\nOur model would be log-linear it will map from the history through a feature function it will map histories to a vector of probability values of the correct dimensionality\n\\(s\\) = W\\(\\theta\\) + b is called the parametric function. Through this \\(s\\) our model predicts from any history h a complete categorical mass function with \\(V\\) values\n\nTo learn the parameters we initialize it, and then we compute the log likelihood of the parameters \\(\\theta\\) give our data \\(D\\). This can be solve due to the idd assumption.\n\n\n\n\nWhen we solve for the grad wrt. \\(\\theta\\) we do not get a closed-form solution like we get in the tabular representation. Thus we use an iterative procedure aka SGD.\nMatmul is the same as doing dot product but for matrices take a look at this link\nExample:\n\n\n\n\ncode at Colab"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#start-of-lecture",
    "href": "blog/2023-11-13_sequence-labelling/index.html#start-of-lecture",
    "title": "Sequence Labelling",
    "section": "4 Start of Lecture",
    "text": "4 Start of Lecture\n\n\n\n\nSlide 2\n\n\n\n\n\n\n\n\n\n\nSlide 5\n\n\n\n\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#why-do-we-want-to-classify-words-in-classescategories",
    "href": "blog/2023-11-13_sequence-labelling/index.html#why-do-we-want-to-classify-words-in-classescategories",
    "title": "Sequence Labelling",
    "section": "5 Why do we want to classify words in classes/categories",
    "text": "5 Why do we want to classify words in classes/categories\n\n\n\n\nSlide 7\n\n\n\n\nReduce the number of dimensions like in bigrams only consider the nouns and adjectives and that is to measure the sentiment of a review. So here we refer to feature as the \\(\\phi(h)\\) so if we reduce this feature vector say \\(D\\) to a less than \\(D\\) i.e in bigrams \\(D=2\\) then we save memory and reduce the dimensions\n\n\n\n\n\nSlide 9\n\n\n\n\nIn out tabular CPDs we are force to treat each words as if they are unrelated to each other, but that is not trueth because as we sa before we can have relatedness i.e adjectives preceed nouns.\nSo to capture relatedness the categorization of words can capture that.\n\nFor example how to classify them:\n\n\n\n\n\nSlide 10\n\n\n\n\n\n\n\n\nSlide 11\n\n\n\n\nOnce we have categorized we can even not use the words because we know already its categories and we know that we this categorization a sentences would be like noun + adjective, so then we dont need the actual word but rather just its category.\n\nCalled the above a mapping. Note this could not be good enough in some instances. A reason why to do this is so that we reduce the dimensionality space.\n\n\n\n\n\nSlide 12\n\n\n\n\nHere an example of how people classify unambiguously\n\n\n\n\nSlide 13\n\n\n\n\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#how-to-annotate-words-with-part-of-speech-pos-using-peen-style",
    "href": "blog/2023-11-13_sequence-labelling/index.html#how-to-annotate-words-with-part-of-speech-pos-using-peen-style",
    "title": "Sequence Labelling",
    "section": "6 How to annotate words with Part Of Speech (POS) using Peen Style",
    "text": "6 How to annotate words with Part Of Speech (POS) using Peen Style\n\n\n\n\nSlide 15\n\n\n\n\n\n\n\n\n\nSlide 17\n\n\n\n\nThis is a join distribution\n\n\n\n\n\n\n\n\nSlide 18\n\n\n\n\nRecall we model text as a join distribution over all available tokens\nNow we model POS_tagged text: joint distribution over text and their POS tags\n\nOur goal is to assigned probabilities to the sequence paits \\((w_{1:l}, c_{1:l})\\)\n\n\n\n\n\nSlide 19\n\n\n\n\n\nRecall \n\n\n\n\n\nSlide 20\n\n\n\n\n\nW Words is a random variable for us, are tokens in Caligraphic_W (a vocabulary of symbols) that I know. The size of the vocabulary is V\nw is an specific word\nC is for Category and is a random variable, that are in Caligraphic_C (the tagset). The size of the tagset is K\nc is an specific tag\nX is a random sequence (our token sequence from lst week), from words from one until the lenght L in order\n\\(w_{1:l}\\) is a sequence of l words from the Caligraphic_W w/ vocabulary size V\nY is a random sequence\n\\(c_{1:l}\\) is a sequence of l tags from the Caligraphic_C w/ tagset size K\n\n\n\n\n\nSlide 21\n\n\n\n\nNow we want a distribution over the cross product of all Text and all Tags sequence: \\(w_{1:l}, c_{1:l}\\). This is an enormous space\n\n\n\n\nSlide 22\n\n\n\n\nYou can do two things with this:\n\nFor instance, I give you the ‘little cat’ and you want to know what is the prob sequence under the model. You can compute the possible probability sequence given that you give me a certain token sequence and then I could look for what maximizes this probability\nAnother we can do a language model, a language model was a mechanism to assign probabilities to strings without any tags, BUT if you get a distribution over two variables, token sequences and class sequences and you marginalized out one of the variables. ie you marginalized my choice for the first class, for the second class, all the way to the last class. If I sum the probabilities of all possible tag sequences that pair with these word sequence then I get a marginal probability. So in the above equation, the left part:\n\n\n\n\n\nSo that is the probability of a sentence so \\(P_X(w_{1:l})\\) this is a language model.\nSo once you design a model that can assign join probabilities to labeled sequences of words then you can use it to either:\n\ntag news sequences of words with their respective categories or\nassign prob to a sequence of words, regardless of which classes they have been annotated\n\nRemember NB classifier you can do:\n\nyou can annotate what is the most probable class of a document\nassign prob to a document regardless of their class (is the denominator of the NB model). Note the den of Bayes rule is the marginal\n\nThis second step is common to all generative model. Every time you have\n\n\n\n\nC1..C3 is the class tha sits at position 1..3\n\n\n\n\nSlide 23\n\n\n\n\nLast time we created a distribution and we created a new one, by chopping out sequences and working with probabilities for words given a short history, now we will do something similar but this time we are gonna built a distribution with pairs of sequences\n\n\n\n\nSlide 24\n\n\n\n\nSteps: a way of describing the generation of a tag sequence pair. And wathever we call step that is the unit we are gonna sign probabilities to.\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov Model Assumption\n\n\n\n\nHere is this image we have assume Markov assumption\nAt the tag level it looks like a bigram model and not somethingelse, we have made the assumption that when we have generated certain variable I can forget a lot of the precedding context all the classes that ocurred before except for the last one and even all the words that were generated before so this conditional independence is a markov assumption.\n\nWhy is hidden? originally it was decided so that you do not get to see which class is for each on every position, these are latent variable. Latent variables are the variables you pretend there is there but you dont observe its value. We do this because it makes modelling easier or more usefull for a certain purpose. Here we are observing both, the token sequence and the tag sequence but if you would not see the tag sequence yet pretend that it exists a modeling desing choice and it will be hidden for you.\n\n\n\nAt the class level this looks like a bigram because when we genreate a symbol. For instance when you generates a symbol for isntance when you generate ‘is’ (when VERB is generated) we can use the fact that you have just generated a NOUN this is like a bigram. To generate one word you can condition on the previous one but this time we are not generating words but are generating tag sequences\nThis model starts with the tag sequence not from the text , isnt that weird? Not really the NB started from class not from the text you generate the class and given the class you generate the words in the text with a conditional independence\nSo at the tag level we are doing bigrams language modelling, the orange stuff exits even without the blue stuff, so the orange stuff is never conditioning on the words. But the words comes from somewhere, from their ‘categories’. I.e if I would generate a verb and I know is a verb, isnt it easier to assign lower probabilities to nouns, I already know is a verb so maybe that in its own is gonna lead to conditional distributions that are much more compact because the set of verbs are more compact than the set of all words\n\nIt is not because of the factorization starts with classes and then generates words, that can be the other way around, we can observe a word sequence and ask: what is the most probable tag sequence.\nWhat is a factorization? It is a decomposition of products see next slide. So I decomposed the enttire sequence pair (the orange sequence and the blue sequence) into the probabilities of the number of circles with an arrow pointing to them.\n\nThe trick again you add the BoS and EoS, why do I do that? because I know that for every class I am generating there is something before to condition before (except in the first class that is why BoS fixes that) and the second reason is that I give to every single symbol a chance to be part of the conditional context that is what EoS is helpful for. In the img above it translates to also given teh punctuation a chance to assign the probability of a sequence ending\n\n\nLets imagine the following, lets take the words away just leave the categories? Now if we were an HHM so a model, then the question is give me a VERB. Then the answer is tha there is no way I would get the same verb twice. Some poeple reply: jump other: run so there is a ot of variation.\nBecause our model is not that advanced our model can: - describe statistical patterns of pair of classes that ocurr in a relative order, - it can explain the distribution of our words given the categories\nIn more concrete notes: the model can lear for instance that 80% of the time VERBS appear after right after NOUNS but it cannot sample the noun and the verb, the actual words together. You can get book is or book are, there is no agreement there which verb it will be.\nIs there a trap where it goes NOUN VERB NOUN VERB … but this is very unlikely if we do MLE it is very unlikely to fall in this trap because you would not see this traps in the data. It is unlikely if you do MLE, but if you start fideling with smoothing things get really weird.\nEvery time that a history h is given, this corresping 1:1 to a factorization, which we can get it whit the chain rule as follows:"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#hidden-markov-model",
    "href": "blog/2023-11-13_sequence-labelling/index.html#hidden-markov-model",
    "title": "Sequence Labelling",
    "section": "7 Hidden Markov Model",
    "text": "7 Hidden Markov Model\n\n\n\n\nSlide 26\n\n\n\n\nIn the slide above we forgot in the same qual to write above ‘idd’ assumption.\nThe \\(w_i\\) here is the word that is emitted for that particular class. We call that an emission probability"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#generative-history",
    "href": "blog/2023-11-13_sequence-labelling/index.html#generative-history",
    "title": "Sequence Labelling",
    "section": "8 Generative History",
    "text": "8 Generative History\n\n\n\n\nSlide 27\n\n\n\n\nWe can sample from an HMM by following the history.\n\nstart two sequence the token sequence (the words) and the label sequence (c_0 = BOS)\nCondition on the previous class so that is c_0 so condition on BOS, and extend Y with BOS. Whatever class you get extend your sequence to Y and move on. Here we use the transition distribution. Here the P(c_i|c_i-1) would be the transition distribution\n\n^ Here we draw a c_i and that we use for the next step\n\nCondition on current class c_i and draw a word w_i with probability P_w|c(w_i_i). So when we say random we meant np.random.choice([vector], weights=P_w|c(w_i_i)). Whatever word you get extend your sequence to X and move on\nFor instance you can get a Determiner, so do P_w|c(w_i)) and draw a word from it. Here we use the emission distribution.\n\nIf your sequence is too long then stop. And this would be the factorization of the HMM\n\n\n\n\nHere in the picture you start with BoS, then you fo to DET and then to the and then to NOUN and book and so on..\nWhich tasks is this model use for? in 2023 none.\n\n\n\n\nSlide 28\n\n\n\n\nIn the trasition distribution we would go from the prev class called ‘\\(r\\)’. Once you know it you can retrieve \\(K\\) probabilities one for each of the tags that you may trasition to. For instance \\(K=10\\) so NOUN, VERBS, ADJ, … so given one of them which we call it i.e ‘\\(r=DETERMINER\\)’ then I can retrieve \\(K\\) numbers which I called them \\(\\lambda_1, \\lambda_2 ... \\lambda_K\\) And these are the probability values for the conditional probability of drawing a NOUN given a DETERMINER, a VERB given a DETERMINER, a ADJECTIVE given a DETERMINER, ….\nNow for the Emission distribution or W|C, we need to specify:\n\nWhat are you emitting from, which class ie. a VERB, that is the little ‘\\(c\\)’. Once you know what are you emitting from, then you can retrieve a collection of \\(V\\) numbers which are \\(\\theta_1,\\theta_2,.. \\theta_V\\). These are the probability from which you draw a certain word in the vocabulary of known words given that you are emitting from little ‘\\(c\\)’ category.\n\nOnce you have all these tabels all the tabular representation, now you can assign a probability to any sequence pair by taking the product of the relevant number. So go to the entry (row) of the table that conditions on the previous tag and find the probability (per each column) of the current tag. Or go to the table of emissions find the row that corresponds to the current class and multiply the probability of generating the current word given that class so multiply it with the (emission distribution). You will find values from each of the tables and you will multiply them together"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#example",
    "href": "blog/2023-11-13_sequence-labelling/index.html#example",
    "title": "Sequence Labelling",
    "section": "9 Example",
    "text": "9 Example\n\n\n\n\nSlide 29\n\n\n\n\n\n\n\n\nWhat is the difference between this and what was done last time?\nAt an abstract level we have two sets of outcomes, we have a token sequence and a class sequence and there is also some difference in the parametrisation like what condition independences we make but the two are familiar and the tables are also familiar\nWhich then leads to the following question:"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#r-rightarrow-c",
    "href": "blog/2023-11-13_sequence-labelling/index.html#r-rightarrow-c",
    "title": "Sequence Labelling",
    "section": "10 \\(r \\rightarrow c\\)",
    "text": "10 \\(r \\rightarrow c\\)\n\nNow I give you data and I want to estimate the values of our parameters and I choose MLE, I give you data, meaning I give you word sequences (aka sentences i.e ‘A nice dog’) for which you know the type sequences (so their PoS: DETERMINANT, ADJECTIVE&lt; NOUN).\n\nThe question is then what is the probability of transition from a tag r –&gt; to a tag c i.e r=DETERMINANT, and c=NOUN. What is the probability?\n\nWe do not need Bayes RUle, we would need it if my query is not a parameter, for i.e if I ask what is the prob of the prev tag given that I know the current, then that is reversive the model so that is what you use Bayes Rule for.\nOur case is different we are asking a query that is a parameter of the model, the probability of given a class generating the next.\n\n10.1 The question:\n\n\nWhat is the probability of transition from a tag r –&gt; to a tag c i.e r=DETERMINANT, and c=NOUN. What is the probability?\n\n\n\n\n\n10.2 The answer:\n\n\n\n\nSlide 32\n\n\n\n\nSo how many times I have seen a DETERMINANT that was follow by a NOUN divided by the how many times you have count DETERMINANT follwed by anything\nThis is the solution for MLE: count(condition_on=DETERMINANT, the_outcome=NOUN)/sum_k count(DETERMINANT,k). The later means the DETERMINANT pair with any other tag from the tag set\nWhere k is any possible outcome in the set of things that you know"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#c-rightarrow-w",
    "href": "blog/2023-11-13_sequence-labelling/index.html#c-rightarrow-w",
    "title": "Sequence Labelling",
    "section": "11 \\(c \\rightarrow w\\)",
    "text": "11 \\(c \\rightarrow w\\)\n\n11.1 The question:\n\n\nWhat is the probability of generating w given c?\n\n\nThe model choice is an HMM the dataset is the ‘A nice dog’, the algorithm of choice is MLE.\n\n\n11.2 The answer:\nThe expression to compute the mission probability. So what is the probaility of emission of w given that have just generated class c.\n\n\n\n\nSlide 34\n\n\n\n\nNote: here in the denominator is the same, sum over all V with c, being fixed\n\n\n\n\nSlide 35\n\n\n\n\nThis approach still suffers from Data Sparcity, so the data contains plenty of zeros. For instance unseen word-tag pairs or unseen tag-tag pairs that are not too frequent and that we have not seen but this could happen. This can also happen as you tag set grows really large and maybe the tag set for a language like Arabic, Turkish, etc, i.e for these languages the tag set is really large so then maybe the transition probabilities aren’t easy to estimate. On the other hand, emission probabilities can be really sparce in the sense that i.e a language like Czeck or Portuguess where you have many word forms that are slight variation of the same thing and all have the same category so you would need a lot of data before you have seen every noun pop up or every verb.\nThis is less sparce compared to NGram LM where you have more data sparcity i.e because you need to memorize long phrases so is easy to find phrases that you have never seen thus creating sparcity.\n\nIn this sense, HMM are more compact than Ngram LM, but what is the limitation with this?\n\nLimitation\n\nWhen we generate a word all we know is a class, is hard to believe that if I sample from this model I would get nice sentences. I will get words that, if I extract from what they are and think only about the categories, then the trnasition from class to class would make sense, but when we look at the semantics (so the words then make no sense) i.e An furious bottle. The tags (A determinant then an adjective and then a noun) are okay but the semantics do not make sense\n\n\n\n\n\n\n\nPoS: DET, Prenomial ADJ Examples\n\n\n\n\n\n\n12 Examples of determinats can be:\n\nDeterminers (DET):\n\na: I saw a cat in the garden.\nan: She has an apple in her hand.\nthe: The sun is shining brightly.\nthis: I would like this book, please.\nsome: Can I have some water?\n\n\n\n\n13 Examples of Prenomial adjectives:\nPrenomial adjectives are adjectives that come before the noun they modify. Here are some examples of prenomial adjectives:\n\nThe red apple\n\n“red” is a prenomial adjective modifying the noun “apple.”\n\nA beautiful sunset\n\n“beautiful” is a prenomial adjective describing the noun “sunset.”\n\nThree large elephants\n\n“large” is a prenomial adjective indicating the size of the noun “elephants.”\n\nAn old book\n\n“old” is a prenomial adjective modifying the noun “book.”\n\nThe happy child\n\n“happy” is a prenomial adjective describing the noun “child.”\n\nSeveral interesting movies\n\n“interesting” is a prenomial adjective modifying the noun “movies.”\n\n\nThese examples illustrate how adjectives are positioned before the nouns they modify in a sentence.\n\n\n\n\n\n\nSyntactics\n\n\nWhen something is syntactically correct, it means that it adheres to the grammatical rules of a given language\n\n\nSyntax are the rules that dictate the order of words in a sentence and how they are structured to convey meaning.\nSemantic content of a sentence refers to the meaning or information conveyed by the arrangement of words and the relationships between them.\nGrammar\n\n\nGrammar rules refer to the set of structural and syntactic principles that govern how words are combined to form meaningful sentences and phrases in a language. These rules define the relationships between different elements of a language, such as nouns, verbs, adjectives, adverbs, and other parts of speech\n\nThis model main caveat is therefore if I abstract the words to which Category they belong, then they almost look like correct words i.e NOUN, VERB, ADVERB. But when we look at the semantic content it makes no sense\nAnother example you have a singular NOUN with a plural VERB\nHMM then are not able to make sentences but its able to cluster the words according to some vague distribution\n\n\n\n\n\n\nSlide 39\n\n\n\n\nThere is two things you can do with an HMM:\n\nObtain a tag sequence when you dont know it. So you know the word sequence and you ask: Give me the most probable tag sequence\nYou can use it as a Language Model\n\n\n\n\n\nSlide 40\n\n\n\n\nUnder this model, it spits out a tag sequence that is most probable. How do we evaluate this task sequence? you compare it to the tag sequence that is annotated in the dataset for you because PoS tag is almost unambiguous. For instance we will all agree of a tag sequence of english. So you compute the performance of the model as if you would in a classifier but instead of classifying once you have many classifications steps. You have one per step of the sequence so you compute accuracy. So then the performance is evaluate as accuracy across tag set."
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#examples-of-determinats-can-be",
    "href": "blog/2023-11-13_sequence-labelling/index.html#examples-of-determinats-can-be",
    "title": "Sequence Labelling",
    "section": "12 Examples of determinats can be:",
    "text": "12 Examples of determinats can be:\n\nDeterminers (DET):\n\na: I saw a cat in the garden.\nan: She has an apple in her hand.\nthe: The sun is shining brightly.\nthis: I would like this book, please.\nsome: Can I have some water?"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#examples-of-prenomial-adjectives",
    "href": "blog/2023-11-13_sequence-labelling/index.html#examples-of-prenomial-adjectives",
    "title": "Sequence Labelling",
    "section": "13 Examples of Prenomial adjectives:",
    "text": "13 Examples of Prenomial adjectives:\nPrenomial adjectives are adjectives that come before the noun they modify. Here are some examples of prenomial adjectives:\n\nThe red apple\n\n“red” is a prenomial adjective modifying the noun “apple.”\n\nA beautiful sunset\n\n“beautiful” is a prenomial adjective describing the noun “sunset.”\n\nThree large elephants\n\n“large” is a prenomial adjective indicating the size of the noun “elephants.”\n\nAn old book\n\n“old” is a prenomial adjective modifying the noun “book.”\n\nThe happy child\n\n“happy” is a prenomial adjective describing the noun “child.”\n\nSeveral interesting movies\n\n“interesting” is a prenomial adjective modifying the noun “movies.”\n\n\nThese examples illustrate how adjectives are positioned before the nouns they modify in a sentence."
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#example-with-3-positions",
    "href": "blog/2023-11-13_sequence-labelling/index.html#example-with-3-positions",
    "title": "Sequence Labelling",
    "section": "14 Example with 3 positions",
    "text": "14 Example with 3 positions\n\n\n\n\n\nMy task: what is the task for the first word, for the second word and for third word - We have a 3-word sentence and I need to know the most probable tag sequence for that sentence - Assume that the tag set contains two tags\n\n\nYou are asked what is the most probable tag sequence, the only way to know is to assign prob to each and every one of the options that has been numerated so every row. More concrete you need to assing a prob to the first row to the second and so on. Assigning prob to this is relatively easy, once the table of parameters exist I just go through my sequence (per each row) and collect the relevant probabilities (so multiply all the parameters in one row)\nSuppose we already did that So the table is filled with numerical values, meaning I will have one join prbability value for each one of the options. So now we could sort the list and pick the best\n\n\n\n\n\nSlide 43\n\n\n\n\n\nSo the size of this computation would be K_1position, K_2position, K_3position, K_Lposition so K^L\n\nBecause this is exponential is expensive to compute\n\n\n\n\nSlide 44\n\n\n\n\nDynamic programing is when you program such that it solves smaller problems whose algorithm for solving repeats itsleve and it is not generally available meaning to every programm can be dinamyc, you need to design your model with careful choices such that a dynamic program can be used. HMM can fit into a Dynamic programm. The key to the dynamic program is to look closely to my screenshots (the img above) and realized that most probabilities are the same anyway. So each row is the join probability of assing to one of the tag sequences and all the numbers are same basically everywhere so there is a lot of structure that repeats itself, the idea is to split into subproblems solve the subproblems and combine their solutions\nIn the following video we see how to do that:\n\n\n\n\n\n\n\n\nSlide 45\n\n\n\n\nThe key is: pretend there is a table alpha(i, j) which tells you if I were tagging the ith position with the jth tag i.e so the 10th position I will make it a NOUN, then I can solve this problem by combining the solutions to the previous one: the 9th position and I can do that if I know the 8th position and so on, so this becomes recursion. You can solve this if you have solve the ones before and by structuring this in a recursive call\nIn the \\({\\) the one above is the transition emission pair for that position. You do not need recusion to implement this you can also do for-loop\nRecall: the 2. thing can be done with an HMM is to marginalized out the tag sequences.\n\n14.1 The concept of marginalization out\nImagine there is two variables, a person and a route to the university. Each path to the univerisy will cost an amount of time. If you ask the question how does does it take you on average to get to work and you have no knowledge of the path that was taken then you reason all paths could have done then you sum all the cost of all the paths and averga them out\n\n\n14.2 What is marginalization\nSo if you have two or more variables, marginalization menas fix some of them and for whathever is left enumerate all hte possible outcomes and sum their probabilities.\n\n\n\n\nSlide 46\n\n\n\n\nWe have the join probability distribution \\(P_{XY(w_{1:l},c_{1:l})}\\)\n\nThere is two sequences: words and classes.\nIf we ask: regardless of class what is the probability of a sentece i.e ‘the nice dog’, they are not asking the nice dog which is a DETERMINATE, ADJECTIVE NOUN, no. They are not asking that. They are asking the prob of the sentence so then we use marginalization like the formula below:\n\n\\[\nP(X = x) = \\sum_{y} P(X = x, Y = y)\n\\]\n\n\n\n\nThis is an exmaple, so based on the formula from SLide 46, we have L=3 words K=3 tags\n\nWe do not know the categories, so need to try them all. We proved before that there is an exponentially K^L growing number of how to assign tags so then we enumerate all this possible outcomes meeaning we enumerate per each row each of the possible combinations\nFor each of these rows, I know how to assing a join probability, I used the relevant \\(\\lambda\\) and the relevants \\(\\theta\\) and then I multiply them toguther all these (per row) and this gives me a number\n\nRecall: - Before I was looking for the one tag sequence for which the joint probability was maximum\nNow: - I am marginalizing out, now we are summing all the alternative probabilities. So all the alternative ways I summed them up (so in each columns beecause here we have different assignment of tags) and the result of summing along the columns (which then we have like a vector of sequence L, so the sentence lenght) and now we sum this vector( so each element in this vector will correspond to one word with all the classes summed over) is the probability of the sequence aka the sentence regardless of tag sequences because we wa\n\n\n\n\nThe algorithms that takes care of this is called the Forward Algorithm where we are interested in find the prob of the sentence regardless of their POS tags\n\n\n\n\n\n\n14.3 In short:\n\nYou are asked what is the most probable tag sequence: Viterbi\nProbability of the sentence regardless of their POS tags: Forward\n\nSide note: the thing that unites these two algorithm is called Value Recursion explained in the video"
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#coming-back-to-the-yt-video-value-recursion-for-hmms",
    "href": "blog/2023-11-13_sequence-labelling/index.html#coming-back-to-the-yt-video-value-recursion-for-hmms",
    "title": "Sequence Labelling",
    "section": "15 Coming back to the YT video: Value recursion for HMMs",
    "text": "15 Coming back to the YT video: Value recursion for HMMs\n\nFormula for HMM:\n\n\n\n\n\n\n\nHere \\(P_{XY}()\\) is the join probability of words and tags\n\n\n\n\n\n\nIn reality though you do not know the tag sequence and you only have a word sequence which you assume it was generated with an HMM generative history. With this you are interested in predicting:\n\nWhat the most probale sequence could have been that is the task of post tagging\n\n\nHere we look at the tag sequence that maximizes the posterior probability for a given input w\n\n\nYou may be interested for a language model and what you want to do is evaluate the marginal probability of the word sequence, that is the total probability regardless of what tag sequence may have generated this text. There you would asses the join probaility for each and every configuration from the tag sequence and sum all those probabilities toguether.\n\n\n\n\n\n\n\n15.1 The Value recursion\n\n15.1.1 Forward Algorithm\n\nImagine you have a sequence c1 trhough c_i which are the tags for sequence w1 throuhg w_i, where w1=the and so on.\n\nAlpha would be the marginal probability of all sequences that end with the assigment \\(c_{i=j}\\). That is making a choice for the ith position and that choice being in the jth tag position. i.e. look at \\(\\alpha(i=3, j=B)\\) (recall i=word_position, j+tag_class). Then we are taking about c1, c2 and c3=B, C1 having generated ‘the’\nWe marginalize C1 and C2, which means we trying all the posibilities:\n\nA A B\nA B B\nB A B\nB B B\n\nWe try all these posibilitis, we asses the joint probabilities, so the whole row of multiplications per each combination so for instance in the first one we would have:\n\nA A B\n\n\n\n\n\nA multiplication of all the parameters with classes A A B. Then for the second combination A B B and so on\nOnce we have calcualted all these combination we add them all toguether and thdt quantity which is the marginal probability is what we store in the function \\(\\alpha (i,j)\\)\nNow with this \\(\\alpha (3,B)\\) we can expressed in in therm os \\(\\alpha (2,A)\\) and \\(\\alpha (2,B)\\) and so on.\n\n\n\n\n\n\n\n\n\n\nRecaping Forward Algorithm\n\n\n\nThe forward recursing boilds down to \\(\\alpha (i,j)\\) which is the marginal probability of the assigment where we have generated all the wrods all they way until the ith, the ith word is tagget with the jth tag in the tagset and the sequence up unil that variable has been marginalized\n\n\nWhen i is more than one then we have a recursive call to alpha but evaluated in the previous position. So escentially to evaluate alpha(i,j) we check for all candidate tags that could be set in the previous psoition with the fixed word and we asses alpha as if that tag was indeed the tag assign to the previous position and then we transition ot the jth tag and we omit the ith word.\nalpha(4, ESO) is the probability that we are looking for because it evaluates the joint probabilities of all sequences and adds them toguether\n\n\n\n\nComplete Forward algorithm:\n\n\n\n\n\n\n\n15.2 Viterbi Algorithm\nNow we want to maximize our choice, we want the probability of the best sequence ending in (c_i=j, w_i=w_i)\nThe probability of the sequence that ends in c_i=c_j and emits the ith words from the jth tag is either the probability of the transition emission probability pair when i=j, or when is the second term of the alpha function.\n\n\n\n\nSo now the best sequence that ends up in the sequence (C_2=A, W_2=nice). Suppose for a moment that going to A is the best choice/the one that maximizes, then to get to alpha(2,A) = alpha(1,A) * \\(\\lambda_A^{A} \\theta_{nice}^{A}\\) is the biggest among the other path posibility so the path below (aplha(1,B)*…)\nNow for alpha(2,B) the best value is by going from alpha(1,A)*\\(\\lambda_A^{B} \\theta_{nice}^{A}\\)\n\n\n\n\nSo if you implement a table of alpha(i,j) values and another table of Backpointers, then you have everything that you need to compute the probability (the sums of probs from the first table) and to compute the tag sequence (from the Backpointers table) and thus outputs the best tag sequence to the user.\n\n\n15.3 Notes on these algorithms\nTo solve all htse computation we want to do it in a logarithm scale because logs convert products into sums leading to more numerically stable computations. For that we need to do some extra mods to the alpha function. With these three forumlas in red I can rewrite alpha into somthing called value recursion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlide 53\n\n\n\n\n\n\n\n\nSlide 54\n\n\n\n\nAn HMM tagger would be a joint distribution over both tag sequences and token sequences but if you are not interested in assigninig probabilities to words or generating words then you may model with a different technique\nSo now the goal is not a generative model that you could use for taggins or for Language Modelling, now the goal is just to have a tagger.\nOther types of taggers may fit in another tools. For instance see next slide:\n\n\n\n\nSlide 55\n\n\n\n\nThere is still applications of this things, they tipically power systems for information extraction for question answering. So it is a form of labelling task with a bit of a smactic twist, I am not interest in the syntatic category of a word in its context but rather I am interest in recognizing mentions to something that is an entity in the real word thats why we call it Name-Entity. For instance ‘America Airlines’ we tagged with some Name entity.\n\nWhy this would be usefull you may ask? suppose you are dealing with question-answering then perhaps by doing name-entity recognition you find the spans of text for which you likely have a wikipedia page or an entry in some knowledge base.\n\nThe idea is to identify the blue spans, maybe it does not look like labelling, like it looks quite different from Speach tagging but now lets look at the nxt slide:\n\n\n\n\nSlide 56\n\n\n\n\nIt is a transformation of the dataset, so the dataset has been prepoceed slighly and now it is a labelling task\n\nI: inside of span\nB: beginnig\nO: Outside\nS: Single token span i.e S-LOC single location\n\nEven though is a sequence labelling task we have one label per token in the token sequence, we are construvting little brackets, because now we are setting the inside of a tag or the end of a tag or the beginnig and so on.\n\n\n\n\nSlide 57\n\n\n\n\n\n\n\n\nSlide 58\n\n\n\n\nFor example they all have the same semantical meaning but different realizations, so the sentences have in common the same meaning but expressed in a different way\n\n\n\n\nSlide 59\n\n\n\n\n\nPrototypical Semantic Roles\n\n\n\n\nSlide 60\n\n\n\n\nAnnother sequence labelling task: see next slide\n\n\n\n\nSlide 61\n\n\n\n\nHere you assing semantic roles to spans in sentences so now you think of a sentence that is specific to a VERB, so given the semantics of BROKE for i.e then Jhon is to be labeld as the AGENT and the window acts as a span of THEME\nSo the sentences from 3 to 5 are different with little change. Where would this information be? it must be in the lexicon it something about Jhon and something about being a rock that makes one thing the agent and another the instrument, so it is not in the syntax of the sentence it is not in the grammar it is really in the selection of preferences of VERBS and the attributes of NOUNS\n\n\n\n\nSlide 62\n\n\n\n\nIn these slides are two verbs in their first senses\n\n\n\n\nSlide 63\n\n\n\n\nHere if I have 7 verbs then I have sevent semantic role sequences I have: arg0… arg1, till arg6 .In POS tagging their is one sequence for one input sequence. Here is a bit different, here for every verb you have one sequence, because foe every verb you look at the sentence and you can interpret who is the agent, who is doing what to who, but if you focus in a different VERB in a the same sentence then different spans of texts will play different roles. So for every single verb you encounter in the sentence there would be a corresponding tag sequence i.e seeing at the columns on the table #1, #2 … #7. These are the tag sequences for each one of the verbs\nImagine you are designinig a semantic role labeller and you are given a sequence of inputs and there are two settings one settings is somebody tells me please tag the sequence for the verb ‘implement’ and then your model would ideally ouput something like the last column\nOr I give you the sentence and I say tag the sentence for semantic roles for the verb ‘lighten up’ and then you would predict soemthin similar like in column number 3\n\n\n\n\nSlide 64\n\n\n\n\nThe scope is now to map a sequence of words to a sequence of tags but doing that with respect to a given position because for each tth meanining the verb of interest i.e ‘use’ then wrt to that position this is the output tag sequence\n\n\n\n\nSlide 65\n\n\n\n\nIf we can express a task as annotating tokens in sequence and for every token I have tag, then normally this tags comes from a finite set. That means that an HMM is available. You could develop an HMM for psot tagging, you could develop an HMM for Nameed-Entity recognition. For isntance for pos tagging you change the tag space and maybe you motivate one or the other variant of IOBES and you could do HMMs, you could use it. A not soo god example is SLR because you cannot phrase as every token gets one tag, it is more like every token gets a tag for a certain verb. So if I change the verb then I have another tag sequence so you have a variable number of outputs sequences, one ouput sequence per verb in the token sequence. This lead us to the conlsuion of SLR are note good candidates for HMMs.\nVerbs –&gt; SRL\nHowever, even in the case that HMMs is a good choice like in post tagging and named-entity recognition it can be argued that it maynot be a choice in general because I know where I am applying it. I am applying it for the purpose of labelling sequences with some linguistic signals and if I am not interest in Language Modelling I do not care about assigning probs to the text. Thus we dont care about probs then there may be better techniques."
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#limitation-of-the-hmm",
    "href": "blog/2023-11-13_sequence-labelling/index.html#limitation-of-the-hmm",
    "title": "Sequence Labelling",
    "section": "16 Limitation of the HMM",
    "text": "16 Limitation of the HMM\n\n\n\n\nSlide 66\n\n\n\n\n\nCannot model long term dependencies, it is very local. Token are not even depent on one another they are conditionally independent given the tags where they were generated from\nThe context is only of the previous tags it does not use linguistics context it use some sintatic context but very limited one just the previous tags\nAs a tagger you are given good english and you are mapping to tag sequences and what happens is that if the tag for a certain position depends on a word that is very far away from it then the HMM cannot see. For instance some name entities may have long names for instance United States Airles, or people with long names. Conclusion HMMS cannot reduce the entrophy of what the label of a certain category may be you need to look far and the HMMs cannot do that\n\n\n\n\n\nSlide 67\n\n\n\n\nBecause the HMMs need to generate text they use very farily limited use of linguistic context in \\(w_{1:L}\\). Using more of thse context in a generative model it will break certain algorithms that you need for training and using these models like the vitori or forward algorithm\nSo to allow a category to interact with more words you would break those algorithms you would make then not scalable.\nIt will also make our CPDs more sparce if we try to memorize more phrases everything gets sparce\n\n\n\n\nSlide 68\n\n\n\n\nUnseen wors and phrases will come out and the HMM would be lost\nIt may also be cases that we would like to extract the fine features of a word fors instance ending in ‘-ed’ or stating with ‘un’ and so on but this cannot be by the HMM because it cannot analyse more fine-grained features."
  },
  {
    "objectID": "blog/2023-11-13_sequence-labelling/index.html#how-to-move-from-the-hmms",
    "href": "blog/2023-11-13_sequence-labelling/index.html#how-to-move-from-the-hmms",
    "title": "Sequence Labelling",
    "section": "17 How to move from the HMMs?",
    "text": "17 How to move from the HMMs?\n\n17.1 First idea: use feature-rich models\n\n\n\n\nSlide 69\n\n\n\n\nLet’s imagine I want to tag the 4th word. I could collect features from the surronding context of that position so I could get features from a windows to the left or a window from the right and features from the word cute itself. In the features I have this vector where I only have one \\(1\\), I could have design more features as: ‘is there capital letters’, ‘does it end in ed?’ and so on more features.\nThis table is bigger in the linguistic context so its more rich than the HMM These are handmade feature vectors\n\n\n17.2 Second idea:desing one classifier and use it many times\n\nHere we map the features of the context to the prob values of the clases using a log linear model\n\n\n\n\n\nSlide 70\n\n\n\n\nFrom the feature vector ‘cute’ predict a vector of probability values as large as the number of classes that adds up to one. Lets explain: if you have a feature representation of a context and your goal is to spit out a prob vector, then you could use a linear model.\nSo from however many features you have, you do a linear operation to obtain exaclty the number of classe you are working withs. For sintance here 11 clases and I am working with 3V features, then I do one linear transformation from 3V features to 11. This vector with 11 cordinates in it, these are not probabilities are real values to force them to become prob we use Softmax. K=11 calsses, D is the dimensionality of the feature vector, if we are using the feature vector from the previous slide that is 3V. So then the number of weights would be 3V * K. Because for every feature you wanna get the importance of the feature towards a class. And also you get the biases which you could think of as the margin frequency of the class regardless of any features. One linear transform maps from D dimensions to K dimensions this thigns are called ‘scores’, ‘logits’. Logits can be though of log probabilites that are not normalized and then the softmax functions maps to K probs.\nWhat we achieve by this is that no matter with position you are you can use the same model, it does not get bigger or smaller to tag the first word, the second, the … its just one classifier that can be sued over and over.\n\n\n\n\nSlide 71\n\n\n\n\nIf I have a sentence from \\(w_{1:l}\\) and I am looking at an specific position of it with those two things I can get the prob values for the classes that I may classify. That is the \\(f\\) the prob vector, one prob per each class. Yuo decide how to decide \\(f\\)\n\n\n\n\n\n\n\nSlide 75\n\n\n\n\nWe have created a tagger by not considering the sequence but only about classification. The idea is that for every position in the sequence that you would want to tag you pretend this is a classification task you dont care that you are actually tagging and entire sequence. You classify one position at the time independently of what you do for other positions. The key for this to work is to featurize the context in which you perfom the clasifications so if I want to classify ‘united’ I have a feature vector that describes best I can ‘united’ in the sentence meaning I am ‘united’ there is adimension for that. There is a feature for ‘my neighboor is an upper case letter’.\n\nIf you do independent classification, so independetly of what you do in other steps do you see a problem in the picture above where I have annotated where is the beginning, inside and end of the span (meaning there is an structure)? If I perform independent decision it is easy to programm but my lead to a problem. The problem is:\nYou get nonsense tag sequences: [O I O ] here you cannot be inside if you have not even enter one\nThe problem with tagging independetly then is that tagging is ins’t made of independent stps and specficially the structure that you want to output it contains constraints so the ouput sequence is constraint not any scramble of tags would do. A better one would be [O S I O] as in ot"
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "",
    "text": "MLE vs MAP in the context of parameter estimation\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 26, 2023"
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#definition",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#definition",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "Definition",
    "text": "Definition\nMaximum Likelihood Estimation (MLE) and Maximum A Posteriori Estimation (MAP) are two common statistical methods used for parameter estimation in various fields, including machine learning and statistics. They are often used in the context of estimating parameters of a statistical model or distribution. Here are the key differences between MLE and MAP:"
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#objective",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#objective",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "Objective:",
    "text": "Objective:\n\nMLE (Maximum Likelihood Estimation): MLE aims to find the parameter values that maximize the likelihood function, which measures how well the observed data fits the model. In other words, it seeks the parameter values that make the observed data most probable under the assumed model.\nBasically, I have prob density ie a Gaussian. I take its log and then compute the derivate of it w/ rspect to some variable of interest i.e \\(\\mu\\), or \\(\\underline{w}\\)\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Generate random data from a Gaussian distribution\nmean = 0  # Mean of the Gaussian distribution\nstd_dev = 1  # Standard deviation of the Gaussian distribution\nsample_size = 10000  # Number of data points\n\ndata = np.random.normal(mean, std_dev, sample_size)\n\n# Create a figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n\n# Plot the Gaussian distribution on the left subplot\nax1.hist(data, bins=50, density=True, color='blue', alpha=0.7, label='Gaussian Data')\nax1.set_xlabel('x')\nax1.set_ylabel('PDF')\nax1.set_title('Gaussian Distribution')\nax1.grid(True)\n\n# Calculate the PDF of the Gaussian distribution for plotting\nx = np.linspace(mean - 4 * std_dev, mean + 4 * std_dev, 1000)\npdf = norm.pdf(x, mean, std_dev)\n\n# Plot the log PDF on the right subplot\nlog_pdf = np.log(pdf)\nax2.plot(x, log_pdf, 'r-', label='Log PDF of Gaussian')\nax2.set_xlabel('x')\nax2.set_ylabel('Log PDF')\nax2.set_title('Log PDF of Gaussian Distribution')\nax2.grid(True)\n\n# Show legends\nax1.legend()\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nMAP (Maximum A Posteriori Estimation): MAP, on the other hand, incorporates prior information about the parameters into the estimation process. It seeks the parameter values that maximize the posterior probability, which combines the likelihood of the data and the prior probability of the parameters.\n\n\n\n\n\n\n\nWant to know more?\n\n\n\n\n\n\nIncorporation of Prior Information:\nMLE: MLE does not consider any prior information or beliefs about the parameters. It solely relies on the likelihood of the observed data. MAP: MAP explicitly incorporates prior beliefs or information about the parameters through the prior probability distribution. This makes MAP especially useful when you have some prior knowledge about the parameters.\n\n\nFormulation:\nMLE: The MLE estimate for a parameter is obtained by maximizing the likelihood function, typically by taking the derivative of the likelihood function with respect to the parameter and setting it to zero. MAP: The MAP estimate for a parameter is obtained by maximizing the posterior probability, which is proportional to the product of the likelihood and the prior probability. Mathematically, it involves finding the mode of the posterior distribution.\n\n\nRobustness to Small Sample Sizes:\nMLE: MLE can be sensitive to small sample sizes because it tends to overfit the data when the sample size is small. MAP: MAP can provide more stable estimates in situations with limited data because it incorporates prior information, which can act as regularization.\n\n\nInterpretability:\nMLE: MLE estimates tend to be more data-driven and may not incorporate external knowledge. They are often considered more objective. MAP: MAP estimates can be influenced by prior information, which can introduce subjectivity into the estimation process. The choice of the prior distribution can significantly impact the MAP estimates.\nIn summary, the main difference between MLE and MAP is the incorporation of prior information. MLE seeks the parameter values that maximize the likelihood of the data, while MAP combines the likelihood with prior information to find parameter values that maximize the posterior probability. The choice between MLE and MAP depends on the specific problem and the availability of prior knowledge about the parameters."
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#incorporation-of-prior-information",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#incorporation-of-prior-information",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "Incorporation of Prior Information:",
    "text": "Incorporation of Prior Information:\nMLE: MLE does not consider any prior information or beliefs about the parameters. It solely relies on the likelihood of the observed data. MAP: MAP explicitly incorporates prior beliefs or information about the parameters through the prior probability distribution. This makes MAP especially useful when you have some prior knowledge about the parameters."
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#formulation",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#formulation",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "Formulation:",
    "text": "Formulation:\nMLE: The MLE estimate for a parameter is obtained by maximizing the likelihood function, typically by taking the derivative of the likelihood function with respect to the parameter and setting it to zero. MAP: The MAP estimate for a parameter is obtained by maximizing the posterior probability, which is proportional to the product of the likelihood and the prior probability. Mathematically, it involves finding the mode of the posterior distribution."
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#robustness-to-small-sample-sizes",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#robustness-to-small-sample-sizes",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "Robustness to Small Sample Sizes:",
    "text": "Robustness to Small Sample Sizes:\nMLE: MLE can be sensitive to small sample sizes because it tends to overfit the data when the sample size is small. MAP: MAP can provide more stable estimates in situations with limited data because it incorporates prior information, which can act as regularization."
  },
  {
    "objectID": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#interpretability",
    "href": "blog/2023-09-26_maximum-likelihood-estimation-(mle)-vs-maximum-a-posteriori-estimation-(map)/index.html#interpretability",
    "title": "Maximum Likelihood vs Maximum a Posteriori Estimation",
    "section": "Interpretability:",
    "text": "Interpretability:\nMLE: MLE estimates tend to be more data-driven and may not incorporate external knowledge. They are often considered more objective. MAP: MAP estimates can be influenced by prior information, which can introduce subjectivity into the estimation process. The choice of the prior distribution can significantly impact the MAP estimates.\nIn summary, the main difference between MLE and MAP is the incorporation of prior information. MLE seeks the parameter values that maximize the likelihood of the data, while MAP combines the likelihood with prior information to find parameter values that maximize the posterior probability. The choice between MLE and MAP depends on the specific problem and the availability of prior knowledge about the parameters."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "\nPython\n",
    "section": "",
    "text": "In this section Danilo writes down his notes, the things he has figured out about his computer, and ocasionally when feeling inspired he writes poems.\n\n\n\n\n\n\nShowing All Posts\n\n \n\nShow All Posts \n\n\n\n\n\n\n\nNotes\n\n\n\n\n\n\n\n\n #   published: \n\n\nPython\n\n\n\n\n\n\n\n\n\n     \n    \n        \n        \n            \n            1\n        \n            \n        \n        \n            \n            2\n        \n            \n        \n        \n            \n            3\n        \n            \n        \n        \n            \n            4\n        \n            \n        \n        \n            \n            5\n        \n            \n        \n        \n            \n            6\n        \n            \n        \n        \n            \n            7\n        \n            \n        \n        \n            \n            8\n        \n            \n        \n        \n            \n            9\n        \n            \n        \n        \n        \n            \n        \n        \n            \n            10\n        \n            \n        \n        \n            \n            11\n        \n            \n        \n        \n            \n            12\n        \n            \n        \n        \n            \n            13\n        \n            \n        \n        \n            \n            14\n        \n            \n        \n        \n            \n            15\n        \n            \n        \n        \n            \n            16\n        \n            \n        \n        \n            \n            17\n        \n            \n        \n        \n        \n            \n                \n                1\n        \n        \n        \n            \n                \n                2\n        \n        \n            \n            18\n        \n            \n        \n        \n            \n            19\n        \n            \n        \n        \n            \n            20\n        \n            \n        \n        \n        \n            \n        \n        \n            \n            21\n        \n            \n                \n                3\n        \n        \n            \n            22\n        \n            \n                \n                4\n        \n        \n            \n            23\n            \n            24\n        \n            \n                \n                5\n        \n        \n            \n            25\n            \n            26\n        \n            \n                \n                6\n        \n        \n            \n            27\n            \n            28\n        \n            \n                \n                7\n        \n        \n            \n            29\n            \n            30\n        \n            \n                \n                8\n        \n        \n            \n            31\n        \n            \n                \n                9\n        \n        \n        \n            \n        \n        \n            \n            32\n            \n            33\n        \n            \n                \n                10\n        \n        \n            \n            34\n            \n            35\n        \n            \n                \n                11\n        \n        \n            \n            36\n        \n            \n                \n                12\n        \n        \n            \n            37\n        \n            \n                \n                13\n                \n                14\n        \n        \n        \n            \n                \n                15\n        \n        \n            \n            38\n        \n            \n                \n                16\n                \n                17\n        \n        \n            \n            39\n        \n            \n                \n                18\n        \n        \n            \n            40\n        \n            \n                \n                19\n        \n        \n            \n            41\n        \n            \n                \n                20\n        \n        \n            \n            42\n        \n            \n                \n                21\n                \n                22\n        \n        \n            \n            43\n        \n\n2023\n    \n    \n                \n                        \n                            \n                                Dec 14\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Self-supervised Learning I NEW\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 14\n                        \n                \n                \n                        \n                            \n                                Dec 07\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Deep Learning & The Natural Sciences\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 07\n                        \n                \n                \n                        \n                            \n                                Dec 07\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Generative modelling and Deep Variational Inference\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 07\n                        \n                \n                \n                        \n                            \n                                Dec 07\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Generative Adversarial Networks and Diffusion models\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 07\n                        \n                \n                \n                        \n                            \n                                Dec 03\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Interpretability of NLP models\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 03\n                        \n                \n                \n                        \n                            \n                                Dec 03\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Discourse processing\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 03\n                        \n                \n                \n                        \n                            \n                                Dec 03\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Large language models\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Dec 03\n                        \n                \n                \n                        \n                            \n                                Nov 27\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Equivariances & Graphical Neural Networks\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 27\n                        \n                \n                \n                        \n                            \n                                Nov 27\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Attention & Transformers\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 27\n                        \n                \n                \n                        \n                            \n                                Nov 25\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Why using Logit Scaling in Softmax?\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 25\n                        \n                \n                \n                        \n                            \n                                Nov 22\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Lexical semantics and word embeddings\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 22\n                        \n                \n                \n                        \n                            \n                                Nov 22\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Compositional semantics and sentence representations\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 22\n                        \n                \n                \n                        \n                            \n                                Nov 20\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Convolutional Neural Networks\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 20\n                        \n                \n                \n                        \n                            \n                                Nov 20\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Modern ConvNets\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 20\n                        \n                \n                \n                        \n                            \n                                Nov 20\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Deep Learning Optimizations II\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 20\n                        \n                \n                \n                        \n                            \n                                Nov 13\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Modelling Syntactic Structure\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 13\n                        \n                \n                \n                        \n                            \n                                Nov 13\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Sequence Labelling\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 13\n                        \n                \n                \n                        \n                            \n                                Nov 08\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Deep Learning Optimizations I\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 08\n                        \n                \n                \n                        \n                            \n                                Nov 07\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Numpy reshape RGB image to 1D array\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 07\n                        \n                \n                \n                        \n                            \n                                Nov 05\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Script to extract pages of PDF as images & apply crop\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 05\n                        \n                \n                \n                        \n                            \n                                Nov 05\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Deep Feedforward Networks\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 05\n                        \n                \n                \n                        \n                            \n                                Nov 02\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Language modelling\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 02\n                        \n                \n                \n                        \n                            \n                                Oct 15\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Kernel methods & SVM\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 15\n                        \n                \n                \n                        \n                            \n                                Oct 12\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        The Covariance Matrix and relation with PCA\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 12\n                        \n                \n                \n                        \n                            \n                                Oct 12\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Bayes Rule Equation\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 12\n                        \n                \n                \n                        \n                            \n                                Oct 11\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Principal Component Analysis (PCA)\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 11\n                        \n                \n                \n                        \n                            \n                                Oct 07\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Latent Variable Models & K-Means Clustering\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 07\n                        \n                \n                \n                        \n                            \n                                Oct 04\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Neural networks\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 04\n                        \n                \n                \n                        \n                            \n                                Oct 04\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Logistic Regression\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 04\n                        \n                \n                \n                        \n                            \n                                Oct 03\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        The Perceptron\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Oct 03\n                        \n                \n                \n                        \n                            \n                                Sep 26\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Matrix Calculus & Derivatives Ax\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Sep 26\n                        \n                \n                \n                        \n                            \n                                Sep 26\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Maximum Likelihood vs Maximum a Posteriori Estimation\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Sep 26\n                        \n                \n                \n                        \n                            \n                                Sep 25\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Classification and Decision Theory\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Sep 25\n                        \n                \n                \n                        \n                            \n                                Sep 05\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Probability Theory in Machine Learning\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Sep 05\n                        \n                \n                \n                        \n                            \n                                Sep 04\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        MNIST Classification\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Sep 04\n                        \n                \n                \n                        \n                            \n                                Aug 21\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Automating the creation of Blog posts\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Aug 21\n                        \n                \n                \n                        \n                            \n                                Aug 18\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        How to use widgets in jupyter notebooks\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Aug 18\n                        \n                \n                \n                        \n                            \n                                Jul 24\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Picking the right tool to show your Machine Learning project\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Jul 24\n                        \n                \n                \n                        \n                            \n                                Jun 09\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        On the topic of Optimization\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Jun 09\n                        \n                \n                \n                        \n                            \n                                Jun 05\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        How PCY Algorithm works\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Jun 05\n                        \n                \n                \n                        \n                            \n                                May 29\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Markdown structure, titles and CSS\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                May 29\n                        \n                \n                \n                        \n                            \n                                Jan 29\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Displaying jupyter notebooks\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Jan 29\n                        \n                \n                \n                        \n                            \n                                Jan 12\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        2022 into 2023\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Jan 12\n                        \n                \n    \n\n\n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            NLP\n        \n         \n            NLP\n        \n         \n            NLP\n        \n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            NLP\n        \n         \n            NLP\n        \n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            NLP\n        \n         \n            NLP\n        \n         \n            Deep Learning\n        \n         \n            Deep Learning\n        \n         \n            NLP\n        \n         \n            Education\n        \n         \n            Education\n        \n         \n            Education\n        \n         \n            Education\n        \n         \n            Machine Learning\n        \n         \n            Education\n        \n         \n            Machine Learning\n        \n         \n            Education\n        \n         \n            Machine Learning\n        \n         \n            Education\n        \n         \n            Machine Learning\n        \n         \n            Education\n        \n         \n            Education\n        \n         \n            Machine Learning\n        \n         \n            Education\n        \n         \n            Machine Learning\n        \n         \n            Machine Learning\n        \n         \n            Extension\n        \n         \n            DevOps\n        \n         \n            Workflow\n        \n         \n            Data Mining\n        \n         \n            Markdown\n        \n         \n            Analysis\n        \n         \n            News\n        \n         \n            Python\n        \n         \n            Python\n        \n         \n            Probability\n        \n         \n            Linear models\n        \n         \n            Python\n        \n         \n            Python\n        \n         \n            Python\n        \n         \n            AI\n        \n         \n            Vector Calculus\n        \n         \n            Classification\n        \n         \n            Probability Theory\n        \n         \n            Python\n        \n         \n            Quarto\n        \n         \n            Python\n        \n         \n            Python\n        \n         \n            Python\n        \n         \n            Shiny\n        \n         \n            Lua\n        \n         \n            Python\n        \n         \n            Testing\n        \n         \n            Testing\n        \n         \n            Python\n        \n         \n            2023-table\n        \n\n\nNo matching items\n\n\n\n\n\n\n\n     \n    \n        \n        \n            \n            1\n        \n\n2022\n    \n    \n                \n                        \n                            \n                                Nov 05\n                            \n                            \n                            \n                        \n                        \n                            \n                            \n                                \n                                        Welcome To My Blog NEW\n                                \n                            \n                            \n                        \n                        \n                            \n                            \n                            \n                                    \n                                    \n                                            \n                                                All\n                                            \n                                                                        \n                            \n                        \n                        \n                            \n                            \n                            \n                            \n                                Nov 05\n                        \n                \n    \n\n\n         \n            News\n        \n         \n            2022-table\n        \n\n\nNo matching items\n\n\n\n\n\n  \n  \n    \n    CATEGORIES\n      \n    \n      \n  \n  \n  \n    \n      TAGS"
  },
  {
    "objectID": "blog/2023-11-07_numpy-reshape-rgb-image-to-1d-array/index.html",
    "href": "blog/2023-11-07_numpy-reshape-rgb-image-to-1d-array/index.html",
    "title": "Numpy reshape RGB image to 1D array",
    "section": "",
    "text": "Numpy reshape RGB image to 1D array\n        \n        \n                    \n                \n                    Description of this Notebook\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 7, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                    \n                 Code\n                        \n            \n    \n\n\nImagine you want to train a NN with:\n\nX: s x 3 x 32 x 32\n\nWhere:\n\ns: num samples\n3 x 32 x 32: color img\n\nNow to feed in the NN we want the shapes to be:\n\nX: s x (3072)\n\nDo we lose information? The answers is not, we just reshape X.\nThe following takes 2 imgs, with dimensions 2x2:\n\nimport numpy as np\n\n# Create example 3x2x2 images (two images)\noriginal_images = np.array([[[[1, 2],\n                             [3, 4]],\n                            [[5, 6],\n                             [7, 8]],\n                            [[9, 10],\n                             [11, 12]]],\n\n                           [[[13, 14],\n                             [15, 16]],\n                            [[17, 18],\n                             [19, 20]],\n                            [[21, 22],\n                             [23, 24]]]])\n\n# Print the shape of the original images\nprint(\"Original Images Shape:\", original_images.shape)\n\n# Print the original images\nprint(\"Original Images:\\n\", original_images)\n\n# Reshape the images into num_imgs x (3*2*2) matrices\nnum_imgs = original_images.shape[0]\nreshaped_images = original_images.reshape(num_imgs, -1)\n\n# Print the shape of the reshaped images\nprint(\"Reshaped Images Shape:\", reshaped_images.shape)\n\n# Print the reshaped images\nprint(\"Reshaped Images:\\n\", reshaped_images)\n\nOriginal Images Shape: (2, 3, 2, 2)\nOriginal Images:\n [[[[ 1  2]\n   [ 3  4]]\n\n  [[ 5  6]\n   [ 7  8]]\n\n  [[ 9 10]\n   [11 12]]]\n\n\n [[[13 14]\n   [15 16]]\n\n  [[17 18]\n   [19 20]]\n\n  [[21 22]\n   [23 24]]]]\nReshaped Images Shape: (2, 12)\nReshaped Images:\n [[ 1  2  3  4  5  6  7  8  9 10 11 12]\n [13 14 15 16 17 18 19 20 21 22 23 24]]"
  },
  {
    "objectID": "blog/2023-08-18_how_to_use_widgets_in_jupyter_notebooks/index.html",
    "href": "blog/2023-08-18_how_to_use_widgets_in_jupyter_notebooks/index.html",
    "title": "How to use widgets in jupyter notebooks",
    "section": "",
    "text": "How to use widgets in jupyter notebooks\n                \n            \n                        \n                \n                    Description of this Notebook\n                \n            \n                        \n            \n                                            \n\n                    \n                                            \n                            \n                                All\n                            \n                         \n                                            \n                            \n                                TAGS\n                            \n                         \n                                            \n                            \n                                Python\n                            \n                         \n                    \n                    \n                                    \n                            \n        \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 18, 2023\n        \n      \n      \n        \n      \n      \n\n        \n                 Code\n            \n    \n\n\n\n\n\n\n    \n        How to use widgets in jupyter notebooks\n        \n        \n                    \n                \n                    Description of this Notebook\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 18, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                    \n                 Code"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html",
    "title": "Self-supervised Learning I",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 14, 2023"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#selt-supervised-learning-for-computer-vision",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#selt-supervised-learning-for-computer-vision",
    "title": "Self-supervised Learning I",
    "section": "1 Selt-supervised learning for computer vision",
    "text": "1 Selt-supervised learning for computer vision\n\n\n\n\nSlide 1"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#organisation",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#organisation",
    "title": "Self-supervised Learning I",
    "section": "2 Organisation",
    "text": "2 Organisation\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-came-up-in-multiple-previous-lectures.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-came-up-in-multiple-previous-lectures.",
    "title": "Self-supervised Learning I",
    "section": "3 Self-supervised learning came up in multiple previous lectures.",
    "text": "3 Self-supervised learning came up in multiple previous lectures.\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#today",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#today",
    "title": "Self-supervised Learning I",
    "section": "4 Today:",
    "text": "4 Today:\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title",
    "title": "Self-supervised Learning I",
    "section": "5 Title",
    "text": "5 Title\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#the-field-of-al-has-made-rapid-progress-the-crucial-fuel-is-data",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#the-field-of-al-has-made-rapid-progress-the-crucial-fuel-is-data",
    "title": "Self-supervised Learning I",
    "section": "6 The field of Al has made rapid progress, the crucial fuel is data",
    "text": "6 The field of Al has made rapid progress, the crucial fuel is data\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#manual-annotations-for-the-data-are-limiting.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#manual-annotations-for-the-data-are-limiting.",
    "title": "Self-supervised Learning I",
    "section": "7 Manual annotations for the data are limiting.",
    "text": "7 Manual annotations for the data are limiting.\n\n\n\n\nSlide 7\n\n\n\n\n\nWeak supervised learnings are for i.e hastags in instagram, this can be noisy because a person can show a pic of a dog with #cute which is not very representative as a label\nWeak supervised learning is a type of machine learning that falls between supervised and unsupervised learning. In weakly supervised learning, the training data is labeled, but the labels are noisy, incomplete, or imprecise. This approach is often used when it’s challenging or expensive to obtain a fully labeled dataset"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#solving-the-problem-of-expensive-annotations-self-supervision.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#solving-the-problem-of-expensive-annotations-self-supervision.",
    "title": "Self-supervised Learning I",
    "section": "8 Solving the problem of expensive annotations: self-supervision.",
    "text": "8 Solving the problem of expensive annotations: self-supervision.\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#general-procedure-of-self-supervised-learning.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#general-procedure-of-self-supervised-learning.",
    "title": "Self-supervised Learning I",
    "section": "9 General procedure of self-supervised learning.",
    "text": "9 General procedure of self-supervised learning.\n\n\n\n\nSlide 9\n\n\n\n\nHere your transformation could be augmentations for instance.\nThe proxy task provides you with some gradients. That trains the DNN. Proxy tasks could be geometry based, clustering and so on"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#general-procedure-of-self-supervised-learning.-1",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#general-procedure-of-self-supervised-learning.-1",
    "title": "Self-supervised Learning I",
    "section": "10 General procedure of self-supervised learning.",
    "text": "10 General procedure of self-supervised learning.\n\n\n\n\nSlide 10\n\n\n\n\nIn Representation Learning you get image in and then vector out\nSueful Slef-supervised learning: You pose some task which was previously done in a supervised manner as a self-supervised task. This could be object detection & segmentation"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-1",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-1",
    "title": "Self-supervised Learning I",
    "section": "11 Title",
    "text": "11 Title\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-1-scalability",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-1-scalability",
    "title": "Self-supervised Learning I",
    "section": "12 Reason 1: Scalability",
    "text": "12 Reason 1: Scalability\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-1-scalability-1",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-1-scalability-1",
    "title": "Self-supervised Learning I",
    "section": "13 Reason 1: Scalability",
    "text": "13 Reason 1: Scalability\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-2-constantly-changing-domains",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-2-constantly-changing-domains",
    "title": "Self-supervised Learning I",
    "section": "14 Reason 2: Constantly changing domains",
    "text": "14 Reason 2: Constantly changing domains\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-2-accessibility-generalisability",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-2-accessibility-generalisability",
    "title": "Self-supervised Learning I",
    "section": "15 Reason 2: Accessibility & generalisability",
    "text": "15 Reason 2: Accessibility & generalisability\n\n\n\n\nSlide 15\n\n\n\n\nWhy do we want to do self-supervised learning?\nOnce you have a pre-trained model you can example use it to classify samples\nSo you can do pre-training on a lot of data and then aftwerwards you can fine tuning it on your specific i.e hospital data.\nAlso pre-trained representations have been used in archeology for figuring out whether a particular sample belongs to a particular period"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-3-ambiguity-of-labels",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-3-ambiguity-of-labels",
    "title": "Self-supervised Learning I",
    "section": "16 Reason 3: Ambiguity of labels",
    "text": "16 Reason 3: Ambiguity of labels\n\n\n\n\nSlide 16\n\n\n\n\nIn weakly supervised learning you dont used lables that humans specifically provided but instead labels that you found i.e hastags that people place. Also for instance for the CLIP model where you have images and captions. This were just drawn from the internet so some of this captions you may see i.e a laptop and the labels may not be laptop but read product #15, or another example there may be a pic of a dog and it may say ‘my fav partner to go on a walk’. So that is ambiguous and confusing for the model. So instead serlf-supervise model implies that we only use the raw data, so we dont use any of the annotations.\nSo a reason to do self-supervised learning is that because these lalbels from the internet are already not accurate then you dont want to use them. Instead you can do slef-supervised learning where you train a DNN with unlabel data and then use it into your task at hand"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-4-investigating-the-fundamentals-of-visual-understanding",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#reason-4-investigating-the-fundamentals-of-visual-understanding",
    "title": "Self-supervised Learning I",
    "section": "17 Reason 4: Investigating the fundamentals of visual understanding",
    "text": "17 Reason 4: Investigating the fundamentals of visual understanding\n\n\n\n\nSlide 17\n\n\n\n\nCan we understand really what happens without labels? so the fundamentals of computer vision."
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-turn-to-your-neighbour-and-briefly-explain-the-core",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-turn-to-your-neighbour-and-briefly-explain-the-core",
    "title": "Self-supervised Learning I",
    "section": "18 Quiz: turn to your neighbour and briefly explain the core",
    "text": "18 Quiz: turn to your neighbour and briefly explain the core\nidea behind self-supervised learning.\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#icwiakmitovat-raf-calf.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#icwiakmitovat-raf-calf.",
    "title": "Self-supervised Learning I",
    "section": "19 ICWIAKMITOVAT RAF Calf.",
    "text": "19 ICWIAKMITOVAT RAF Calf.\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#here-we-will-only-cover-the-most-important-works.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#here-we-will-only-cover-the-most-important-works.",
    "title": "Self-supervised Learning I",
    "section": "20 Here, we will only cover the most important works.",
    "text": "20 Here, we will only cover the most important works.\nFurther details and recent developments can be found here:\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#how-does-one-learn-without-labels",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#how-does-one-learn-without-labels",
    "title": "Self-supervised Learning I",
    "section": "21 How does one learn without labels?",
    "text": "21 How does one learn without labels?\n\n\n\n\nSlide 21\n\n\n\n\n\nWe say that we need to generate gradients. So some type of signals that we can leverage include:\nReconstructions: we can remove some aprt of the image and ask the model to reconstruct what has been hidden\nGeometry"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#early-methods-context-prediction",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#early-methods-context-prediction",
    "title": "Self-supervised Learning I",
    "section": "22 Early methods: Context prediction",
    "text": "22 Early methods: Context prediction\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#note-similar-to-how-bert-has-been-trained",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#note-similar-to-how-bert-has-been-trained",
    "title": "Self-supervised Learning I",
    "section": "23 Note: similar to how BERT has been trained",
    "text": "23 Note: similar to how BERT has been trained\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#early-methods",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#early-methods",
    "title": "Self-supervised Learning I",
    "section": "24 Early methods",
    "text": "24 Early methods\n\n\n\n\nSlide 24\n\n\n\n\n\nContext Encoders, you maks now a part of a image, so you put a white mask on top of the image and then you trained a model to ouput a dense feature map that will put the pixeles at that location. You only apply the loss at this locations but because you use a CNN you train all the weights."
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#geometry-rotnet-learn-features-by-predicting-which-way-is-up.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#geometry-rotnet-learn-features-by-predicting-which-way-is-up.",
    "title": "Self-supervised Learning I",
    "section": "25 Geometry: RotNet: learn features by predicting “which way is up”.",
    "text": "25 Geometry: RotNet: learn features by predicting “which way is up”.\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#image-uniqueness-exemplar-cnn-precursor-to-contrastive-learning",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#image-uniqueness-exemplar-cnn-precursor-to-contrastive-learning",
    "title": "Self-supervised Learning I",
    "section": "26 Image-uniqueness: Exemplar CNN, precursor to contrastive learning",
    "text": "26 Image-uniqueness: Exemplar CNN, precursor to contrastive learning\n\n\n\n\nSlide 26\n\n\n\n\nEXampler CNN came before Contrastive Learning, this helps to work on CLip. Here you augment the image multiple times for each iamge, and now you model needs to ouput which image identity it was.\nThe idea of image uniqueness is that if you have near dusplicate copies of the same image then it makes for a string signal. For isntance if you have a dog jumping vs a dog sitting, that is very difficult to differentiate so in that sense the model needs to learn quite some good feature in order to be differentiating these two classes.\nThis also enforces augmentation-invariance because all the different views, all the different augmentations of the image they should be the same"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#modern-noise-contrastive-self-supervised-learning",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#modern-noise-contrastive-self-supervised-learning",
    "title": "Self-supervised Learning I",
    "section": "27 Modern Noise-contrastive self-supervised learning",
    "text": "27 Modern Noise-contrastive self-supervised learning\n\n\n\n\nSlide 27\n\n\n\n\nAfter that people develop contrastive models.\nThe basic idea for simCLR is: you take two views for two images. You have two augmentations of the dogs a and two for the chair.\nHere the softmax is calcualting the similarity of z_i and z_j. These are the last representations. The sim() function is the dot products which tells you how similar they are. You apply the softmax across all these dot products"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#clip-from-lect-9-and-assignment-2-simply-applies-simclr-across-modalities",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#clip-from-lect-9-and-assignment-2-simply-applies-simclr-across-modalities",
    "title": "Self-supervised Learning I",
    "section": "28 CLIP from Lect 9 and assignment 2 simply applies SimCLR across modalities",
    "text": "28 CLIP from Lect 9 and assignment 2 simply applies SimCLR across modalities\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#modern-noise-contrastive-self-supervised-learning-1",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#modern-noise-contrastive-self-supervised-learning-1",
    "title": "Self-supervised Learning I",
    "section": "29 Modern Noise-contrastive self-supervised learning",
    "text": "29 Modern Noise-contrastive self-supervised learning\n\n\n\n\nSlide 29"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#masked-image-modelling-recent-development",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#masked-image-modelling-recent-development",
    "title": "Self-supervised Learning I",
    "section": "30 Masked Image Modelling (recent development)",
    "text": "30 Masked Image Modelling (recent development)\n\n\n\n\nSlide 30\n\n\n\n\nTransformers work on sequences but CNN this approach would not work because the ouputs are always spatial that means you can simply leave some. Then you get a representation and your task is to predict all these missing patches given the patches that you have seen"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#clustering",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#clustering",
    "title": "Self-supervised Learning I",
    "section": "31 Clustering",
    "text": "31 Clustering\n\n\n\n\nSlide 31"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-2",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-2",
    "title": "Self-supervised Learning I",
    "section": "19 Title",
    "text": "19 Title\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#datasets-for-images-pretraining-and-downstream",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#datasets-for-images-pretraining-and-downstream",
    "title": "Self-supervised Learning I",
    "section": "33 Datasets for images: Pretraining and downstream",
    "text": "33 Datasets for images: Pretraining and downstream\n\n\n\n\nSlide 33"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#recent-surge-in-research-on-problematic-images-in-imagenet",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#recent-surge-in-research-on-problematic-images-in-imagenet",
    "title": "Self-supervised Learning I",
    "section": "34 Recent surge in research on problematic images in ImageNet",
    "text": "34 Recent surge in research on problematic images in ImageNet\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-3",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-3",
    "title": "Self-supervised Learning I",
    "section": "32 Title",
    "text": "32 Title\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#the-dataset-diverse-containing-nature-and-buildings.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#the-dataset-diverse-containing-nature-and-buildings.",
    "title": "Self-supervised Learning I",
    "section": "36 The dataset: diverse, containing nature and buildings.",
    "text": "36 The dataset: diverse, containing nature and buildings.\n\n\n\n\nSlide 36"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#datasets-for-images-pretraining-and",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#datasets-for-images-pretraining-and",
    "title": "Self-supervised Learning I",
    "section": "37 Datasets for images: Pretraining and",
    "text": "37 Datasets for images: Pretraining and\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#downstream-semi-supervised-tasks-self-supervised-learning-helps",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#downstream-semi-supervised-tasks-self-supervised-learning-helps",
    "title": "Self-supervised Learning I",
    "section": "38 Downstream semi-supervised tasks: Self-supervised Learning helps",
    "text": "38 Downstream semi-supervised tasks: Self-supervised Learning helps\n\n\n\n\nSlide 38\n\n\n\n\n\nSupervised: red\nSelf-supervised: Blue"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-4",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-4",
    "title": "Self-supervised Learning I",
    "section": "35 Title",
    "text": "35 Title\n\n\n\n\nSlide 35"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-5",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-5",
    "title": "Self-supervised Learning I",
    "section": "39 Title",
    "text": "39 Title\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#goal-discover-visual-concepts-without-annotations.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#goal-discover-visual-concepts-without-annotations.",
    "title": "Self-supervised Learning I",
    "section": "41 Goal: Discover visual concepts without annotations.",
    "text": "41 Goal: Discover visual concepts without annotations.\n\n\n\n\nSlide 41"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#how-can-we-solve-this-chicken-and-egg-problem",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#how-can-we-solve-this-chicken-and-egg-problem",
    "title": "Self-supervised Learning I",
    "section": "42 How can we solve this chicken and egg problem?",
    "text": "42 How can we solve this chicken and egg problem?\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#the-key-to-image-understanding-is-separating-meaning-from-appearance.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#the-key-to-image-understanding-is-separating-meaning-from-appearance.",
    "title": "Self-supervised Learning I",
    "section": "43 The key to image understanding is separating meaning from appearance.",
    "text": "43 The key to image understanding is separating meaning from appearance.\n\n\n\n\nSlide 43\n\n\n\n\nEven though the pixel values are different"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-what-other-ways-of-incorporating-prior-knowledge",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-what-other-ways-of-incorporating-prior-knowledge",
    "title": "Self-supervised Learning I",
    "section": "44 , Quiz: What other ways of incorporating prior knowledge |",
    "text": "44 , Quiz: What other ways of incorporating prior knowledge |\nhave we already learned about? (MC)\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#our-work-applies-the-idea-of-augmentation-invariance-to-assign-concepts.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#our-work-applies-the-idea-of-augmentation-invariance-to-assign-concepts.",
    "title": "Self-supervised Learning I",
    "section": "45 Our work applies the idea of augmentation invariance to assign concepts.",
    "text": "45 Our work applies the idea of augmentation invariance to assign concepts.\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#our-work-applies-the-idea-of-transformation-invariance-to-assign-concepts.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#our-work-applies-the-idea-of-transformation-invariance-to-assign-concepts.",
    "title": "Self-supervised Learning I",
    "section": "46 Our work applies the idea of transformation invariance to assign concepts.",
    "text": "46 Our work applies the idea of transformation invariance to assign concepts.\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#how-can-we-optimize-the-labels-and-make-assignments-consistents",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#how-can-we-optimize-the-labels-and-make-assignments-consistents",
    "title": "Self-supervised Learning I",
    "section": "47 How can we optimize the labels and make assignments consistents",
    "text": "47 How can we optimize the labels and make assignments consistents\n\n\n\n\nSlide 47\n\n\n\n\nhere we want to make y differentiable because we want to learn those labels."
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#sk-optimisation-not-needed-for-exam",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#sk-optimisation-not-needed-for-exam",
    "title": "Self-supervised Learning I",
    "section": "48 SK optimisation (not needed for exam)",
    "text": "48 SK optimisation (not needed for exam)\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#sk-optimisation-of-assignments-q-not-needed-for-exam",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#sk-optimisation-of-assignments-q-not-needed-for-exam",
    "title": "Self-supervised Learning I",
    "section": "49 SK optimisation of assignments Q (not needed for exam)",
    "text": "49 SK optimisation of assignments Q (not needed for exam)\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#algorithm",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#algorithm",
    "title": "Self-supervised Learning I",
    "section": "50 Algorithm",
    "text": "50 Algorithm\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#our-method-applied-on-1.2-million-images",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#our-method-applied-on-1.2-million-images",
    "title": "Self-supervised Learning I",
    "section": "51 Our method applied on 1.2 million images:",
    "text": "51 Our method applied on 1.2 million images:\nExamples\n\n\n\n\nSlide 51"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#automatically-discovered-concepts-match-manual-annotation.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#automatically-discovered-concepts-match-manual-annotation.",
    "title": "Self-supervised Learning I",
    "section": "52 Automatically discovered concepts match manual annotation.",
    "text": "52 Automatically discovered concepts match manual annotation.\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#alexnet-imagenet-linear-probes-remember-lecture-5",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#alexnet-imagenet-linear-probes-remember-lecture-5",
    "title": "Self-supervised Learning I",
    "section": "53 AlexNet, ImageNet linear probes (remember Lecture 5)",
    "text": "53 AlexNet, ImageNet linear probes (remember Lecture 5)\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-labelling-from-three-core-ideas",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-labelling-from-three-core-ideas",
    "title": "Self-supervised Learning I",
    "section": "54 Self-supervised labelling from three core ideas",
    "text": "54 Self-supervised labelling from three core ideas\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#more-recently",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#more-recently",
    "title": "Self-supervised Learning I",
    "section": "55 More recently…",
    "text": "55 More recently…\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#dino-has-remarkable-properties",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#dino-has-remarkable-properties",
    "title": "Self-supervised Learning I",
    "section": "56 DINO has remarkable properties",
    "text": "56 DINO has remarkable properties\n\n\n\n\nSlide 56"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#am-rebelling-against-the-hierarchical-system-that-7m-fraser-stoddart",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#am-rebelling-against-the-hierarchical-system-that-7m-fraser-stoddart",
    "title": "Self-supervised Learning I",
    "section": "57 | am rebelling against the hierarchical system that {7m Fraser Stoddart:",
    "text": "57 | am rebelling against the hierarchical system that {7m Fraser Stoddart:\nHow research gets done: part 9 visited me during the early days of my career. Bef You’ve got to\n\n. - break the rules”\n\nIN Previous parts: a ee” [fundamental understanding/read papers. how-to-read-papers. implement & tinker with code. realise and seek funny\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#however-the-world-is-not-object-centric.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#however-the-world-is-not-object-centric.",
    "title": "Self-supervised Learning I",
    "section": "58 However: The world is not object-centric.",
    "text": "58 However: The world is not object-centric.\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-of-object-parts-for-semantic-segmentation",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-of-object-parts-for-semantic-segmentation",
    "title": "Self-supervised Learning I",
    "section": "59 Self-Supervised Learning of Object Parts for Semantic Segmentation",
    "text": "59 Self-Supervised Learning of Object Parts for Semantic Segmentation\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-larning-of-odject-parts-for-semantic-segmentation",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-larning-of-odject-parts-for-semantic-segmentation",
    "title": "Self-supervised Learning I",
    "section": "60 Self-Supervised L@arning of Odject Parts for semantic Segmentation",
    "text": "60 Self-Supervised L@arning of Odject Parts for semantic Segmentation\ngs, i z=. ==&gt;, aa, Be Ea, Ae\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-has-to-move-from-image-level-to-spatially-dense-learning",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-has-to-move-from-image-level-to-spatially-dense-learning",
    "title": "Self-supervised Learning I",
    "section": "61 Self-Supervised Learning has to move from image-level to spatially-dense learning",
    "text": "61 Self-Supervised Learning has to move from image-level to spatially-dense learning\n\n\n\n\nSlide 61"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#we-propose-a-dense-clustering-pretext-task-to-learn-object-parts",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#we-propose-a-dense-clustering-pretext-task-to-learn-object-parts",
    "title": "Self-supervised Learning I",
    "section": "62 We propose a dense clustering pretext task to learn object parts",
    "text": "62 We propose a dense clustering pretext task to learn object parts\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-why-did-we-use-rol-align-and-not-rol-pools",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-why-did-we-use-rol-align-and-not-rol-pools",
    "title": "Self-supervised Learning I",
    "section": "63 Quiz: Why did we use Rol-Align and not Rol-Pools |",
    "text": "63 Quiz: Why did we use Rol-Align and not Rol-Pools |\n\n\n\n\nSlide 63"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-6",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-6",
    "title": "Self-supervised Learning I",
    "section": "40 Title",
    "text": "40 Title\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#additional-innovation-2-overclustering-with-community-detection-cd",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#additional-innovation-2-overclustering-with-community-detection-cd",
    "title": "Self-supervised Learning I",
    "section": "65 Additional Innovation 2: Overclustering with Community Detection (CD)",
    "text": "65 Additional Innovation 2: Overclustering with Community Detection (CD)\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#overclustering-with-community-detection-ran.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#overclustering-with-community-detection-ran.",
    "title": "Self-supervised Learning I",
    "section": "66 Overclustering with Community Detection ran.",
    "text": "66 Overclustering with Community Detection ran.\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-7",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-7",
    "title": "Self-supervised Learning I",
    "section": "57 Title",
    "text": "57 Title\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#leopart-improves-fully-unsupervised-sota-by-6",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#leopart-improves-fully-unsupervised-sota-by-6",
    "title": "Self-supervised Learning I",
    "section": "68 Leopart improves fully unsupervised SOTA by >6%",
    "text": "68 Leopart improves fully unsupervised SOTA by &gt;6%\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#leopart-achieves-transfer-sota-on-three-datasets-simultaneously",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#leopart-achieves-transfer-sota-on-three-datasets-simultaneously",
    "title": "Self-supervised Learning I",
    "section": "69 Leopart achieves transfer SOTA on three datasets simultaneously",
    "text": "69 Leopart achieves transfer SOTA on three datasets simultaneously\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#augmentations-were-key-for-both-sela-and-leopart.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#augmentations-were-key-for-both-sela-and-leopart.",
    "title": "Self-supervised Learning I",
    "section": "70 Augmentations were key for both SeLa and Leopart.",
    "text": "70 Augmentations were key for both SeLa and Leopart.\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#how-can-we-isolate-the-effect-of-augmentations",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#how-can-we-isolate-the-effect-of-augmentations",
    "title": "Self-supervised Learning I",
    "section": "71 How can we Isolate the effect of augmentations?",
    "text": "71 How can we Isolate the effect of augmentations?\nBy learning from a single image\nPy4t4\n\n\n\n\nSlide 71"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#how-do-we-go-about-thiss",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#how-do-we-go-about-thiss",
    "title": "Self-supervised Learning I",
    "section": "72 How do we go about thiss",
    "text": "72 How do we go about thiss\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#what-do-we-learn",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#what-do-we-learn",
    "title": "Self-supervised Learning I",
    "section": "73 What do we learn?",
    "text": "73 What do we learn?\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#tested-images",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#tested-images",
    "title": "Self-supervised Learning I",
    "section": "74 Tested images",
    "text": "74 Tested images\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-from-one-image",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-from-one-image",
    "title": "Self-supervised Learning I",
    "section": "75 Self-supervised learning from one image:",
    "text": "75 Self-supervised learning from one image:\nFirst convolutional layer\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-from-one-image-1",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-from-one-image-1",
    "title": "Self-supervised Learning I",
    "section": "76 Self-supervised learning from one image:",
    "text": "76 Self-supervised learning from one image:\nQuality (ImageNet linear probes)\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-from-one-image-2",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-from-one-image-2",
    "title": "Self-supervised Learning I",
    "section": "77 Self-supervised learning from one image:",
    "text": "77 Self-supervised learning from one image:\nQuality (ImageNet linear probes)\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#style-transfer-with-a-1-image-trained-cnn",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#style-transfer-with-a-1-image-trained-cnn",
    "title": "Self-supervised Learning I",
    "section": "78 Style transfer with a 1-image trained CNN",
    "text": "78 Style transfer with a 1-image trained CNN\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#update-feb-2021-using-a-resnet-50-and-moco-loss",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#update-feb-2021-using-a-resnet-50-and-moco-loss",
    "title": "Self-supervised Learning I",
    "section": "79 | Update Feb 2021 |] Using a ResNet-50 and MoCo loss,",
    "text": "79 | Update Feb 2021 |] Using a ResNet-50 and MoCo loss,\nwe get even closer for fine-tuning tasks.\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#update-2",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#update-2",
    "title": "Self-supervised Learning I",
    "section": "80 Update 2:",
    "text": "80 Update 2:\n\n\n\n\nSlide 80"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#conclusion",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#conclusion",
    "title": "Self-supervised Learning I",
    "section": "81 Conclusion",
    "text": "81 Conclusion\n\n\n\n\nSlide 81"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#unsup.-pre-train",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#unsup.-pre-train",
    "title": "Self-supervised Learning I",
    "section": "82 unsup. pre-train",
    "text": "82 unsup. pre-train\n\n\n\n\nSlide 82"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#yi-5-.-fam-remove-all-the-human-.",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#yi-5-.-fam-remove-all-the-human-.",
    "title": "Self-supervised Learning I",
    "section": "83 ’ : yi 5 — . , ’ fam : Remove all the human .",
    "text": "83 ’ : yi 5 — . , ’ fam : Remove all the human .\n) z ; , , rat et Part: Why’? map Bee\n\n\n\n\nSlide 83"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#thats-it-friday-you-will-have-a-lecture-by-prof.-cees-snoek",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#thats-it-friday-you-will-have-a-lecture-by-prof.-cees-snoek",
    "title": "Self-supervised Learning I",
    "section": "84 That’s it! (Friday you will have a lecture by Prof. Cees Snoek)",
    "text": "84 That’s it! (Friday you will have a lecture by Prof. Cees Snoek)\n\n\n\n\nSlide 84"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 27, 2023"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "1 Title",
    "text": "1 Title\n\n\n\n\nSlide 1"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#organisation",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#organisation",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "2 Organisation",
    "text": "2 Organisation\n\n\n\n\nSlide 2"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#lecture-overview",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#lecture-overview",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "3 Lecture overview",
    "text": "3 Lecture overview\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graphs-theyre-everywhere",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graphs-theyre-everywhere",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "4 Graphs! They’re everywhere",
    "text": "4 Graphs! They’re everywhere\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-are-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-are-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "5 What are graphs?",
    "text": "5 What are graphs?\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-are-graphs-1",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-are-graphs-1",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "6 What are graphs?",
    "text": "6 What are graphs?\n\n\n\n\nSlide 6"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graphs-as-geometry.",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graphs-as-geometry.",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "7 Graphs as geometry.",
    "text": "7 Graphs as geometry.\n\n\n\n\nSlide 7"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#classifying-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#classifying-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "8 1) Classifying graphs",
    "text": "8 1) Classifying graphs\n\n\n\n\nSlide 8"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#classifying-nodes",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#classifying-nodes",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "9 2) Classifying nodes",
    "text": "9 2) Classifying nodes\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-generation",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-generation",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "10 3) Graph generation",
    "text": "10 3) Graph generation\n\n\n\n\nSlide 10"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#linkedge-prediction",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#linkedge-prediction",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "11 4) Link/Edge prediction",
    "text": "11 4) Link/Edge prediction\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#three-tasks-visualized-here-with-nodes-that-carry-features",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#three-tasks-visualized-here-with-nodes-that-carry-features",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "12 Three tasks visualized: here with nodes that carry features",
    "text": "12 Three tasks visualized: here with nodes that carry features\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graphs-can-be-static-varying-or-even-evolving-with-time",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graphs-can-be-static-varying-or-even-evolving-with-time",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "13 Graphs can be static, varying, or even evolving with time",
    "text": "13 Graphs can be static, varying, or even evolving with time\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#regular-structures-vs-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#regular-structures-vs-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "14 Regular structures vs graphs",
    "text": "14 Regular structures vs graphs\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-1",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-1",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "15 Title",
    "text": "15 Title\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#directed-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#directed-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "16 Directed graphs",
    "text": "16 Directed graphs\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#undirected-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#undirected-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "17 Undirected graphs",
    "text": "17 Undirected graphs\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-neighborhood",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-neighborhood",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "18 Graph neighborhood",
    "text": "18 Graph neighborhood\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#attributes",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#attributes",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "19 Attributes",
    "text": "19 Attributes\n\n\n\n\nSlide 19\n\n\n\n\nThe attention score is measured by these softmax\nThe dot product here it ends up being 2x3 again"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#adjacency-matrix",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#adjacency-matrix",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "20 Adjacency matrix",
    "text": "20 Adjacency matrix\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#adjacency-matrix-for-undirected-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#adjacency-matrix-for-undirected-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "21 Adjacency matrix for undirected graphs",
    "text": "21 Adjacency matrix for undirected graphs\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#weighted-adjacency-matrix",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#weighted-adjacency-matrix",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "22 Weighted adjacency matrix",
    "text": "22 Weighted adjacency matrix\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-representation-for-us",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-representation-for-us",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "23 Graph representation for us",
    "text": "23 Graph representation for us\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#quiz",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#quiz",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "24 ; Quiz: ;",
    "text": "24 ; Quiz: ;\n\n\n\n\nSlide 24"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-laplacian",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-laplacian",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "25 Graph Laplacian",
    "text": "25 Graph Laplacian\n\n\n\n\nSlide 25"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-laplacian-meaning",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-laplacian-meaning",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "26 Graph Laplacian: meaning",
    "text": "26 Graph Laplacian: meaning\n\n\n\n\nSlide 26"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#applications-of-the-graph-laplacian",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#applications-of-the-graph-laplacian",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "27 Applications of the Graph Laplacian",
    "text": "27 Applications of the Graph Laplacian\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#applied-laplacian-written-out",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#applied-laplacian-written-out",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "28 Applied Laplacian written out:",
    "text": "28 Applied Laplacian written out:\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-2",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-2",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "29 Title",
    "text": "29 Title\n\n\n\n\nSlide 29"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#the-shift-operator-a-special-circulant-matrix",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#the-shift-operator-a-special-circulant-matrix",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "30 The shift operator, a special circulant matrix",
    "text": "30 The shift operator, a special circulant matrix\n\n\n\n\nSlide 30"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#now-we-want-to-know",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#now-we-want-to-know",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "31 Now we want to know:",
    "text": "31 Now we want to know:\n\n\n\n\nSlide 31"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#as-it-turns-out-circulant-matrices-commute",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#as-it-turns-out-circulant-matrices-commute",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "32 As it turns out: circulant matrices commute",
    "text": "32 As it turns out: circulant matrices commute\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-this-means-translation-equivariance-circulant-matricesconvolutions",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-this-means-translation-equivariance-circulant-matricesconvolutions",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "33 What this means: Translation equivariance > circulant matrices/convolutions",
    "text": "33 What this means: Translation equivariance &gt; circulant matrices/convolutions\n\n\n\n\nSlide 33"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#where-we-are",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#where-we-are",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "34 Where we are",
    "text": "34 Where we are\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#maths-all-circulant-matrices-have-the-same-eigenvectors",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#maths-all-circulant-matrices-have-the-same-eigenvectors",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "35 Maths: All circulant matrices have the same eigenvectors!",
    "text": "35 Maths: All circulant matrices have the same eigenvectors!\n\n\n\n\nSlide 35"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#all-circulant-matrices-have-the-same-eigenvectors",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#all-circulant-matrices-have-the-same-eigenvectors",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "36 All circulant matrices have the same eigenvectors!",
    "text": "36 All circulant matrices have the same eigenvectors!\n\n\n\n\nSlide 36"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#circulant-eigenvectors-shift-eigenvectors",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#circulant-eigenvectors-shift-eigenvectors",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "37 Circulant eigenvectors © Shift eigenvectors",
    "text": "37 Circulant eigenvectors © Shift eigenvectors\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#but-first-what-are-the-eigenvectors-of-the-shift-operator",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#but-first-what-are-the-eigenvectors-of-the-shift-operator",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "38 But first: What are the eigenvectors of the shift operator",
    "text": "38 But first: What are the eigenvectors of the shift operator\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#computing-a-convolution-in-the-frequency-domain",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#computing-a-convolution-in-the-frequency-domain",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "39 Computing a convolution in the frequency domain",
    "text": "39 Computing a convolution in the frequency domain\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#convolution-theorem",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#convolution-theorem",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "40 Convolution Theorem",
    "text": "40 Convolution Theorem\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#convolution-theorem-1",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#convolution-theorem-1",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "41 Convolution theorem",
    "text": "41 Convolution theorem\n\n\n\n\nSlide 41"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#frequency-representation",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#frequency-representation",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "42 Frequency representation:",
    "text": "42 Frequency representation:\n\n\n\n\nSlide 42"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#quiz-remember-the-fourier-transform-for-images",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#quiz-remember-the-fourier-transform-for-images",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "43 ; Quiz: Remember the Fourier transform for images: |",
    "text": "43 ; Quiz: Remember the Fourier transform for images: |\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#convolution-theorem-x-w---aw-----x",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#convolution-theorem-x-w---aw-----x",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "44 Convolution theorem: x * w = ®- (A(w) - (®* - x))",
    "text": "44 Convolution theorem: x * w = ®- (A(w) - (®* - x))\n\n\n\n\nSlide 44"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#implications",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#implications",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "45 Implications",
    "text": "45 Implications\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#if-translation-equivariance-leads-to-cnns-what-else-is-there",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#if-translation-equivariance-leads-to-cnns-what-else-is-there",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "46 If translation equivariance leads to CNNs, what else is there?",
    "text": "46 If translation equivariance leads to CNNs, what else is there?\n\n\n\n\nSlide 46"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#a-large-field-group-equivariant-deep-learning",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#a-large-field-group-equivariant-deep-learning",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "47 A large field: Group Equivariant Deep Learning",
    "text": "47 A large field: Group Equivariant Deep Learning\n\n\n\n\nSlide 47"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#circulant-matrices",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#circulant-matrices",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "48 Circulant matrices",
    "text": "48 Circulant matrices\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#i-was-lucky",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#i-was-lucky",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "49 “I was lucky…",
    "text": "49 “I was lucky…\nHow research gets done part 6\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-3",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-3",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "50 Title",
    "text": "50 Title\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#from-convolutions-to-spectral-graph-convolutions",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#from-convolutions-to-spectral-graph-convolutions",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "51 From convolutions to spectral graph convolutions",
    "text": "51 From convolutions to spectral graph convolutions\n\n\n\n\nSlide 51"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#approach-use-eigenvectors-of-graph-laplacian-to-replace-fourier",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#approach-use-eigenvectors-of-graph-laplacian-to-replace-fourier",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "52 Approach: Use Eigenvectors of Graph Laplacian to replace Fourier",
    "text": "52 Approach: Use Eigenvectors of Graph Laplacian to replace Fourier\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#actually",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#actually",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "53 Actually:",
    "text": "53 Actually:\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#further-details",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#further-details",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "54 Further details",
    "text": "54 Further details\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#in-analogy-to-convolutions-in-frequency-domain",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#in-analogy-to-convolutions-in-frequency-domain",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "55 In analogy to convolutions in frequency domain:",
    "text": "55 In analogy to convolutions in frequency domain:\nWe now define spectral graph convolutions\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#where-we-are-part-2",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#where-we-are-part-2",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "56 Where we are, part 2",
    "text": "56 Where we are, part 2\n\n\n\n\nSlide 56"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#why-the-graph-laplacian",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#why-the-graph-laplacian",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "57 Why the graph Laplacian*?",
    "text": "57 Why the graph Laplacian*?\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#spectral-graph-convolution",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#spectral-graph-convolution",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "58 Spectral graph convolution",
    "text": "58 Spectral graph convolution\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#some-drawbacks-of-this-variant",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#some-drawbacks-of-this-variant",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "59 Some drawbacks of this variant",
    "text": "59 Some drawbacks of this variant\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#easy-to-increase-the-field-of-view-with-powers-of-the-laplacian",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#easy-to-increase-the-field-of-view-with-powers-of-the-laplacian",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "60 Easy to increase the field of view with powers of the Laplacian",
    "text": "60 Easy to increase the field of view with powers of the Laplacian\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#putting-it-together-stacking-graph-convolutions",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#putting-it-together-stacking-graph-convolutions",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "61 Putting it together: stacking graph convolutions",
    "text": "61 Putting it together: stacking graph convolutions\n\n\n\n\nSlide 61"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#quiz-what-nronerties-does-this-nolvnomial-variant-have",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#quiz-what-nronerties-does-this-nolvnomial-variant-have",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "62 . Quiz: What nronerties does this nolvnomial variant have? |",
    "text": "62 . Quiz: What nronerties does this nolvnomial variant have? |\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#some-drawbacks-of-this-variant-now",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#some-drawbacks-of-this-variant-now",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "63 Some drawbacks of this variant now",
    "text": "63 Some drawbacks of this variant now\n\n\n\n\nSlide 63"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-4",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-4",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "64 Title",
    "text": "64 Title\n\n\n\n\nSlide 64"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#a-ff-ff-f-fg",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#a-ff-ff-f-fg",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "65 A FF fF F Fg",
    "text": "65 A FF fF F Fg\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-convolutions",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-convolutions",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "66 Graph convolutions",
    "text": "66 Graph convolutions\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-can-we-use-from-the-spectral-approach",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#what-can-we-use-from-the-spectral-approach",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "67 What can we use from the spectral approach?",
    "text": "67 What can we use from the spectral approach?\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-convolutional-networks-gcn",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-convolutional-networks-gcn",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "68 Graph Convolutional Networks (GCN)",
    "text": "68 Graph Convolutional Networks (GCN)\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-convolutional-networks-gcn-1",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#graph-convolutional-networks-gcn-1",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "69 Graph Convolutional Networks (GCN)",
    "text": "69 Graph Convolutional Networks (GCN)\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#putting-it-together",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#putting-it-together",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "70 Putting it together:",
    "text": "70 Putting it together:\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#other-kind-of-aggregation-graph-attention-networks-gat",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#other-kind-of-aggregation-graph-attention-networks-gat",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "71 Other kind of aggregation: Graph Attention Networks (GAT)",
    "text": "71 Other kind of aggregation: Graph Attention Networks (GAT)\n\n\n\n\nSlide 71"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#self-attention-for-graph-convolutions",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#self-attention-for-graph-convolutions",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "72 Self-attention for graph convolutions",
    "text": "72 Self-attention for graph convolutions\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#connection-to-transformers",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#connection-to-transformers",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "73 Connection to transformers",
    "text": "73 Connection to transformers\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#message-passing-neural-network-mpnn",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#message-passing-neural-network-mpnn",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "74 Message Passing Neural Network (MPNN)",
    "text": "74 Message Passing Neural Network (MPNN)\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#pytorch-geometric-baseclass",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#pytorch-geometric-baseclass",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "75 PyTorch Geometric baseclass",
    "text": "75 PyTorch Geometric baseclass\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#overview",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#overview",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "76 Overview",
    "text": "76 Overview\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#finally-a-note-about-coarsening-graphs",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#finally-a-note-about-coarsening-graphs",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "77 Finally, a note about coarsening graphs",
    "text": "77 Finally, a note about coarsening graphs\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#where-we-are-part-3",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#where-we-are-part-3",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "78 Where we are, part 3",
    "text": "78 Where we are, part 3\n\n\n\n\nSlide 78"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#the-last-few-lectures",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#the-last-few-lectures",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "79 The last few lectures",
    "text": "79 The last few lectures\n\n\n\n\nSlide 79"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-5",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-5",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "80 Title",
    "text": "80 Title\n\n\n\n\nSlide 80"
  },
  {
    "objectID": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-6",
    "href": "blog/2023-11-27_equivariances-&-graphical-neural-networks/index.html#title-6",
    "title": "Equivariances & Graphical Neural Networks",
    "section": "81 Title",
    "text": "81 Title\n\n\n\n\nSlide 81"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          December 7, 2023"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#lecture-overview",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#lecture-overview",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "1 Lecture overview",
    "text": "1 Lecture overview\n\n\n\n\nSlide 3\n\n\n\n\n\n\n\n\n\n\nWhat are implicit density models?\n\n\n\n\n\nThese models are “implicit” because they do not explicitly define the probability distribution, but rather define it indirectly. Instead of specifying a probability density function, implicit density models rely on generating samples from a distribution and use these samples to implicitly model the underlying probability distribution.\nOne popular approach to implicit density modeling is through the use of generative models"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#a-map-of-generative-models",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#a-map-of-generative-models",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "2 A map of generative models",
    "text": "2 A map of generative models\n\n\n\n\nSlide 4\n\n\n\n\nHere in the implicity density you can sample from the probability distribution, but it cannot give you an estimate of how likely a particular image is"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#last-time",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#last-time",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "3 Last time",
    "text": "3 Last time\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#today",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#today",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "4 Today",
    "text": "4 Today\n\n\n\n\nSlide 6\n\n\n\n\nHere we say Generated as refering to fake, because it was generated. The Discriminator would be in such a manner that it will learn how to tell between real and fake. And the Generator will also be a NN trained in a manner that tries to fool the Discriminator"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#explicit-density-vs-implicit-density",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#explicit-density-vs-implicit-density",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "5 Explicit density vs implicit density",
    "text": "5 Explicit density vs implicit density\n\n\n\n\nSlide 7\n\n\n\n\n\nPurple: if x was taken from the true distribution then P_theta is high\n\nExplicit and VAE kinda do both, it includes both things. Explicitly density is more difficult because we can do both escentially.\n\nGreen: samples taken from p_theta they should behave similar to the real sample\n\nHere we can get a sample but we cannot give get the distribution p_theta of x so the probability values"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#learning-an-implicit-density-function",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#learning-an-implicit-density-function",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "6 Learning an implicit density function",
    "text": "6 Learning an implicit density function\n\n\n\n\nSlide 8\n\n\n\n\nIf the generation is not possible, then lets train some gradients to train the generator."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#generations-of-high-quality-various-potential-applications",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#generations-of-high-quality-various-potential-applications",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "7 Generations of high quality, various potential applications",
    "text": "7 Generations of high quality, various potential applications\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "8 Title",
    "text": "8 Title\n\n\n\n\nSlide 10\n\n\n\n\nLEFT:\n\nSample on image from the dataset\nPut it through the differentiable function \\(D\\), Discriminator. Now the descriminator for real iamges will try to retunr a value near 1, if is from the real image.\n\nRIGHT: Generated image\n\nSample noise\nPut it into the generator \\(G\\) network and we will end up with some image.\n\nThen we do it all over again. Sample x from data, but now the Discriminator tries to detect if its wrong, a fake image. The Generator then tries to fool it"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#what-is-a-gan",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#what-is-a-gan",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "9 What is a GAN?",
    "text": "9 What is a GAN?\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gan-intuition-arms-race",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gan-intuition-arms-race",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "10 GAN: Intuition: arms race",
    "text": "10 GAN: Intuition: arms race\n\n\n\n\nSlide 12\n\n\n\n\nAt the start the discriminator will discriminate well. The analogy is of detecting money. At the begining, we want to detect whether the money is true. Both do not have clue. At the beginning the police is the baby and the money looks rather fake. You start trianing these and they become better.\nWhen they reach Nash equilibrium it means they cannot improve their solution in its own. It may not be a globally optimal solution but that is how the Nash equilibrium is defined."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-1",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-1",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "11 Title",
    "text": "11 Title\n\n\n\n\nSlide 13"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gan-architecture",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gan-architecture",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "12 GAN architecture",
    "text": "12 GAN architecture\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gan-has-no-encoder-its-a-discriminator",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gan-has-no-encoder-its-a-discriminator",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "13 GAN has no ”encoder” — it’s a discriminator",
    "text": "13 GAN has no ”encoder” — it’s a discriminator\n\n\n\n\nSlide 15\n\n\n\n\nGAN has no encoder but instead it has a discriminator. The features that the discriminator learn are not necessarily as noise as for example the higuer level features may be car types or not. Because for detecting real vs fake all what we need is for example if the image is blurry. GANs do not end up with blurry features if trained correctly vs VAE tend to end up with blurry features\nWe cannot test the likelihood but we can only sample new data points"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#generator-network-x-gz-theta_g",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#generator-network-x-gz-theta_g",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "14 1) Generator network x = G(z; \\(\\theta\\)_G)",
    "text": "14 1) Generator network x = G(z; \\(\\theta\\)_G)\n\n\n\n\nSlide 16\n\n\n\n\nWe start with a lower dimensionality z, and generate x, except we are not giveing z any constraints. We are simply sampling it from some distribution. Since i.e you are sampling from a Gaussian distribution 0, 1, whithou any correlations between the dimensions of z, it means you can sample new samples easility because you just generate a new noise sample and is very likely that if you sample a new one, this has not been seen yet. So then this new image that is not seen it goes trhough the network and you get a completely new sample.\nIn the image on the right you see de-convolutions happening to create a new image. With transpose convolutions you also have the bias that neighboring pixels are similar and that operations are shared across the spatial dimensions"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#discriminator-network-y-dx-theta_d9",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#discriminator-network-y-dx-theta_d9",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "15 2) Discriminator network y = D(x; \\(\\theta\\)_D9)",
    "text": "15 2) Discriminator network y = D(x; \\(\\theta\\)_D9)\n\n\n\n\nSlide 17\n\n\n\n\nThe loss that we will learn is the binary cross entropy loss, because we are simply trying to detect between one and another class so the loss is a binary Cross entropy loss. We want to ouput 1 if real, 0 if fake,"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#generator-discriminator-implementation",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#generator-discriminator-implementation",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "16 Generator & Discriminator: Implementation",
    "text": "16 Generator & Discriminator: Implementation\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#quiz-the-starting-point-of-the-gan-is-the-random-noise-z.",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#quiz-the-starting-point-of-the-gan-is-the-random-noise-z.",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "17 Quiz: The starting point of the GAN is the random noise z.",
    "text": "17 Quiz: The starting point of the GAN is the random noise z.\n\n\n\n\nSlide 19\n\n\n\n\n\nIs always true because anything can be called regularizer. It simply means help training\nNot true because you are really using augmentations. You dont want a network that i.e to ouput heavily color jiter ouputs or you dont want vertically flipped images. Noisy isn’t really meant for that\nIt is not true because it is trained in a manner that it should do because as long as you sampling z from the Gaussian distribution so you could end up with a point that is very far away but is unlikely\nIf your dataset is small enough, for i.e it is only 5k points then, one millions points is close to be random at that point. And should work because even if you grid is a fixed thins and your model needs to map this to a dog and this to a cat. It will do it by itself\n\nZ needs to vary. If z is much smaller than the dataset size you will have trouble becuase then you can only generate say 3 different kind of classes but the dataset is 4 classes. Then the discriminator will have an easy time at telling what is real and what is fake"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#how-do-we-train-the-generator",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#how-do-we-train-the-generator",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "18 How do we train the generator?",
    "text": "18 How do we train the generator?\n\n\n\n\nSlide 20\n\n\n\n\nWe generate somthing that looks like a deer and we can compare it to an image of a deer. We cannot do this. They are completely independent\nThe question is how can we get meaningfull gradients if we are not comaparing the same things?\nWe have:\n\nMinimax Loss\nHeuristic non-saturating loss\nModifying GANs for max-likelihood"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#minimax-loss",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#minimax-loss",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "19 1) Minimax Loss",
    "text": "19 1) Minimax Loss\n\n\n\n\nSlide 21\n\n\n\n\nSo basically the loss of the generator is the negative version of the discriminator. his make it symmetric, the higher one the lower the other\nIt is called minimax because the discriminator tries to maximize its loss so \\(J_D\\) and then the generator tries to minimize that loss. What you end up with this min max loss is topically a saddle point. See next slide"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#minimax-loss-1",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#minimax-loss-1",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "20 1) Minimax Loss",
    "text": "20 1) Minimax Loss\n\n\n\n\nSlide 22\n\n\n\n\nWe get a saddle point as the learning stops at some point. At that point we have have gradients being zero in both directions\nThe generator first will not recieve any gradient anymore and at that point the discriminator will also not get any gradients any more. This behaviour is not great because it allows for easy analysis but it will cause training to get stuck because the job of the discriminator is very easy at that point."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#heuristic-non-saturating-loss",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#heuristic-non-saturating-loss",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "21 2) Heuristic non-saturating loss",
    "text": "21 2) Heuristic non-saturating loss\n\n\n\n\nSlide 23\n\n\n\n\nThis is the adapeted version of minmax which is a non saturated loss.\nChanges pretty easily in that, changes to the discriminator loss as it is. And instead you use the generator loss.\nNow, you cannot describe the equilibrium by a single loss anymore as we did earlier with the minus loss. That means you dont end up at a saddle point. The discriminator job is now to maximize the log-likelihood of discovering the real samples and the fake samples. And the generator maximizes the log likelihood of the discriminator being wrong\nWhat changes now is that the generator learns still when the discriminator is too good on real images. This is because previously the generator loss is also this mixture of how good the discriminator is on real images and how good it is in fake iamges and now the generator loss only depend on the generator ouputs\nWhat you generate with GANs can look like augmentations but depending on the augmentation, z can model that.\nA generator is not a different augmentation, a generator has a whole different purpose that you actually can model and generate whole new images. It is not like these faces are just slightly blurrier or slightly brighter of existing faces. It is literally new samples that do not exist in the real word. This is different from augmentations\nIn the above formulas we can sample three batches from the generator and one batch from the real distribution and still calculate the loss. You can leave out the 1/2, you can simply multiply by the learning rate by 1/2 and then you end with the same result. These batches are computed in average so you could compute this loss with ten times as many from the real one and only one from the real distribution"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#modifying-gans-for-max-likelihood",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#modifying-gans-for-max-likelihood",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "22 3) Modifying GANs for max-likelihood",
    "text": "22 3) Modifying GANs for max-likelihood\n\n\n\n\nSlide 24\n\n\n\n\nBy computing this loss we have the advantage that when the discriminator is optimal, the gradient for the generator gradient matches that of maximum likelihood."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#comparison-of-generator-losses",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#comparison-of-generator-losses",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "23 Comparison of Generator Losses",
    "text": "23 Comparison of Generator Losses\n\n\n\n\nSlide 25\n\n\n\n\n0 means we can detect the fake images and thus we set them with a lower value. When is 1 it means that the discriminator believes that the generated image is a true image.\n1: Not a good discriminator because he thinks the fake image is true\n0: Good discriminator, can distinguish between fake an real images"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#optimal-discriminator",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#optimal-discriminator",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "24 Optimal discriminator",
    "text": "24 Optimal discriminator\n\n\n\n\nSlide 26\n\n\n\n\nThe real distribution given the data is given in black. The model distribution is in green, that is the thing that the generator would be adapting. The discrimnator after a while stops learning. That is the picture from the right. The discriminator tries to see where the overlap is. At the last point the discriminator measure this ratio of overlaps"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#why-is-this-the-optimal-discriminator",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#why-is-this-the-optimal-discriminator",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "25 Why is this the optimal discriminator?",
    "text": "25 Why is this the optimal discriminator?\n\n\n\n\nSlide 27\n\n\n\n\nX: is data\nz: is the generated data\nWe sum the losses, in the second row we rewrite z into another x"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gans-and-jensen-shannon-divergence",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#gans-and-jensen-shannon-divergence",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "26 GANs and Jensen-Shannon divergence",
    "text": "26 GANs and Jensen-Shannon divergence\n\n\n\n\nSlide 28\n\n\n\n\nThe Jenses-Shannon divergence (in grey) is a metric to compare two distributions, it compares them by taking the weighted average of the KL.\nNow solving for the optimal discriminator and you get the two term. There you can see these two integrals. Rewriting we have that the loss is two times the Jensen-Shannon between the two probabilities distributions. For the optimal loss -2log2. Then that means the Jensen divergence is minimized, meaning it becomes zero. What we are optimizing is actually the Jensen divergence between these two probabilities distributions."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#is-the-divergence-important",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#is-the-divergence-important",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "27 Is the divergence important?",
    "text": "27 Is the divergence important?\n\n\n\n\nSlide 29\n\n\n\n\nThe question is why is the divergence important, it could be any kind of divergence like the KL and another ones."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#kl-vs-js",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#kl-vs-js",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "28 KL vs JS",
    "text": "28 KL vs JS\n\n\n\n\nSlide 30\n\n\n\n\nHere we see that the JS is symmetric"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#is-the-divergence-important-1",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#is-the-divergence-important-1",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "29 Is the divergence important?",
    "text": "29 Is the divergence important?\n\n\n\n\nSlide 31\n\n\n\n\nIt matters which divergence metric we take depending on what we do. You will either capture something that is in between two modes of the data which means the samples you will get will not be realistic or only rarely. Or the other option is that you capture one mode i.e with the reversed KL, except that in our case is the other way around which means the KL wil give you the mode capturing so depending on how you write you will have this risk of adversion behaviour this mode capturing behaviour.\nFor the backward KL is also called ‘zero forcing’ which make it more conservative, it will try to avoid putting probability mass where there is none. So it will avoid this zero areas whereas on the other way around is completely fine"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#general-observations",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#general-observations",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "30 General observations",
    "text": "30 General observations\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#training-procedure",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#training-procedure",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "31 Training procedure",
    "text": "31 Training procedure\n\n\n\n\nSlide 33"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#how-research-gets-done-part-8",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#how-research-gets-done-part-8",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "32 How research gets done part 8",
    "text": "32 How research gets done part 8\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenges-of-training-gans",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenges-of-training-gans",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "33 Challenges of Training GANs",
    "text": "33 Challenges of Training GANs\n\n\n\n\nSlide 35\n\n\n\n\nThere is three versions Loss of GANS and the original one (minimax) does not work in practice"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-1-vanishing-gradients",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-1-vanishing-gradients",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "34 Challenge 1: Vanishing Gradients",
    "text": "34 Challenge 1: Vanishing Gradients\n\n\n\n\nSlide 36\n\n\n\n\nHere in the right you see a trained GAN model. You keep the generator fix and you only train the discriminator for a few iterations. Then what you can observe is that in the y-axis is the loss that you would get to the generator so you keep the generator fix but you simply look at the loss that the generator would get for this given discriminator. What you can see is that the loss is given in absolute value and you can see that the blue line goes from 10^0 to 10^-3. So it crosses three levels of magnitude the red one even crosses 5 values of magnitud which means the discriminator can learn to distinguish this real-true extremely quickly and at that point your gradients are 10^-6 so then is like zero. At that point you do not get gradients anymore if the discriminator is really good, so the gradients will go to zero.\nIf the discriminator is bad then the generator will not get any good gradients as well which means it will not get good training. Specially earlier in the training where everything is bad. The discriminator will have an easier time because it can still recognize these weird patches and rare edges, so some solutions is that you give to the generator more iterations. Another solution is that you add an encoder that encodes a real image and then you discriminate on this encoding of the image"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-2-low-dimensional-supports",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-2-low-dimensional-supports",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "35 Challenge 2: Low dimensional supports",
    "text": "35 Challenge 2: Low dimensional supports\n\n\n\n\nSlide 37\n\n\n\n\nAnother challenge is the with low-dimensional manifolds. They idally capture a theme or an object and once you have them all the pixels can be generated from it very easily. That is why this low dimensional embedding space. However if this manifolds (which are surfaces or lines) they do not match at all so it is very easy for the discriminator to simple put a line trough this and understand they are different.\nIn training the manifold is not completely perfect so it is extremelly easy for these models to discriminate this because the Jensen divergence gives you extremelly low value if they do not have an overlap. So here you have a chicken-egg problem that the loss will start being meningful once your generationed data is good.\nFor this we have the solution wGAN which basically uses other metric to compare the real vs false. So even if you have 2D surfaces, such as here, you can see that the overlap will be fairly low that means the discriminator has a easy time"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-3-batch-normalization",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-3-batch-normalization",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "36 Challenge 3: Batch Normalization",
    "text": "36 Challenge 3: Batch Normalization\n\n\n\n\nSlide 38\n\n\n\n\nBatch norm is a problem here because you are mixing for example batches of real images and batches from not real images. Then you compute the bach statistics from this combination. That leads to a smooth but awkward faces. So instead of this you can do the following. See next slide"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#reference-batch-normalization",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#reference-batch-normalization",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "37 Reference batch normalization",
    "text": "37 Reference batch normalization\n\n\n\n\nSlide 39\n\n\n\n\nHere we can keeo a refence batch. Say you train with two minibatches, you have a reference batch where you compute the mean and standard deviation. Then you use the other second minibatch for training. However this lack of variation wrt to the reference batch will give you troubles because one of the main benefits from batch norm was that it sort of gives you this variability and regularization effect. A solution for that is"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#virtual-batch-normalization",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#virtual-batch-normalization",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "38 Virtual batch normalization",
    "text": "38 Virtual batch normalization\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-4-convergence",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-4-convergence",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "39 Challenge 4: Convergence",
    "text": "39 Challenge 4: Convergence\n\n\n\n\nSlide 41\n\n\n\n\nThe other challenge is convergence. In the context of Generative Adversarial Networks (GANs), convergence to a saddle point is the desired outcome of the training process. This is because saddle points represent a stable equilibrium point in the game between the generator and the discriminator, where neither agent can unilaterally improve its performance. In other words, both the generator and the discriminator are “optimal” in their respective roles, but they cannot push each other further towards their respective optima without sacrificing their own performance.\nThe Challenges of Saddle Points\nWhile saddle points are the desired outcome of GAN training, they can be challenging to achieve due to the complex and non-convex nature of the optimization landscape. This means that there are many different paths that the training process can take, and it is possible to get stuck in local optima that are not truly optimal."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-mode-collapse",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-mode-collapse",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "40 Challenge: mode collapse",
    "text": "40 Challenge: mode collapse\n\n\n\n\nSlide 42\n\n\n\n\nMode-collapse is a phenomenon that occurs when the generator becomes unable to generate a diverse range of samples. Instead, the generator becomes fixated on producing a small subset of samples, effectively “collapsing” into a single mode. This can result in blurry, repetitive, or unrealistic generated images.\nFor example if you have this simple target distribution of these 8 Gaussians and you want to train a generative model, it sometimes will fit towards one of these modes and it will have a difficult time fitting to one of these modes of input data. Similarly for image based models you can see that all these images of flowers they literally look the same, so there is no variation capture at all. It is simply capturing one of the modes. This happens because if the discriminator wants to see that it look similar to the real data. Then the Generator can simply learn only one target and not care about divergence at all.\nThis is not desired and so a solution would be to add diversity as a regularization loss"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#potential-solution-regularize-for-diversity",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#potential-solution-regularize-for-diversity",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "41 Potential solution regularize for diversity",
    "text": "41 Potential solution regularize for diversity\n\n\n\n\nSlide 43\n\n\n\n\nThe solution for falling into mode-collapse is that you compare each sample by the other examples in the mini-batch. Yuo check whether you can discriminate between these two. You look what is the entropy whithin these samples. And you can add this as a loss. If you simply inforce a huge amount of entropy whithin the mini-batch then this will give more more varied exmaples but this will not look realistic pictures anymore"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#mode-collapse-vs-over-generalisation",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#mode-collapse-vs-over-generalisation",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "42 Mode-collapse vs over-generalisation",
    "text": "42 Mode-collapse vs over-generalisation\n\n\n\n\nSlide 44\n\n\n\n\nHere in over-generalization is not like you train with cat and dogs and then you can generate pictures of planes. Here in generalization images go over the training set which generally means they are looking realistic\nIn the case of unconditional generation, we generate dogs and catgs. In the case con conditinal generation we generate dogs from different type of dogs"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-how-to-evaluate",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-how-to-evaluate",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "43 Challenge: how to evaluate?",
    "text": "43 Challenge: how to evaluate?\n\n\n\n\nSlide 45\n\n\n\n\nFReshee inception distance, You push the real and the generated images of same size through a pre-trained inception network. You then use the dense features and compare these dense features by contruction a Gaussian distribution out of them. So you then comparing them by simply taking the difference between the dataset wise features. This tells me how good a set of generated images matches another images.\n\nLow FID: the distribution of the real images is similar to the fake ones.\nFor instance if you do mode dropping the FID will be low because the distance to all these dog images would be suddenly pretty high\nHigh FID: the distribution of the real images is different to the fake ones.\nIf these things do not look at all like the real class then the FDI would be high\n\nThe Fréchet Inception Distance (FID) is a metric used to measure the similarity between the distribution of real and fake images generated by a GAN. It is calculated by comparing the activations of a pre-trained InceptionNet model on both the real and fake images.\nThe FID is a powerful metric for evaluating the performance of GANs, as it can provide insights into both the realism and diversity of the generated images. A lower FID indicates that the distribution of the fake images is more similar to the distribution of the real images, suggesting that the GAN is generating more realistic and diverse images."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-beyond-images",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#challenge-beyond-images",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "44 Challenge: beyond images",
    "text": "44 Challenge: beyond images\n\n\n\n\nSlide 46\n\n\n\n\n\nText are discrete chunks, so you cannot model as continous variables where you can simply push the gradietns trough, which is what we are doing for RGB images. For these iamges we are pretending that they are continuous."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#some-open-challenges-for-gans",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#some-open-challenges-for-gans",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "45 Some open challenges for GANs",
    "text": "45 Some open challenges for GANs\n\n\n\n\nSlide 47\n\n\n\n\nWhat sorts of distributions can GANs model?\nGANs can model a wide variety of distributions, including both continuous and discrete distributions. For example, GANs have been used to generate images, text, music, and code.\nWhat can we say about the global convergence of the training dynamics?\nThe global convergence of GAN training dynamics is a challenging problem that is not fully understood. However, there has been some progress in developing theoretical guarantees for the convergence of GANs. For example, it has been shown that GANs can converge to a Nash equilibrium under certain conditions.\nHow should we evaluate GANs and when should we use them?\nThere is no single best way to evaluate GANs. However, some common evaluation metrics include the Fréchet Inception Distance (FID) for image generation and the BLEU score for text generation. GANs should be used when the goal is to generate realistic and diverse samples from a given distribution.\nGAN scaling: dataset size and model size\nGANs can be scaled to large datasets and model sizes. However, scaling GANs can be challenging, as it can lead to training instability and mode collapse. There are a number of techniques that have been developed to help scale GANs, such as gradient clipping and spectral normalization.\nGANs and adversarial examples?\nGANs have been used to generate adversarial examples, which are inputs that are designed to fool machine learning models. Adversarial examples can be a security risk, as they can be used to attack machine learning models that are used in critical applications, such as facial recognition and self-driving cars."
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#one-sided-label-smoothing",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#one-sided-label-smoothing",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "46 One-sided label smoothing",
    "text": "46 One-sided label smoothing\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#benefits-of-label-smoothing",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#benefits-of-label-smoothing",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "47 Benefits of label smoothing",
    "text": "47 Benefits of label smoothing\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#ganss-sometimes-explode",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#ganss-sometimes-explode",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "48 GANSs sometimes explode",
    "text": "48 GANSs sometimes explode\n\n\n\n\nSlide 50"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-2",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-2",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "49 Title",
    "text": "49 Title\n\n\n\n\nSlide 51"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#dcgan",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#dcgan",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "50 DCGAN",
    "text": "50 DCGAN\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#examples",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#examples",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "51 Examples",
    "text": "51 Examples\n\n\n\n\nSlide 53"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#even-vector-space-arithmetics",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#even-vector-space-arithmetics",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "52 Even vector space arithmetics …",
    "text": "52 Even vector space arithmetics …\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#can-generate-new-views",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#can-generate-new-views",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "53 Can generate new views",
    "text": "53 Can generate new views\n\n\n\n\nSlide 55"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#wasserstein-gan",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#wasserstein-gan",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "54 Wasserstein GAN",
    "text": "54 Wasserstein GAN\n\n\n\n\nSlide 56"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#differences-in-gans",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#differences-in-gans",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "55 Differences in GANs",
    "text": "55 Differences in GANs\n\n\n\n\nSlide 57"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#bigbigan",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#bigbigan",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "56 BigBiGAN",
    "text": "56 BigBiGAN\n\n\n\n\nSlide 58"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#so-what-changed-more-data-no",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#so-what-changed-more-data-no",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "57 So what changed? More data? – No",
    "text": "57 So what changed? More data? – No\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#so-what-changed-architectures-and-compute-yes",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#so-what-changed-architectures-and-compute-yes",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "58 So what changed? Architectures and compute: yes",
    "text": "58 So what changed? Architectures and compute: yes\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#conditional-gan",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#conditional-gan",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "59 Conditional GAN",
    "text": "59 Conditional GAN\n\n\n\n\nSlide 61"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#image-to-image-translation",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#image-to-image-translation",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "60 Image to image translation",
    "text": "60 Image to image translation\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#adversarial-autoencoders-and-adversarial-network-in-latent-space",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#adversarial-autoencoders-and-adversarial-network-in-latent-space",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "61 Adversarial AutoEncoders: and adversarial network in latent space",
    "text": "61 Adversarial AutoEncoders: and adversarial network in latent space\n\n\n\n\nSlide 63\n\n\n\n\nIn the autoencoder we want to minimize the reconstructed error. Here we simply add an adversarial loss in the latent space \\(z\\). That means we will now be containing the latent space in a manner that it will be easy to sample from.\nEncoder: This part takes an input image x and encodes it into a latent representation z through a deterministic function q(z|x), which is typically a neural network. The deterministic aspect means that for the same input x, the encoder will always produce the same latent code z.\n\n\n\n\nHere the q(z|x) is called the variational posterior. It can be Gaussian and then we will make it Gaussian\nBefore we have that we wanted to approximate the real posterior p(z|x) with this q(z|x). So here we are saying that before we use the KL in VAE to give it structure but not the AAE uses an adversarial network for this.\nRemember that: p(z∣x) is the true posterior distribution of the latent variables given the input data, which is generally intractable to compute directly. Remember the picture below we were unable to commpute this posterior because it related to compute this integral\n\n\n\n\nThe AAE does not need to operate in the image domain it can just work in the latent domain. It also does not need to be image model but instead a bunch of MLPs"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#cyclegan-img2img-models",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#cyclegan-img2img-models",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "62 CycleGAN: “img2img” models",
    "text": "62 CycleGAN: “img2img” models\n\n\n\n\nSlide 64\n\n\n\n\nIt is called cycle because you go from one domain to another, if you look at for example image B. You go from domian x to y and then you go back to domain x. And then you know that you should end up in the same place. If you do then that means that the mapping worked perfectly"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#stylegan-and-styleganv2",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#stylegan-and-styleganv2",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "63 StyleGAN and StyleGANv2",
    "text": "63 StyleGAN and StyleGANv2\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-3",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-3",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "64 Title",
    "text": "64 Title\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#overview-of-methods",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#overview-of-methods",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "65 Overview of methods",
    "text": "65 Overview of methods\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#basic-idea-of-diffusion-models-learning-how-to-denoise",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#basic-idea-of-diffusion-models-learning-how-to-denoise",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "66 Basic idea of diffusion models: learning how to denoise",
    "text": "66 Basic idea of diffusion models: learning how to denoise\n\n\n\n\nSlide 68\n\n\n\n\nYou start with a real image and then you start increasingly adding noise. So you are trying to learn something that is noisy to something that is less noisy.\nThis is a completely new paradigm\n\n\n\n\n\n\nThis is how difussion models work\n\n\n\n\nForward Process (Adding Noise): Starting with an original image \\(X_0\\), noise is gradually added over a series of steps until the image becomes a noisy version \\(X_T\\) that is typically assumed to follow a Gaussian distribution. This process is termed the forward diffusion process and is denoted by the rightward arrows in the image. Each step adds a controlled amount of noise, progressively making the image less recognizable and more like random noise.\nReverse Process (Removing Noise): The key idea behind diffusion models is to learn how to reverse this noising process. This is where the model learns to ‘denoise’ the image. Starting from a noisy image \\(X_T\\), the model attempts to recover the clean image \\(X_0\\) through a series of reverse steps. This is the reverse diffusion process and is indicated by the leftward arrows in the image. Each step in the reverse process is denoted by \\(p_\\theta(X_{t-1}|X_t)\\), which is the learned distribution to predict the cleaner image \\(X_{t-1}\\) from the noisier image \\(X_t\\).\nVariational Lower Bound: The training of diffusion models involves optimizing the variational lower bound, which is a way to ensure that the learned distribution over the reverse process closely matches the true distribution of the data. It’s a technique derived from variational inference used to approximate complex distributions.\nUnknown Transition Distributions: In the diagram, it is noted that \\(q(X_{t-1}|X_t)\\) is unknown. This represents the true reverse transition probabilities from a noisier to a less noisy image, which we do not have explicitly. The model has to learn an approximation \\(p_\\theta(X_{t-1}|X_t)\\) without knowing \\(q(X_{t-1}|X_t)\\).\nLearning Process: During training, the model learns the parameters \\(\\theta\\) that define how to reverse the noise added during the forward process effectively. The learned distribution \\(p_\\theta(X_{t-1}|X_t)\\) is used to iteratively generate less noisy images until a clear image is formed.\nImage Generation: Once trained, diffusion models can generate new images by sampling from the noise distribution \\(X_T\\) and applying the reverse process. This allows the model to create images that were not in the training set but share the same statistical properties.\n\n\n\nEscentially what you are trying to learn is the green function which is how to reverse the noise added during the forward process"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#diffusion-models-turn-generative-learning-into-a-sequence-of-supervised-problems",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#diffusion-models-turn-generative-learning-into-a-sequence-of-supervised-problems",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "67 Diffusion models turn generative learning into a sequence of supervised problems",
    "text": "67 Diffusion models turn generative learning into a sequence of supervised problems\n\n\n\n\nSlide 69\n\n\n\n\nThese difussion models basically turn generative modelling into a sequence of supervised models because you have your image and you have noise. So you know how it looks before so you simply are trying to turn this whole dinoising procedure into a different steps. These steps are form \\(t\\) to \\(t-1\\). You can use this as a supervised learning method"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#the-architecture-a-modified-u-net-that-uses-diffusion-time-t",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#the-architecture-a-modified-u-net-that-uses-diffusion-time-t",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "68 The architecture: a modified U-Net that uses diffusion time t",
    "text": "68 The architecture: a modified U-Net that uses diffusion time t\n\n\n\n\nSlide 70\n\n\n\n\nThe architecture here is a U-Net because a U-Net is an architecture that takes an image in and ouputs an image out. Like one of those segmentations. The only difference is that there is a time \\(t\\) which is used for going from going noisy to less noisy. In adittion the NN also knows when this time frame is"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#combining-this-with-text-as-cond.-inputs-dall-e-v2-unclip",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#combining-this-with-text-as-cond.-inputs-dall-e-v2-unclip",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "69 Combining this with text as cond. inputs: DALL-E v2 / “unCLIP”",
    "text": "69 Combining this with text as cond. inputs: DALL-E v2 / “unCLIP”\n\n\n\n\nSlide 71\n\n\n\n\nSo you first train a CLIP which gives you an encoding and a text encoding and then image encoding. Afterwards you use this text encondings to convert"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#final-note-about-deep-fakes-and-ethics",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#final-note-about-deep-fakes-and-ethics",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "70 Final note about deep fakes and ethics",
    "text": "70 Final note about deep fakes and ethics\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#recommended-watch-just-34min",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#recommended-watch-just-34min",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "71 Recommended watch (just 34min)",
    "text": "71 Recommended watch (just 34min)\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#quiz-what-dimensions-need-to-be-considered-when-thinking-about",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#quiz-what-dimensions-need-to-be-considered-when-thinking-about",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "72 Quiz: what dimensions need to be considered when thinking about ;",
    "text": "72 Quiz: what dimensions need to be considered when thinking about ;\n\n\n\n\nSlide 74"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#more-on-that-previous-one..",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#more-on-that-previous-one..",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "73 More on that previous one..",
    "text": "73 More on that previous one..\n\n\n\n\nSlide 75"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#sy-generative-adversarial-networks-gans-are-a-type-of-deep-learning-model",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#sy-generative-adversarial-networks-gans-are-a-type-of-deep-learning-model",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "74 Sy Generative Adversarial Networks (GANs) are a type of deep learning model",
    "text": "74 Sy Generative Adversarial Networks (GANs) are a type of deep learning model\nSi | I } } I } } ar of GANs that is used for unsupervised learning. GANs consist of two components: a\ngenerative model, which is trained to generate samples that are similar to a\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-4",
    "href": "blog/2023-12-07_generative-adversarial-networks-and-diffusion-models/index.html#title-4",
    "title": "Generative Adversarial Networks and Diffusion models",
    "section": "75 Title",
    "text": "75 Title\n\n\n\n\nSlide 77"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "",
    "text": "Lecture Notes UvA on 2-10-2023\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Education\n                            \n                        \n                                            \n                            \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Education\n                            \n                        \n                                            \n                            \n                               \n                                Machine Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 7, 2023"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#supervised-learning",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#supervised-learning",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "1 Supervised Learning",
    "text": "1 Supervised Learning\nHere the mission of the model is to learn the relation between the labels and the features."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#unsupervised-learning",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#unsupervised-learning",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "2 Unsupervised Learning",
    "text": "2 Unsupervised Learning\nYou do not have labels you only have features.\nThe mission of the model is to discover latent or hidden structures.\nLatent means to be not measurable, to be hidden i.e. thoughts –&gt; words. You can sample words, you have data from the words, but for thoughts you don’t have data available, the variable is hidden\n\n\n\n\nUnsupervised learning is understanding the data discovering properties to exploit."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#unsupervised-vs.-supervised-learning",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#unsupervised-vs.-supervised-learning",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "3 Unsupervised vs. Supervised learning",
    "text": "3 Unsupervised vs. Supervised learning"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#latent-variable-models",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#latent-variable-models",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "4 Latent variable Models",
    "text": "4 Latent variable Models\n\n\n\n\nIn Unsupervised learning:\nYou have Latent variable \\(Z\\) and that generates \\(X\\).\nIn supervised learning:\nFor instance in Classification you would have features \\(X\\) and that generates some label \\(y\\).\n\n\nSo you could think of Unsupervised learning as we are given our labels and we need to learn the features that generate the label\n\n\\(Z\\) in the figure above are the labels. We want to get the distriution of the data in the continuous case i.e. by marginalizing over \\(z\\) to get the probability of the data alone."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#examples",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#examples",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "5 Examples",
    "text": "5 Examples\n\n\n\n\n\nFor instance in Clustering we are given scattered plot data and you want to find the hidden groups.\nFor instance in Dimensionality Reduction you are given a 2-dim data and you wonder whether you can learn a lower representation that could describe my data"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-clustering",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-clustering",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "6 K-Means Clustering",
    "text": "6 K-Means Clustering\n\nGoal: Automatically partition unlabeled data into groups of similar data points.\n\n\nUseful for:\n\nAutomatically organizing data.\nUnderstanding hidden structure in data.\nPreprocessing for downstream analysis."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#applications",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#applications",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "7 Applications",
    "text": "7 Applications"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#clustering-with-k-means",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#clustering-with-k-means",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "8 Clustering with K-means",
    "text": "8 Clustering with K-means\n\nGoal: minimize distance between data points and \\(k\\) cluster representations"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "9 K-means",
    "text": "9 K-means\n\n\n\n\n\nYou need to set \\(K\\) the number of clusters ahead of time\nThe mean \\(m_j\\) per ach cluster\nThe cluster that corresponds \\(C_j\\)\nYou only compute the \\(x_n\\) that belongs to that cluster \\(C_j\\) so that we do not compute for all data points\n\n\n\n\n\n\\(|C_j|\\) is the number of datapoints assigned to that cluster \\(j\\)"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-clustering-example",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-clustering-example",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "10 K-Means Clustering Example",
    "text": "10 K-Means Clustering Example\n0. Randomly initialize \\(k=3\\) means to data points. (That means we will end up with \\(3\\) Clusters)\n\n\n\n\n1. Assign point to cluster with nearest representation.\n\n\n\n\n2. Compute new cluster representation by taking mean of currently assigned points.\n\n\n\n\nThe coloured points you see above are the new computed means. They are NOT data points they are just the locations where the mean should be.\nRepeat: Step 1 -&gt; 2\n1. Assign point to cluster with nearest representation.\n\n\n\n\n2. Compute new cluster representation by taking mean of currently assigned points.\n\n\n\n\nRepeat: Step 1 -&gt; 2\n1. Assign point to cluster with nearest representation.\n\n\n\n\n2. Compute new cluster representation by taking mean of currently assigned points.\n\n\n\n\nConverging\n1. Assign point to cluster with nearest representation.\n\n\n\n\n\nNote: The algorithm has converged: the cluster assignments do not change. So stop here\n\n\nFinal means and cluster assignments"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-clustering-pipeline",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-clustering-pipeline",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "11 K-Means Clustering Pipeline",
    "text": "11 K-Means Clustering Pipeline\nOur model is represented by our \\(k\\) mean vectors: \\(\\{{m_j\\}}_{j=1}^{k}\\)"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-optimization",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-optimization",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "12 K-Means: Optimization",
    "text": "12 K-Means: Optimization\n\nThe number of iterations to get the same error is lower in the second run\nThe cost in the same iteration number is higuer in the first iteration\n\n\n\n\n\n\n\nAlgorithm guaranteed to decrease error at each iteration, but not guaranteed to obtain Global Optimum.\nIn between:\n\nIn Linear Regression you can find the global minimum\nIn Neural Networks we cannot never find the global optima, we need to adjust our steep size\nIn K-means you guarantee to reduce error but not guaranteed that if you keep decreasing you will find global minimum.\n\n\nFor the last reason above (the not guaranteeing thing people in practice re-run algorithm multiple times (and take solution with lowest cost, for instance)"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-overfitting",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-overfitting",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "13 K-Means: Overfitting",
    "text": "13 K-Means: Overfitting\n\n\n\nThe more clusters we have, the better the chance that a cluster mean will be near held-out data, thus improving the validation error. Thus, looking at validation error alone is not helpful.\nSo how do we quantify this, because the more clusters then the lower the validation error. How do we choose an appropriate \\(k\\)-number: Answer with the Elbow method discussed below"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-choosing-k",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-choosing-k",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "14 k-Means: Choosing \\(k\\)",
    "text": "14 k-Means: Choosing \\(k\\)\n\nMethod 1\n\nElbow method: is called like this because think about the error function and your arm. Now we look at the point of hinge where the error loss changes significantly.\n\nFor instance in the image below you find your elbow point where adding more cluster does not improve the error.\n\n\n\n\n\nMethod 2\nAnother method to choose an appropiate \\(k\\) value is: to have a modified error function that account for the number of clusters:\n\n\n\n\nFor instance if I have \\(k=1\\) then the newly introduced term becomes zero. But if you have more than one cluster it is going to get penalize with a factor \\(\\lambda\\)\n\n\nMethod 3\nHere you do external supervision, that means i.e. you have houses clustering by price and size, if you new or wanted to group some houses intentionally then you could stop the \\(k\\)-size number increasing because you have knowledge domain. Thus you control the granularity.\n\n\nMethod 4\n\n\n\n\nYou want the Dunn index to be large, meaning you want in the numerator to be large meaning the minimum distance between cluster representations (so between means) should be large and conversely you want the denominator to be small meaning that you want the maximum distance between points within the cluster to be close to one another (meaning we want to have all point very close to the mean of this cluster)"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-generalizations",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#k-means-generalizations",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "15 K-Means: Generalizations",
    "text": "15 K-Means: Generalizations\nYou could also compute the error function \\(E\\) with other function rather than squarer error, you could for instance use another distance function like Hamming, or medians like below:"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#improvements-for-big-data",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#improvements-for-big-data",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "16 Improvements for Big Data",
    "text": "16 Improvements for Big Data\n\nWhen the data is long then we may take a lot of time to compute one iteration, the solution for this could be to use:\n\nStochastic gradient: for each datapoint, update nearest cluster mean”\n\n\nWhat this equation is allowing us is to compute the mean not looking at all the data points, again we have the value \\(\\eta\\) which is our learning rate\nThe mean that we are computing its only nearest to the cluster we compute it"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#application-image-compression",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#application-image-compression",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "17 Application: image compression",
    "text": "17 Application: image compression"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#failures-of-k-means",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#failures-of-k-means",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "18 Failures of K-Means",
    "text": "18 Failures of K-Means\nThis is because of the reliance of square distance, then the points close to the means do not preserve the non-euclidean aspect of this data\n\n\n\n\nHere we should expect to see three clusters: 2 ears and 1 face. The explanation to this is because we cannot modify how th distance will be computed. That means we cannot say that the region of the ears to have a smaller distance, compared to the face’s mean.\n\n\n\n\nAnother way to look at this problems is that, you cannot change the scale of the clusters, every cluster would have more or less the same square distance around its cluster mean. That means that you cannot have tighter distance for the ears and more relaxed notion of distance for the same. They are gonna have same squared distances with equal weight."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#pros-cons-of-k-means",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#pros-cons-of-k-means",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "19 Pros & Cons of K-Means",
    "text": "19 Pros & Cons of K-Means\n\nGood:\n\nSimple to implement\nFast\n\nBad:\n\nLocal minima\nModel only “spherical” clusters that cannot change size\nSensitive to the features scale\nNumber of clusters \\(K\\) to be chosen in advance\nCluster assignments are “hard”, not probabilistic =&gt; next topic, Gaussian Mixture Model"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#minimize-error-em-algorithm",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#minimize-error-em-algorithm",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "20 Minimize Error (EM Algorithm)",
    "text": "20 Minimize Error (EM Algorithm)\n\nThe above algorithm can be formalized in the Expectation Maximization\n\n\nThe E step computes the assignments\nThe M step computes the means\n\n\n\n\n\nSame as computing:\n\n\n\n\n\n20.1 Derivation of M-Step"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#constrained-optimization",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#constrained-optimization",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "21 Constrained optimization",
    "text": "21 Constrained optimization\n\n\n\n\n\n21.1 Lagrange Constrained Optimization: Example"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#probabilistic-version-of-k-means-clustering",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#probabilistic-version-of-k-means-clustering",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "Probabilistic version of k-Means Clustering",
    "text": "Probabilistic version of k-Means Clustering\n\nIn this section we talked about the probabilistic side of k-means"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#clustering-with-gaussian-mixture-model-gmm",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#clustering-with-gaussian-mixture-model-gmm",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "22 Clustering with Gaussian Mixture Model (GMM)",
    "text": "22 Clustering with Gaussian Mixture Model (GMM)\n\nWe assume that the data follows a generative model. Recall what a generative model is:\n\n\nApproaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.\n\n\nWe also assume that with each observation there is an associated discrete latent variable. Recall what latent variable is:\n\n\nA latent variable is a variable that is not directly observed is hidden like our thoughs, that is z\n\nFor instance: in the image below we see that the purple data, each of these dots is related to a latent variable \\(z\\), for instance a discrete class \\(z\\) i.e, that can take on the values {red, green, or blue}. Now these latent variables are hidden is not directly observable, you dont’ know them. Here the latent variable is discrete can take values like: red, green and so on, can also be like the example of the houses, it can be boat houses, studios and so on.\nRecap from the upper text. It means that we are no seeing the bottom left graph with the distinctions between classes but we only see the purple dots\n\nWe also assume there is a join distribution \\((x,z) \\sim p(x,z)\\) so in this case in the left plot\n\n\nGoal: given our unlabelled data \\(x \\sim p(x)\\) (purple dots) you want to go to a cluster ( from our latent variable) assignment. Mathematically you want to be able to compute \\(p(z|\\textbf{x})\\) which is the right bottom graph where you see all datapoints assigned one color. For instance you want to say that a purple dot in the corner belows to class \\(z = green\\) or another dot that belongs to \\(z = blue\\). How do we do this? Sol: in a probabilistic way using Multiple Gaussians hence called Gaussian Mixture models."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#modelling-assumptions",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#modelling-assumptions",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "23 Modelling assumptions",
    "text": "23 Modelling assumptions\n\nWe model our prior with a Generalized Bernoulli distribution\n\nCategorical Distribution: \\[\nP(X = x_i) = p_i, \\quad \\text{for } i = 1, 2, \\ldots, K\n\\]\nwhere \\(p_i\\) is the probability of outcome \\(x_i\\), and \\(\\sum_{i=1}^{K} p_i = 1\\).\nGeneralized Bernoulli: \\[\nP(X = x) = \\theta^x (1 - \\theta)^{1 - x}, \\quad \\text{for } x = 0, 1\n\\]\nwhere \\(\\theta\\) is the probability of \\(X = 1\\), and \\((1 - \\theta)\\) is the probability of \\(X = 0\\)\n\nThe second part part of our generative model is the latent class or conditional distributions \\(p(\\textbf{x}|z_k)\\) modelled with Gaussians. NOTE this is not the likelihood\n\n\\(p(\\textbf{x}|z_k)\\): is the probability that if I know the class \\(z_k\\) then I will observed the data in i.e the red region, see picture above in orange highlighted\n\nThe \\(\\pi_k\\), \\(\\mu_k\\), \\(\\Sigma_k\\) are learnable parameters\nWith the prior and the latent conditional distribution we can compute the joint \\(p(\\textbf{x},z_k =1)\\). That is, we compute the probability for each class\nGiven this model we can define a \\(p(x)\\) for the entire data set which is simply the marginalization over \\(z\\). But this is gonna be a sum of Gaussians, see the equation at bottom of slide. Because of the sum of gaussians we call it Gaussian mixture model\n\n\n\n\n\nThis is how we model \\(p(x)\\) but now we want to optimize it and since we are working in a probabilistic setting: we can compute the posterior \\(p(z_k=1|\\textbf{x})\\) which we know we can get it with Bayes rule.\n\n\\(p(z_k=1|\\textbf{x})\\) I have a observation x and we want to know to which class \\(z\\) it belongs. But this will give a opacity meaning its not totally certain\n\n\nRecall our goal is to get: \\(p(z_k=1|\\textbf{x})\\). Also recall that in k-means we have a hard assignment of labels. in this case because we are using probabilities then we will end up with soft assignment which means that we will have i.e. one point as green color but also a bit transparent with a red color.\n\n\n23.1 The Posterior\n\n\n\n\n\n\n23.2 The Log-likelihood\n\nThe likelihood can be optimized i,e by taking the log and then derivative with respect to its parameters \\(\\mu_k\\), \\(\\Sigma_k\\)\n\nWait a min, but when you replace the gaussian (which is how we assume to model the data) then because of the sumation over \\(\\pi_k\\) things get matematically and analitically complicated and then we need to resort in another optimization rather just taking the derivative and set it to zero. Thus Expecation Maximization comes into play\nCrucial bit why we are taking the log-likelihood as \\(p(\\text{x})\\). See this below:\n\n\n\n\n\nThat is why we compute:"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#expectation-maximization-algorithm-em",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#expectation-maximization-algorithm-em",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "24 Expectation-Maximization algorithm (EM)",
    "text": "24 Expectation-Maximization algorithm (EM)\n\nHere we assume the posteriors \\(\\gamma(z_k)\\) do not depend on the parameters\n\nHow to get to teh answers for the parameters?\n\nWith the values for the parameters anc compute the expected posterior\nMaximize for parameters\n\nThe cycle repeats"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#example-gmm",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#example-gmm",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "25 Example: GMM",
    "text": "25 Example: GMM\n\nHere the optimization of the likelihood is called Maximization\n\n\n\n\n\nIn K-means I only updated the cluster centers whereas here I update all the parameters \\(\\pi_k\\), \\(\\mu_k\\), \\(\\Sigma_k\\)"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#derivations-for-em-algorithm",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#derivations-for-em-algorithm",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "26 Derivations for EM algorithm",
    "text": "26 Derivations for EM algorithm\n\nRefer to slides"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#the-mouse-data-again",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#the-mouse-data-again",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "27 The mouse data again",
    "text": "27 The mouse data again\n\n\n\n\nDifferences between K-Means and GMM: - K-means can only custer groups of the same size, so same distances from each mean cluster - GMM can compensate for this"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#how-do-we-assign-points-to-cluster",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#how-do-we-assign-points-to-cluster",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "28 How do we assign points to cluster?",
    "text": "28 How do we assign points to cluster?"
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#final-remarks",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#final-remarks",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "29 Final remarks",
    "text": "29 Final remarks\n\n\n\n\n\nWe need more initializations in GMM because of different parameters so the trick its to do it with k-means\nSame as k-means we cannot expect to find the best solution so the global minimum for the parameters because the problem its not convex. That means the final solution depends on how you initializes the model parameters\nThe last point refers that GMM is for unsupervised learning so you compute things with the latent variable whereas in QDA its for supervised learning."
  },
  {
    "objectID": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#faq",
    "href": "blog/2023-10-07_latent-variable-models-&-k-means-clustering/index.html#faq",
    "title": "Latent Variable Models & K-Means Clustering",
    "section": "30 FAQ",
    "text": "30 FAQ\n\n\n\n\n\n\nWhat is Latent variable?\n\n\n\n\n\nA latent variable is a variable that is not directly observed but is inferred or estimated from other observed variables\nFor example, in psychology, intelligence is a latent variable because it cannot be directly measured. Instead, researchers use various observable indicators, such as IQ test scores, academic performance, and problem-solving abilities, to infer a person’s level of intelligence.\nLatent class analysis (LCA) is a statistical technique used to identify unobservable or latent classes or groups within a population based on patterns of responses to a set of observed categorical variables\nWhat do we use Latent variables for?\nThe primary role of the latent variables is to allow a complicated distribution over the observed variables to be represented in terms of a model constructed from simpler (typically exponential family) conditional distributions.\n\n\n\n\n\n\n\n\n\nCategorical Distribution AND Generalized Bernoulli Distribution:\n\n\n\n\n\nThe categorical distribution describes the probability distribution of a discrete random variable with \\(K\\) possible outcomes or categories. The probability mass function (PMF) of the categorical distribution is given by:\n\\[\nP(X = x_i) = p_i, \\quad \\text{for } i = 1, 2, \\ldots, K\n\\]\nwhere \\(p_i\\) is the probability of outcome \\(x_i\\), and \\(\\sum_{i=1}^{K} p_i = 1\\).\nGeneralized Bernoulli Distribution: The generalized Bernoulli distribution is a distribution for a binary random variable that can take on values 0 or 1 with different probabilities. The PMF of the generalized Bernoulli distribution is given by:\n\\[\nP(X = x) = \\theta^x (1 - \\theta)^{1 - x}, \\quad \\text{for } x = 0, 1\n\\]\nwhere \\(\\theta\\) is the probability of \\(X = 1\\), and \\((1 - \\theta)\\) is the probability of \\(X = 0\\). This distribution is sometimes used when modeling binary data with different success probabilities."
  },
  {
    "objectID": "blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.html",
    "href": "blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.html",
    "title": "Script to extract pages of PDF as images & apply crop",
    "section": "",
    "text": "Description of this Notebook\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Python\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Python\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 5, 2023\n        \n      \n      \n        \n      \n      \n\n    \n        Quick Links\n    \n         Quick Links:\n                                    \n                 Code"
  },
  {
    "objectID": "blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.html#goal",
    "href": "blog/2023-11-05_script-to-extract-pages-of-pdf-as-images-and-crop-them/index.html#goal",
    "title": "Script to extract pages of PDF as images & apply crop",
    "section": "Goal",
    "text": "Goal\nImagine you want to extract pages from a PDF as images and then you want to crop them to an specific size. The following scrip does exactly that\npip install Pillow\npip install pdf2image\nbrew install poppler\n\nfrom pdf2image import convert_from_path\nfrom PIL import Image\nimport os\n\n# Function to crop an image to specific dimensions\ndef crop_image(input_path, output_path, left, top, right, bottom):\n    image = Image.open(input_path)\n    cropped_image = image.crop((left, top, right, bottom))\n    cropped_image.save(output_path)\n\n# Function to extract all pages from a PDF and crop them as images\ndef extract_and_crop_pdf(pdf_path, output_dir, width, crop_up, crop_down):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    images = convert_from_path(pdf_path)\n\n    for page_number, pdf_image in enumerate(images):\n        output_path = os.path.join(output_dir, f'page_{page_number + 1}.png')\n\n        # Calculate the crop coordinates\n        left = (pdf_image.width - width) // 2\n        top = crop_up\n        right = left + width\n        bottom = pdf_image.height - crop_down\n\n        # Save the image as a temporary file\n        temp_image_path = os.path.join(output_dir, f'temp_page_{page_number + 1}.png')\n        pdf_image.save(temp_image_path)\n\n        # Crop the temporary image and save the final cropped image\n        crop_image(temp_image_path, output_path, left, top, right, bottom)\n\n        # Clean up the temporary image\n        os.remove(temp_image_path)\n\nif __name__ == '__main__':\n    input_pdf = 'input.pdf'  # Specify the input PDF file\n    output_directory = 'output_images'  # Specify the output folder\n    crop_up = 177  # Change this to your desired crop for height up\n    crop_down = 118  # Change this to your desired crop for height down\n    target_width = 2667  # Change this to your desired width\n\n    extract_and_crop_pdf(input_pdf, output_directory, target_width, crop_up, crop_down)"
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html",
    "href": "blog/2023-05-29_markdown-structure/index.html",
    "title": "Markdown structure, titles and CSS",
    "section": "",
    "text": "Showing how titles and sections will be displayed in all posts\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Markdown\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                            \n                            \n                                Testing\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Markdown\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                                            \n                            \n                               \n                                Testing\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          May 29, 2023"
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html#posts",
    "href": "blog/2023-05-29_markdown-structure/index.html#posts",
    "title": "Markdown structure, titles and CSS",
    "section": "1. Posts",
    "text": "1. Posts\ndate-modified: last-modified\n\n# Bootstrap Icons\ncategories: [ &lt;i class='bi bi-archive'&gt;&lt;/i&gt; DevOps, TAGS, Python]                    \n\n# Material Icons\ncategories: [ \"&lt;i class='material-icons'&gt;account_circle&lt;/i&gt;\",  DevOps, TAGS, Python] \n\n# Hides post\ncoming-soon: true\n\n# Make numbered the sections\nnumber-sections: true\n\n# Until what numebr to show in TOC\ntoc-depth: 4"
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html#notebook",
    "href": "blog/2023-05-29_markdown-structure/index.html#notebook",
    "title": "Markdown structure, titles and CSS",
    "section": "2. Notebook",
    "text": "2. Notebook\n# Uses Bootstrap icons\nlinks:\n  - icon: download\n    name: Code\n    href: index.out.ipynb\n  - icon: file-earmark-pdf\n    name: See Article \n    url: https://www.researchgate.net/\n  - icon: file-slides-fill\n    name: Slides\n    url: https://www.google.com/\n  - icon: play-btn-fill\n    name: Video\n    url: https://www.google.com/\n\n# Creates downloadable icon below TOC\nformat:\n  ipynb: default"
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html#this-is-an-h2-title",
    "href": "blog/2023-05-29_markdown-structure/index.html#this-is-an-h2-title",
    "title": "Markdown structure, titles and CSS",
    "section": "1.1 This is an H2 title",
    "text": "1.1 This is an H2 title\n\n1.1.1 This is an H3 title\n\n1.1.1.1 This is an H4 title\nAbove we use the left option to specify items for the left side of the navigation bar. You can also use the right option to specify items for the right side.\nThe text for navigation bar items will be taken from the underlying target document’s title. Note that in the above example we provide a custom text: “Home” value for index.qmd.\nYou can also create a navigation bar menu by including a menu (which is a list of items much like left and right). For example:\nleft: - text: “More” menu: - talks.qmd - about.qmd\nHere are all of the options available for top navigation:\nOption Description title Navbar title (uses the site: title if none is specified). Use title: false to suppress the display of the title on the navbar. logo Logo image to be displayed left of the title. logo-alt Alternate text for the logo image. logo-href Target href from navbar logo / title. By default, the logo and title link to the root page of the site (/index.html). background Background color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color) foreground Foreground color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color). The foreground color will be used to color navigation elements, text and links that appear in the navbar. search Include a search box (true or false) tools List of navbar tools (e.g. link to github or twitter, etc.). See Navbar Tools for details. left / right Lists of navigation items for left and right side of navbar pinned Always show the navbar (true or false). Defaults to false, and uses headroom.js to automatically show the navbar when the user scrolls up on the page. collapse Collapse the navbar items into a hamburger menu when the display gets narrow (defaults to true) collapse-below Responsive breakpoint at which to collapse navbar items to a hamburger menu (“sm”, “md”, “lg”, “xl”, or “xxl”, defaults to “lg”) Here are the options available for individual navigation items:\nOption Description href Link to file contained with the project or external URL. text Text to display for navigation item (defaults to the document title if not provided). icon Name of one of the standard Bootstrap 5 icons (e.g. “github”, “twitter”, “share”, etc.). aria-label Accessible label for the navigation item. rel Value for rel attribute. Multiple space-separated values are permitted. menu List of navigation items to populate a drop-down menu.\n\n\n\n1.1.2 This is a title\nAbove we use the left option to specify items for the left side of the navigation bar. You can also use the right option to specify items for the right side.\nThe text for navigation bar items will be taken from the underlying target document’s title. Note that in the above example we provide a custom text: “Home” value for index.qmd.\nYou can also create a navigation bar menu by including a menu (which is a list of items much like left and right). For example:\nleft: - text: “More” menu:\n- talks.qmd\n\n- about.qmd\n\n1.1.2.1 This is a title\nHere are all of the options available for top navigation:\nOption Description\ntitle Navbar title (uses the site: title if none is specified). Use title: false to suppress the display of the title on the navbar. logo Logo image to be displayed left of the title. logo-alt Alternate text for the logo image. logo-href Target href from navbar logo / title. By default, the logo and title link to the root page of the site (/index.html). background Background color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color) foreground Foreground color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color). The foreground color will be used to color navigation elements, text and links that appear in the navbar. search Include a search box (true or false) tools List of navbar tools (e.g. link to github or twitter, etc.). See Navbar Tools for details. left / right Lists of navigation items for left and right side of navbar pinned Always show the navbar (true or false). Defaults to false, and uses headroom.js to automatically show the navbar when the user scrolls up on the page. collapse Collapse the navbar items into a hamburger menu when the display gets narrow (defaults to true) collapse-below Responsive breakpoint at which to collapse navbar items to a hamburger menu (“sm”, “md”, “lg”, “xl”, or “xxl”, defaults to “lg”) Here are the options available for individual navigation items:\nOption Description href Link to file contained with the project or external URL. text Text to display for navigation item (defaults to the document title if not provided). icon Name of one of the standard Bootstrap 5 icons (e.g. “github”, “twitter”, “share”, etc.). aria-label Accessible label for the navigation item. rel Value for rel attribute. Multiple space-separated values are permitted. menu List of navigation items to populate a drop-down menu."
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html#this-is-a-title-2",
    "href": "blog/2023-05-29_markdown-structure/index.html#this-is-a-title-2",
    "title": "Markdown structure, titles and CSS",
    "section": "1.2 This is a title",
    "text": "1.2 This is a title\nOption Description title Navbar title (uses the site: title if none is specified). Use title: false to suppress the display of the title on the navbar. logo Logo image to be displayed left of the title. logo-alt Alternate text for the logo image. logo-href Target href from navbar logo / title. By default, the logo and title link to the root page of the site (/index.html). background Background color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color) foreground Foreground color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color). The foreground color will be used to color navigation elements, text and links that appear in the navbar. search Include a search box (true or false) tools List of navbar tools (e.g. link to github or twitter, etc.). See Navbar Tools for details. left / right Lists of navigation items for left and right side of navbar pinned Always show the navbar (true or false). Defaults to false, and uses headroom.js to automatically show the navbar when the user scrolls up on the page. collapse Collapse the navbar items into a hamburger menu when the display gets narrow (defaults to true) collapse-below Responsive breakpoint at which to collapse navbar items to a hamburger menu (“sm”, “md”, “lg”, “xl”, or “xxl”, defaults to “lg”) Here are the options available for individual navigation items:\nOption Description href Link to file contained with the project or external URL. text Text to display for navigation item (defaults to the document title if not provided). icon Name of one of the standard Bootstrap 5 icons (e.g. “github”, “twitter”, “share”, etc.). aria-label Accessible label for the navigation item. rel Value for rel attribute. Multiple space-separated values are permitted. menu List of navigation items to populate a drop-down menu."
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html#this-is-a-title-5",
    "href": "blog/2023-05-29_markdown-structure/index.html#this-is-a-title-5",
    "title": "Markdown structure, titles and CSS",
    "section": "2.1 This is a title",
    "text": "2.1 This is a title\nAbove we use the left option to specify items for the left side of the navigation bar. You can also use the right option to specify items for the right side.\nThe text for navigation bar items will be taken from the underlying target document’s title. Note that in the above example we provide a custom text: “Home” value for index.qmd.\nYou can also create a navigation bar menu by including a menu (which is a list of items much like left and right). For example:\nleft: - text: “More” menu: - talks.qmd - about.qmd\nHere are all of the options available for top navigation:"
  },
  {
    "objectID": "blog/2023-05-29_markdown-structure/index.html#this-is-a-title-6",
    "href": "blog/2023-05-29_markdown-structure/index.html#this-is-a-title-6",
    "title": "Markdown structure, titles and CSS",
    "section": "2.2 This is a title",
    "text": "2.2 This is a title\n\n2.2.1 This is a title\nOption Description title Navbar title (uses the site: title if none is specified). Use title: false to suppress the display of the title on the navbar. logo Logo image to be displayed left of the title. logo-alt Alternate text for the logo image. logo-href Target href from navbar logo / title. By default, the logo and title link to the root page of the site (/index.html). background Background color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color) foreground Foreground color (“primary”, “secondary”, “success”, “danger”, “warning”, “info”, “light”, “dark”, or hex color). The foreground color will be used to color navigation elements, text and links that appear in the navbar. search Include a search box (true or false) tools List of navbar tools (e.g. link to github or twitter, etc.). See Navbar Tools for details. left / right Lists of navigation items for left and right side of navbar pinned Always show the navbar (true or false). Defaults to false, and uses headroom.js to automatically show the navbar when the user scrolls up on the page. collapse Collapse the navbar items into a hamburger menu when the display gets narrow (defaults to true) collapse-below Responsive breakpoint at which to collapse navbar items to a hamburger menu (“sm”, “md”, “lg”, “xl”, or “xxl”, defaults to “lg”) Here are the options available for individual navigation items:"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html",
    "href": "blog/2023-11-27_attention-&-transformers/index.html",
    "title": "Attention & Transformers",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Deep Learning\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 27, 2023"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#background-knowledge",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#background-knowledge",
    "title": "Attention & Transformers",
    "section": "1 Background knowledge",
    "text": "1 Background knowledge\n\n\n\n\n\n\n\n\n\n\n\nSlide 3"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#seq2seq-models",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#seq2seq-models",
    "title": "Attention & Transformers",
    "section": "2 Seq2seq models",
    "text": "2 Seq2seq models\n\n\n\n\nSlide 4"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#neural-machine-translation-with-a-seq2seq-model",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#neural-machine-translation-with-a-seq2seq-model",
    "title": "Attention & Transformers",
    "section": "3 Neural machine translation with a seq2seq model",
    "text": "3 Neural machine translation with a seq2seq model\n\n\n\n\nSlide 5"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#defining-seq2seq-for-nmt-encoder",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#defining-seq2seq-for-nmt-encoder",
    "title": "Attention & Transformers",
    "section": "4 Defining seq2seq for NMT”: Encoder",
    "text": "4 Defining seq2seq for NMT”: Encoder\n\n\n\n\nSlide 6\n\n\n\n\nh_{t-1} is the hidden state, is this what makes a NN like \\(f()\\) be recurrent"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#defining-seq2seg-for-nmt-decoder",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#defining-seq2seg-for-nmt-decoder",
    "title": "Attention & Transformers",
    "section": "5 Defining seq2seg for NMT: Decoder",
    "text": "5 Defining seq2seg for NMT: Decoder\n\n\n\n\nSlide 7"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#issue-of-seq2seq-models",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#issue-of-seq2seq-models",
    "title": "Attention & Transformers",
    "section": "6 Issue of seq2seq models",
    "text": "6 Issue of seq2seq models\n\n\n\n\nSlide 8\n\n\n\n\nThe problem is that they were trying to compress all info into a context vector \\(c\\)"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#attention",
    "title": "Attention & Transformers",
    "section": "7 Attention",
    "text": "7 Attention\n\n\n\n\nSlide 9"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#here-found-what-you-were",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#here-found-what-you-were",
    "title": "Attention & Transformers",
    "section": "8 Here, | found what you were",
    "text": "8 Here, | found what you were\n\n\n\n\nSlide 10"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#attention-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#attention-1",
    "title": "Attention & Transformers",
    "section": "9 Attention",
    "text": "9 Attention\n\n\n\n\nSlide 11"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#formal-definition-of-attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#formal-definition-of-attention",
    "title": "Attention & Transformers",
    "section": "10 Formal definition of Attention",
    "text": "10 Formal definition of Attention\n\n\n\n\nSlide 12"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#formal-definition-of-attention-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#formal-definition-of-attention-1",
    "title": "Attention & Transformers",
    "section": "11 Formal definition of Attention",
    "text": "11 Formal definition of Attention\n\n\n\n\nSlide 13\n\n\n\n\nIf the value e or the alignment model is high then it tells you how similar the inputs around position j and the outputs at position i match."
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#formal-definition-of-attention-2",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#formal-definition-of-attention-2",
    "title": "Attention & Transformers",
    "section": "12 Formal definition of Attention",
    "text": "12 Formal definition of Attention\n\n\n\n\nSlide 14"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#why-attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#why-attention",
    "title": "Attention & Transformers",
    "section": "13 Why attention?",
    "text": "13 Why attention?\n\n\n\n\nSlide 15"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#self-attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#self-attention",
    "title": "Attention & Transformers",
    "section": "14 Self-attention",
    "text": "14 Self-attention\n\n\n\n\nSlide 16"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#paying-attention-in-vision",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#paying-attention-in-vision",
    "title": "Attention & Transformers",
    "section": "15 Paying attention in vision",
    "text": "15 Paying attention in vision\n\n\n\n\nSlide 17"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#attention-is-all-you-need",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#attention-is-all-you-need",
    "title": "Attention & Transformers",
    "section": "16 Attention is all you need",
    "text": "16 Attention is all you need\n\n\n\n\nSlide 18"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#queries-keys-and-values",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#queries-keys-and-values",
    "title": "Attention & Transformers",
    "section": "17 Queries, keys and values",
    "text": "17 Queries, keys and values\n\n\n\n\nSlide 19"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#scaled-dot-product-attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#scaled-dot-product-attention",
    "title": "Attention & Transformers",
    "section": "18 Scaled Dot-Product Attention",
    "text": "18 Scaled Dot-Product Attention\n\n\n\n\nSlide 20"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#multi-head-attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#multi-head-attention",
    "title": "Attention & Transformers",
    "section": "19 Multi-head attention",
    "text": "19 Multi-head attention\n\n\n\n\nSlide 21"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#multi-head-self-attention",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#multi-head-self-attention",
    "title": "Attention & Transformers",
    "section": "20 Multi-head self-attention",
    "text": "20 Multi-head self-attention\n\n\n\n\nSlide 22"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#multi-head-self-attention-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#multi-head-self-attention-1",
    "title": "Attention & Transformers",
    "section": "21 Multi-head self-attention",
    "text": "21 Multi-head self-attention\n\n\n\n\nSlide 23"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#transformer-encoder",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#transformer-encoder",
    "title": "Attention & Transformers",
    "section": "22 Transformer encoder",
    "text": "22 Transformer encoder\n\n\n\n\nSlide 24"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#transformer-decoder",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#transformer-decoder",
    "title": "Attention & Transformers",
    "section": "23 Transformer decoder",
    "text": "23 Transformer decoder\n\n\n\n\nSlide 25\n\n\n\n\n\n\n\n\n\n\nWhy do we use a mask in the decoder of self attention?\n\n\n\n\n\n\n23.1 Autoregressive Property:\nAutoregressive models generate outputs one step at a time in a sequential manner. In the context of natural language processing, this means predicting the next word in a sequence given the preceding words. Autoregressive models are trained to predict the next token in the sequence based on the tokens that have already been generated.\n\n\n23.2 Decoder in a Transformer:\nThe decoder in a transformer is responsible for generating the output sequence. It consists of multiple layers, each containing a self-attention mechanism and feedforward neural networks. The self-attention mechanism allows the model to weigh different parts of the input sequence differently when generating each token.\n\n\n23.3 Cheating and the Mask:\n“Cheating” refers to the undesirable situation where the model uses information from future positions in the sequence during training. During training, the model is fed the true output sequence up to the current position to calculate loss and update its parameters. If the model were allowed to attend to future positions, it might artificially inflate its performance by relying on information that it wouldn’t have during actual generation.\nThe mask applied in the decoder’s self-attention mechanism prevents the model from accessing future information. The mask sets the attention scores for future positions to very small values, essentially blocking the model from attending to tokens that haven’t been generated yet. This ensures that the model learns to generate each token based only on the information available up to that point, aligning with the autoregressive nature of the decoding process.\n\n\n23.4 Example:\nConsider the task of language translation. When translating a sentence from English to French, the decoder generates the translation one word at a time. If it were allowed to attend to words in the future, it might incorrectly use information from the French translation that hasn’t been generated yet. This could lead to overfitting during training and poor generalization to unseen data.\nIn summary, preventing “cheating” by using a mask ensures that the decoder learns to generate outputs based on the information available up to the current step, improving the model’s ability to generalize to unseen data and maintain the autoregressive property essential for sequence generation tasks."
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#the-full-transformer",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#the-full-transformer",
    "title": "Attention & Transformers",
    "section": "24 The full Transformer",
    "text": "24 The full Transformer\n\n\n\n\nSlide 26"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#title",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#title",
    "title": "Attention & Transformers",
    "section": "25 Title",
    "text": "25 Title\n\n\n\n\nSlide 27"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#coding-a-transformer-pytorch-init",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#coding-a-transformer-pytorch-init",
    "title": "Attention & Transformers",
    "section": "26 Coding a Transformer (PyTorch): init",
    "text": "26 Coding a Transformer (PyTorch): init\n\n\n\n\nSlide 28"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#coding-a-transformer-pytorch-forward-pass",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#coding-a-transformer-pytorch-forward-pass",
    "title": "Attention & Transformers",
    "section": "27 Coding a Transformer (PyTorch): forward pass",
    "text": "27 Coding a Transformer (PyTorch): forward pass\n\n\n\n\nSlide 29\n\n\n\n\ntgt: in roder to predict the next word, you can look at the preceding word. So the target is the same as source except that is shofted one to the left. We do it so that it shofts and we are able to predict the last word"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#transformer-positional-encodings",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#transformer-positional-encodings",
    "title": "Attention & Transformers",
    "section": "28 Transformer: Positional encodings",
    "text": "28 Transformer: Positional encodings\n\n\n\n\nSlide 30\n\n\n\n\nAttention is a permutation-invariant operation, but this is not ideal because we might have that sometimes the order is important like with ‘not’\npositional enconding to locate where are you at the beginning or at the end"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#transformer-positional-encodings-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#transformer-positional-encodings-1",
    "title": "Attention & Transformers",
    "section": "29 Transformer: Positional encodings",
    "text": "29 Transformer: Positional encodings\n\n\n\n\nSlide 31\n\n\n\n\nif they are apart from each other their positional encoding should be different"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#coding-the-positional-encodings-pytorch",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#coding-the-positional-encodings-pytorch",
    "title": "Attention & Transformers",
    "section": "30 Coding the Positional Encodings (PyTorch)",
    "text": "30 Coding the Positional Encodings (PyTorch)\n\n\n\n\nSlide 32"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#pros-cons",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#pros-cons",
    "title": "Attention & Transformers",
    "section": "31 Pros & Cons",
    "text": "31 Pros & Cons\n\n\n\n\nSlide 33\n\n\n\n\nIt scales quadratically with the num inputs, the matrix is N * N let see an example:\nThe quadratic scaling of transformers with respect to the number of inputs primarily arises from the self-attention mechanism used in transformers. In self-attention, each element in the input sequence attends to all other elements, and the attention scores are computed pairwise. This leads to a quadratic dependency on the number of inputs.\nLet’s consider a simple example with a sequence of length \\(N\\). For simplicity, let’s assume each input element has a dimension of 1 for illustration purposes.\nWhat about other dimensions, wel that can be possible because remember we have our embeddings as the input to the NN, not the words itself\n\n\n\n\nLike in this picture our dimensions are clearly larger than 1 for the embeddings\n\nOriginal Sequence (1D): \\[\nx_1, x_2, x_3, \\ldots, x_N\n\\]\nSelf-Attention Weights: For each element \\(x_i\\), self-attention computes a weight for all other elements \\(x_j\\) based on their relationships. This results in a square matrix of attention weights:\n\\[\n\\begin{bmatrix}\nw_{1,1} & w_{1,2} & \\ldots & w_{1,N} \\\\\nw_{2,1} & w_{2,2} & \\ldots & w_{2,N} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nw_{N,1} & w_{N,2} & \\ldots & w_{N,N} \\\\\n\\end{bmatrix}\n  \\]\nEach entry \\(w_{i,j}\\) represents the attention weight between \\(x_i\\) and \\(x_j\\).\nOutput for Each Element: The output for each element \\(x_i\\) is computed as a weighted sum of all elements based on the attention weights:\n\\[\n\\text{output}_{i} = w_{i,1} \\cdot x_1 + w_{i,2} \\cdot x_2 + \\ldots + w_{i,N} \\cdot x_N\n  \\]\nThis involves \\(N\\) multiplications for each element.\nTotal Complexity: For \\(N\\) elements, we need to compute \\(N\\) attention weights for each element, resulting in a total of \\(N^2\\) attention weights. Therefore, the overall complexity is quadratic, \\(O(N^2)\\), due to the pairwise comparisons.\n\nThis quadratic scaling becomes computationally expensive as the sequence length increases, leading to challenges in handling long sequences efficiently. To address this, techniques like sparse attention patterns and approximations have been proposed in research to reduce the computational cost while maintaining the benefits of self-attention.\n\n\n\n\nSlide 34"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#recommended-papers",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#recommended-papers",
    "title": "Attention & Transformers",
    "section": "32 Recommended papers",
    "text": "32 Recommended papers\n\n\n\n\nSlide 35\n\n\n\n\n\n\n\n\nSlide 36\n\n\n\n\n\n\n\n\nSlide 37"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#bert",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#bert",
    "title": "Attention & Transformers",
    "section": "33 BERT",
    "text": "33 BERT\n\n\n\n\nSlide 38"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#bert-input-representation",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#bert-input-representation",
    "title": "Attention & Transformers",
    "section": "34 BERT input representation",
    "text": "34 BERT input representation\n\n\n\n\nSlide 39"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#bert-pre-training",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#bert-pre-training",
    "title": "Attention & Transformers",
    "section": "35 BERT pre-training",
    "text": "35 BERT pre-training\n\n\n\n\nSlide 40"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#bert-fine-tuning",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#bert-fine-tuning",
    "title": "Attention & Transformers",
    "section": "36 BERT fine-tuning",
    "text": "36 BERT fine-tuning\n\n\n\n\nSlide 41"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#bert-for-feature-extraction",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#bert-for-feature-extraction",
    "title": "Attention & Transformers",
    "section": "37 BERT for feature extraction",
    "text": "37 BERT for feature extraction\n\n\n\n\nSlide 42\n\n\n\n\nWith BERT we gained contextualized word embeddings"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#bertology",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#bertology",
    "title": "Attention & Transformers",
    "section": "38 BERTology",
    "text": "38 BERTology\n\n\n\n\nSlide 43"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#gpt-1-2-3-4",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#gpt-1-2-3-4",
    "title": "Attention & Transformers",
    "section": "39 GPT-{1, 2, 3, 4}",
    "text": "39 GPT-{1, 2, 3, 4}\n\n\n\n\nSlide 44\n\n\n\n\nWith bert you did not have a generative model, with GPT you can because it only relies on the past to predict the next ones. Bert mask the word in the middle, but sees at the right and left to see the context.\nYou dont need to have labels, because pred the next word is just looking in the corpus what is the actual word that should fit."
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#gpt-1-2-3",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#gpt-1-2-3",
    "title": "Attention & Transformers",
    "section": "40 GPT-{1, 2, 3}",
    "text": "40 GPT-{1, 2, 3}\n\n\n\n\nSlide 45"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#gpt-in--context-learning",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#gpt-in--context-learning",
    "title": "Attention & Transformers",
    "section": "41 GPT: In- context learning",
    "text": "41 GPT: In- context learning\nThe three settings we explore for in-context learning Traditional fine-tuning (not used for GPT-3)\n\n\n\n\nSlide 46\n\n\n\n\nThe ability to not train gradients is a cool ability that these hug models have\nWhat is in-context learning?\nIn natural language processing or conversation, understanding a word or phrase in context means considering the words or sentences that precede and follow it to grasp its intended meaning. This is important because the same word can have different meanings in different situations.\nIn the context of machine learning, especially with language models like GPT-3, providing information “in context” often involves supplying relevant details or context so that the model can generate responses or perform tasks that take into account the broader context of the input. This is particularly important for tasks that require understanding and generating coherent and contextually appropriate language."
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#discuss",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#discuss",
    "title": "Attention & Transformers",
    "section": "42 Discuss",
    "text": "42 Discuss\nFor models like StableDiffusion, Dalle, EMU video etc. T5\n\n\n\n\nSlide 47\n\n\n\n\nWhy may encoders models be favorable compared to decoder models?\n\nDecoders are also trained with masks but if you want to predict the next word, this representation looks at everything that comes before, so in a way if that is what you care about there is no mack really (because you are looking at everything that was looked before)\nNobody knows the answer for this question\nHypothesis is that encoders compress the information, while for Large language models, they are basically the job of encoding and decoding at the same time, because th closer you get to the ouput the more you need to go back to i.e correct grammar and very low level features, and somewhere in the middle of these decoder models there is the summary semantics that you could use for the vission models but you don’t know exactly where those features are. So for encoders you know exaclty where th summary is because that is still the bottleneck still but for decoders we dont know where to take the features\nA CNN is an encoder\nUnet also has this decoder then the decoder part like"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#gpt-vs-bert",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#gpt-vs-bert",
    "title": "Attention & Transformers",
    "section": "43 GPT vs BERT",
    "text": "43 GPT vs BERT\n\n\n\n\nSlide 48"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#multimodal-transformer-architecture-clip",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#multimodal-transformer-architecture-clip",
    "title": "Attention & Transformers",
    "section": "44 Multimodal Transformer architecture: CLIP",
    "text": "44 Multimodal Transformer architecture: CLIP\n\n\n\n\nSlide 49"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#multimodal-transformer-architecture-clip-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#multimodal-transformer-architecture-clip-1",
    "title": "Attention & Transformers",
    "section": "45 Multimodal Transformer architecture: CLIP",
    "text": "45 Multimodal Transformer architecture: CLIP\n\n\n\n\nSlide 50\n\n\n\n\nHere we want things to be close but different. That is hard, and with these hard examples we learn new features and learn more\nNow differentiating a dog vs a sheep that would be easier and eventually you will not learn anything.\n1:32"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#multimodal-transformer-architecture-clip-2",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#multimodal-transformer-architecture-clip-2",
    "title": "Attention & Transformers",
    "section": "46 Multimodal Transformer architecture: CLIP",
    "text": "46 Multimodal Transformer architecture: CLIP\n\n\n\n\nSlide 51\n\n\n\n\nBecause they use a text encoder, like BERT, they can do Zero-shot for classification images"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#clip-zero-shot-examples",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#clip-zero-shot-examples",
    "title": "Attention & Transformers",
    "section": "47 CLIP: Zero-Shot Examples",
    "text": "47 CLIP: Zero-Shot Examples\n\n\n\n\nSlide 52"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#clip-robustness",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#clip-robustness",
    "title": "Attention & Transformers",
    "section": "48 CLIP: Robustness",
    "text": "48 CLIP: Robustness\n\n\n\n\nSlide 53\n\n\n\n\n\nBetter because of the internet:\n\nCLIP is pre-trained on a large dataset with diverse images and associated text from the internet. This diverse pre-training data helps the model learn features that are more transferable across different tasks and domains. In contrast, a supervised ImageNet model might be optimized for the specific categories present in ImageNet, and its features may not generalize as well to new, unseen classes.\n\nBetter because we can guide it using engineered prompts:\n\nIn zero-shot learning with CLIP, you can provide textual prompts to guide the model’s behavior. This allows you to specify the task or class you’re interested in, enabling the model to adapt its predictions based on the provided textual information\n\nLess prone to overfitting due to have trained in larger dataset:\n\nSupervised models trained on specific datasets, such as ImageNet, may be prone to overfitting to the idiosyncrasies of that dataset. CLIP, having been trained on a broader range of data, may be less prone to overfitting to specific dataset biases\n\nMore data more understanding of semantics:\n\nCLIP’s strength lies in its ability to understand the semantic relationships between images and text. A larger dataset provides more examples of diverse language-image pairs, allowing the model to learn richer semantic embeddings"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#clip-usage-in-other-models",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#clip-usage-in-other-models",
    "title": "Attention & Transformers",
    "section": "49 CLIP: Usage in other models",
    "text": "49 CLIP: Usage in other models\n\n\n\n\nSlide 54"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#clip-shortcomings",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#clip-shortcomings",
    "title": "Attention & Transformers",
    "section": "50 CLIP: Shortcomings",
    "text": "50 CLIP: Shortcomings\n\n\n\n\nSlide 55\n\n\n\n\nCLIP does not have a decoder, so it cannot generate text"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#visual-language-model-flamingo",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#visual-language-model-flamingo",
    "title": "Attention & Transformers",
    "section": "51 Visual Language Model: Flamingo",
    "text": "51 Visual Language Model: Flamingo\n\n\n\n\nSlide 56\n\n\n\n\nBasically with CLIP you give images, out labels in form of a prompt text\nWith flamingo you give images and text prompts and can generate now the output cpation for an specific image\nGPT, you give it some text and is able to see what is next because it uses decoders"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#visual-language-model-flamingo-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#visual-language-model-flamingo-1",
    "title": "Attention & Transformers",
    "section": "52 Visual Language Model: Flamingo",
    "text": "52 Visual Language Model: Flamingo\n\n\n\n\nSlide 57\n\n\n\n\nThe language model is frozen, but you add this cross attention gates. So the cross atentions is sort of similar to the encoder decoder structure, when this language model can attend to the visual inputs. The pink is what is being learned. The visual encoder are also keept frozen.\nThe perciver part allows you to change the representation of the encoder"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#visual-language-model-flamingo-2",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#visual-language-model-flamingo-2",
    "title": "Attention & Transformers",
    "section": "53 Visual Language Model: Flamingo",
    "text": "53 Visual Language Model: Flamingo\n\n\n\n\nSlide 58\n\n\n\n\nHere there is an encoder and a decoder"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#vision-transformer",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#vision-transformer",
    "title": "Attention & Transformers",
    "section": "54 Vision Transformer",
    "text": "54 Vision Transformer\n\n\n\n\nSlide 59"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#understanding-a-figure-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#understanding-a-figure-1",
    "title": "Attention & Transformers",
    "section": "55 Understanding a “Figure 1”",
    "text": "55 Understanding a “Figure 1”\n\n\n\n\nSlide 60\n\n\n\n\nThis is similar to BERT, in bert we have positional embedding.\nHere we split the picture but still we conserve the order by remembering the index values which define the value\nBert process information in parallel, like in the paper see image below and why do we need positional embeding is because:\nBERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for natural language processing tasks. Unlike traditional sequential models, transformers process input data in parallel, which makes them highly efficient but also means they don’t inherently understand the order of the input sequence. To address this limitation and enable transformers to capture sequential information, positional embeddings are used.\nPositional embeddings are added to the input embeddings to provide information about the position of each token in a sequence. In BERT, the model processes the input tokens in parallel, and without positional embeddings, it would have no inherent understanding of the order of the tokens. Positional embeddings help the model distinguish between tokens based on their position in the sequence, allowing it to capture the sequential structure of the input.\nComing back to ViT model, here we can see that the model also process information in parallel that’s why we need positional embedding so that we can then learnthe order of how the pic was constructed"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#quiz-from-what-you-now-know-about-attention-what",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#quiz-from-what-you-now-know-about-attention-what",
    "title": "Attention & Transformers",
    "section": "56 Quiz: From what you now know about attention, what",
    "text": "56 Quiz: From what you now know about attention, what\n\n\n\n\nSlide 61\n\n\n\n\nBoth attention mechanisms and convolutions are essential components in neural network architectures, and they each have their advantages and disadvantages. Here’s a comparison of the two:\n\n56.1 Attention Mechanisms:\n\n56.1.1 Advantages:\n\nGlobal Context Handling: Attention mechanisms allow the model to focus on different parts of the input sequence when making predictions, enabling the model to consider global context and dependencies.\nVariable Receptive Field: Attention doesn’t enforce a fixed receptive field, meaning the model can attend to different parts of the input sequence with varying degrees of focus. This flexibility can be beneficial for tasks where capturing long-range dependencies is crucial.\nSequence-to-Sequence Tasks: Attention mechanisms have been particularly successful in sequence-to-sequence tasks, such as machine translation, where the input and output sequences can have varying lengths and alignments.\n\n\n\n56.1.2 Disadvantages:\n\nComputational Complexity: Attention mechanisms can be computationally expensive, especially with large sequences, as they require pairwise comparisons between all elements in the sequence.\nMemory Requirements: The model needs to store attention weights for each element in the sequence, leading to increased memory requirements.\n\n\n\n\n56.2 Convolutional Operations:\n\n56.2.1 Advantages:\n\nParameter Sharing: Convolutional layers use shared weights, which reduces the number of parameters in the model. This can make convolutional networks more computationally efficient and easier to train, especially on tasks with limited data.\nLocal Receptive Field: Convolutional layers have a fixed-size receptive field, allowing them to capture local patterns and spatial hierarchies efficiently.\nTranslation Invariance: Convolutional layers can provide some degree of translation invariance, meaning they can recognize patterns regardless of their exact position in the input.\n\n\n\n56.2.2 Disadvantages:\n\nLimited Global Context: Convolutional layers have a fixed receptive field, which may limit their ability to capture long-range dependencies in the data.\nNot Well-Suited for Sequence Tasks: While convolutional layers are effective for image-related tasks, they may not be as naturally suited for sequence-to-sequence tasks where the input and output lengths can vary.\n\nIn practice, a combination of both attention mechanisms and convolutional layers is often used in hybrid models to leverage the strengths of each. For example, the Transformer architecture combines self-attention mechanisms with feedforward layers, providing an effective approach for a variety of natural language processing tasks."
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#vision-transformer-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#vision-transformer-1",
    "title": "Attention & Transformers",
    "section": "57 Vision Transformer",
    "text": "57 Vision Transformer\n\n\n\n\nSlide 62"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#vision-transformer-2",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#vision-transformer-2",
    "title": "Attention & Transformers",
    "section": "58 Vision Transformer",
    "text": "58 Vision Transformer\n\n\n\n\nSlide 63\n\n\n\n\nIn ViT we actually learn the positional embeddings, compared to Bert,"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#attention-as-a-superset-of-convolutions",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#attention-as-a-superset-of-convolutions",
    "title": "Attention & Transformers",
    "section": "59 Attention as a superset of convolutions",
    "text": "59 Attention as a superset of convolutions\n\n\n\n\nSlide 64"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#training-a-vit-is-more-difficult",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#training-a-vit-is-more-difficult",
    "title": "Attention & Transformers",
    "section": "60 Training a ViT is more difficult",
    "text": "60 Training a ViT is more difficult\n\n\n\n\nSlide 65"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#vil-features",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#vil-features",
    "title": "Attention & Transformers",
    "section": "61 Vil features",
    "text": "61 Vil features\n\n\n\n\nSlide 66"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#also-here-imagenet-can-more-or-less-be-solved-with-textures",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#also-here-imagenet-can-more-or-less-be-solved-with-textures",
    "title": "Attention & Transformers",
    "section": "62 Also here: ImageNet can (more or less) be solved with textures",
    "text": "62 Also here: ImageNet can (more or less) be solved with textures\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#swin-transformer-add-hierarchy-back-in",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#swin-transformer-add-hierarchy-back-in",
    "title": "Attention & Transformers",
    "section": "63 Swin Transformer: add hierarchy back in?",
    "text": "63 Swin Transformer: add hierarchy back in?\n\n\n\n\nSlide 68"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#hybrid-architectures-get-best-performances-atm",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#hybrid-architectures-get-best-performances-atm",
    "title": "Attention & Transformers",
    "section": "64 Hybrid Architectures get best performances (atm)",
    "text": "64 Hybrid Architectures get best performances (atm)\n\n\n\n\nSlide 69"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#the-perceiver",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#the-perceiver",
    "title": "Attention & Transformers",
    "section": "65 The Perceiver",
    "text": "65 The Perceiver\n\n\n\n\nSlide 70"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#the-perceiver-main-idea",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#the-perceiver-main-idea",
    "title": "Attention & Transformers",
    "section": "66 The Perceiver: main idea",
    "text": "66 The Perceiver: main idea\n\n\n\n\nSlide 71"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#the-perceiver-taming-quadratic-complexity",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#the-perceiver-taming-quadratic-complexity",
    "title": "Attention & Transformers",
    "section": "67 The Perceiver: Taming quadratic complexity",
    "text": "67 The Perceiver: Taming quadratic complexity\n\n\n\n\nSlide 72"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#title-1",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#title-1",
    "title": "Attention & Transformers",
    "section": "68 Title",
    "text": "68 Title\n\n\n\n\nSlide 73"
  },
  {
    "objectID": "blog/2023-11-27_attention-&-transformers/index.html#notes-on-weight-sharing-for-cnn",
    "href": "blog/2023-11-27_attention-&-transformers/index.html#notes-on-weight-sharing-for-cnn",
    "title": "Attention & Transformers",
    "section": "69 Notes on weight sharing for CNN",
    "text": "69 Notes on weight sharing for CNN\nA convolutional layer is generally comprised of many “filters”, which are usually 2x2 or 3x3. These filters are applied in a “sliding window” across the entire layer’s input. The “weight sharing” is using fixed weights for this filter across the entire input. It does not mean that all of the filters are equivalent.\nTo be concrete, let’s imagine a 2x2 filter 𝐹 striding a 3x3 input 𝑋 with padding, so the filter gets applied 4 times. Let’s denote the unrolled filter 𝛽\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n  x_{11} & x_{21} & x_{31} \\\\\n  x_{12} & x_{22} & x_{32} \\\\\n  x_{13} & x_{23} & x_{33} \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{F} = \\begin{bmatrix}\n  w_{11} & w_{21} \\\\\n  w_{12} & w_{22} \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\boldsymbol{\\beta} = \\begin{bmatrix}\n  w_{11} & w_{12} & w_{21} & w_{22} \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{F} \\cdot \\boldsymbol{\\beta} = \\begin{bmatrix}\n  \\beta \\cdot [x_{11}, x_{12}, x_{21}, x_{22}] & \\beta \\cdot [x_{12}, x_{13}, x_{22}, x_{23}] \\\\\n  \\beta \\cdot [x_{21}, x_{22}, x_{31}, x_{32}] & \\beta \\cdot [x_{22}, x_{23}, x_{32}, x_{33}]\n\\end{bmatrix}\n\\]\n“Weight sharing” means when we apply this 2x2 filter to our 3x3 input, we reuse the same four weights given by the filter across the entire input. The alternative would be each filter application having its own set of inputs (which would really be a separate filter for each region of the image), giving a total of 16 weights, or a dense layer with 4 nodes giving 36 weights.\nSharing weights in this way significantly reduces the number of weights we have to learn, making it easier to learn very deep architectures, and additionally allows us to learn features that are agnostic to what region of the input is being considered."
  },
  {
    "objectID": "blog/2023-11-02_language-modelling/index.html",
    "href": "blog/2023-11-02_language-modelling/index.html",
    "title": "Language modelling",
    "section": "",
    "text": "Description of this Post\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                NLP\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                NLP\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          November 2, 2023"
  },
  {
    "objectID": "blog/2023-11-02_language-modelling/index.html#example-of-categorical-distribution",
    "href": "blog/2023-11-02_language-modelling/index.html#example-of-categorical-distribution",
    "title": "Language modelling",
    "section": "1 Example of Categorical Distribution",
    "text": "1 Example of Categorical Distribution\n\nVocabulary:\nV=3, {dog, cat,bird}\nNum Observations:\nN=9\nCategories:\nC=3, {negative, neutral, positive}\nDataset:\nD={(class1, word1)…}\n\n\n1.1 Formula for parameter estimation\n\n\n\n\n\n\n\n\nHere the denominator means to count all pairs that have the same class\n\nTabular Representation:\n\n\\[\nW|C=c \\, \\textasciitilde \\, \\text{ Categorical}(\\theta_{1:V}^{(c)})\n\\]\n\n\n\n\n\nThe dataset:\n\n\n\n\nMaximum Likelihood Estimates:"
  },
  {
    "objectID": "blog/2023-11-02_language-modelling/index.html#smoothing",
    "href": "blog/2023-11-02_language-modelling/index.html#smoothing",
    "title": "Language modelling",
    "section": "2 Smoothing",
    "text": "2 Smoothing\n\n\n\n\nFor instance if I have to compute the prob for \\(\\theta_{bird}^{neg}=\\frac{0}{3}\\) then because I will have a zero probability then I can use only the count plus some constant (smoothing)so that the probability estimate does not become \\(0\\)\nFor instance:\n\n\n\n\nHere 0.1 is the smoothing constant\n\nSo if not smoothing then use probs, if use smoothing the we use the counts\n\n\n\n\n\nSlide 1\n\n\n\n\n\n\n\n\nSlide 2\n\n\n\n\n\n\n\n\nSlide 3\n\n\n\n\n\n\n\n\nSlide 4\n\n\n\n\n\n\n\n\nSlide 5\n\n\n\n\n\n\n\n\nSlide 6\n\n\n\n\n\n\n\n\nSlide 7\n\n\n\n\n\n\n\n\nSlide 8\n\n\n\n\n\n\n\n\nSlide 9\n\n\n\n\n\n\n\n\nSlide 10\n\n\n\n\n\n\n\n\nSlide 11\n\n\n\n\n\n\n\n\nSlide 12\n\n\n\n\n\n\n\n\nSlide 13\n\n\n\n\n\n\n\n\nSlide 14\n\n\n\n\n\n\n\n\nSlide 15\n\n\n\n\n\n\n\n\nSlide 16\n\n\n\n\n\n\n\n\nSlide 17\n\n\n\n\n\n\n\n\nSlide 18\n\n\n\n\n\n\n\n\nSlide 19\n\n\n\n\n\n\n\n\nSlide 20\n\n\n\n\n\n\n\n\nSlide 21\n\n\n\n\n\n\n\n\nSlide 22\n\n\n\n\n\n\n\n\nSlide 23\n\n\n\n\n\n\n\n\nSlide 24\n\n\n\n\n\n\n\n\nSlide 25\n\n\n\n\n\n\n\n\nSlide 26\n\n\n\n\n\n\n\n\nSlide 27\n\n\n\n\n\n\n\n\nSlide 28\n\n\n\n\n\n\n\n\nSlide 29\n\n\n\n\n\n\n\n\nSlide 30\n\n\n\n\n\n\n\n\nSlide 31\n\n\n\n\n\n\n\n\nSlide 32\n\n\n\n\n\n\n\n\nSlide 33\n\n\n\n\n\n\n\n\nSlide 34\n\n\n\n\n\n\n\n\nSlide 35\n\n\n\n\n\n\n\n\nSlide 36\n\n\n\n\n\n\n\n\nSlide 37\n\n\n\n\n\n\n\n\nSlide 38\n\n\n\n\n\n\n\n\nSlide 39\n\n\n\n\n\n\n\n\nSlide 40\n\n\n\n\n\n\n\n\nSlide 41\n\n\n\n\n\n\n\n\nSlide 42\n\n\n\n\n\n\n\n\nSlide 43\n\n\n\n\n\n\n\n\nSlide 44\n\n\n\n\n\n\n\n\nSlide 45\n\n\n\n\n\n\n\n\nSlide 46\n\n\n\n\n\n\n\n\nSlide 47\n\n\n\n\n\n\n\n\nSlide 48\n\n\n\n\n\n\n\n\nSlide 49\n\n\n\n\n\n\n\n\nSlide 50\n\n\n\n\n\n\n\n\nSlide 51\n\n\n\n\n\n\n\n\nSlide 52\n\n\n\n\n\n\n\n\nSlide 53\n\n\n\n\n\n\n\n\nSlide 54\n\n\n\n\n\n\n\n\nSlide 55\n\n\n\n\n\n\n\n\nSlide 56\n\n\n\n\n\n\n\n\nSlide 57\n\n\n\n\n\n\n\n\nSlide 58\n\n\n\n\n\n\n\n\nSlide 59\n\n\n\n\n\n\n\n\nSlide 60\n\n\n\n\n\n\n\n\nSlide 61\n\n\n\n\n\n\n\n\nSlide 62\n\n\n\n\n\n\n\n\nSlide 63\n\n\n\n\n\n\n\n\nSlide 64\n\n\n\n\n\n\n\n\nSlide 65\n\n\n\n\n\n\n\n\nSlide 66\n\n\n\n\n\n\n\n\nSlide 67\n\n\n\n\n\n\n\n\nSlide 68\n\n\n\n\n\n\n\n\nSlide 69\n\n\n\n\n\n\n\n\nSlide 70\n\n\n\n\n\n\n\n\nSlide 71\n\n\n\n\n\n\n\n\nSlide 72\n\n\n\n\n\n\n\n\nSlide 73\n\n\n\n\n\n\n\n\nSlide 74\n\n\n\n\n\n\n\n\nSlide 75\n\n\n\n\n\n\n\n\nSlide 76"
  },
  {
    "objectID": "sites/index.html",
    "href": "sites/index.html",
    "title": "My Sites",
    "section": "",
    "text": "My Sites\n\n\n\n\n\n\nMain section where I keep track of important news about my professional carrier.\n\n\nHOME\n\n\n\n\n\n\n\n\nAbout me, my interests and experience, and my work as a AI Research Engineer.\n\n\nABOUT\n\n\n\n\n\n\n\n\nI sometimes write about what I’m doing and learning, mostly about CS and building websites.\n\n\nBLOG\n\n\n\n\n\n\n\n\nI have worked on big-scale projects. Here are links to their repos, websites and any other resources.\n\n\nPROJECTS\n\n\n\n\n\n\n\n\nSlides and resources from talks I’ve given for educational purposes or conferences.\n\n\nSLIDES\n\n\n\n\n\n\n\n\n\nWhat I have read and I am reading at the moment. Recomendations? I’d love to hear from you!\n\n\nBOOKS"
  },
  {
    "objectID": "notes/2023-09-03_create-website-using-local/index.html",
    "href": "notes/2023-09-03_create-website-using-local/index.html",
    "title": "Create Website using Local",
    "section": "",
    "text": "Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          September 3, 2023"
  },
  {
    "objectID": "notes/2023-09-03_create-website-using-local/index.html#setting-up-the-terminal",
    "href": "notes/2023-09-03_create-website-using-local/index.html#setting-up-the-terminal",
    "title": "Create Website using Local",
    "section": "1 Setting up the Terminal",
    "text": "1 Setting up the Terminal\nFirst you need to have an idea to control the terminal. Look at the following link\n\nEnvironments\n\n\nCreate an environment called web with python\nconda create -n web python=3.7\nTo activate the new environment\nconda activate web\nTo move terminal to website folder in Desktop:\ncd /Users/datoapanta/Desktop/website"
  },
  {
    "objectID": "notes/2023-09-03_create-website-using-local/index.html#setting-up-next.js",
    "href": "notes/2023-09-03_create-website-using-local/index.html#setting-up-next.js",
    "title": "Create Website using Local",
    "section": "2 Setting up Next.js",
    "text": "2 Setting up Next.js\n\nInstallation Guide\n\nTo kills a port\nnpx kill-port 3000"
  },
  {
    "objectID": "notes/2023-10-09_numpy-arrays/index.html",
    "href": "notes/2023-10-09_numpy-arrays/index.html",
    "title": "Numpy Arrays",
    "section": "",
    "text": "Numpy Arrays\n        \n        \n                    \n                \n                    Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 9, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n# To create 1D array (5,) aka list\narray = [1, 2, 3]\n\n# To create 2D array (5,1)\narray = [[9],[9],[9],[9],[9]]\n\n# To create 2D array (1,5)\narray = [[9, 9, 9, 9, 9]]\n\n\nCode\nimport numpy as np\narray_2d = np.array([[9, 9, 9, 9, 9]])\narray_1d = np.array([1,1,1,1,1])\nprint(array_2d + array_1d)\nprint((array_2d+array_1d).shape)\n\n\n[[10 10 10 10 10]]\n(1, 5)\n\n\n\n2D-array of (1,5) \\(+\\) 1D-array aka list of (5,):\n\n2D-array of (1,5)\n\n\n\n\nCode\nimport numpy as np\narray_2d = np.array([[1], [2], [3], [4], [5]])\n\n# Create a 1D array of shape (5,)\narray_1d = np.array([10, 20, 30, 40, 50])\n\n# Add the 2D array and 1D array\nresult = array_2d + array_1d\n\nprint(result)\nprint(result.shape)\n\n\n[[11 21 31 41 51]\n [12 22 32 42 52]\n [13 23 33 43 53]\n [14 24 34 44 54]\n [15 25 35 45 55]]\n(5, 5)\n\n\n\n2D-array of (5,1) \\(+\\) 1D-array aka list of (5,):\n\n2D-array of (5,5) this is due to broadcasting see fig below"
  },
  {
    "objectID": "notes/2023-08-27_changes-to-vanilla-quarto/index.html",
    "href": "notes/2023-08-27_changes-to-vanilla-quarto/index.html",
    "title": "Changes to Vanilla Quarto",
    "section": "",
    "text": "Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 27, 2023"
  },
  {
    "objectID": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#find-styles-code-highlighting",
    "href": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#find-styles-code-highlighting",
    "title": "Changes to Vanilla Quarto",
    "section": "1 Find Styles Code Highlighting",
    "text": "1 Find Styles Code Highlighting\n\nPandoc highlight styles: /Applications/quarto/share/pandoc/highlight-styles"
  },
  {
    "objectID": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#changes-the-moon-icon",
    "href": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#changes-the-moon-icon",
    "title": "Changes to Vanilla Quarto",
    "section": "2 Changes the moon icon",
    "text": "2 Changes the moon icon\n\nChange toggle-icons: /Applications/quarto/share/formats/html/bootstrap/\\_bootstrap-rules.scss\n/Users/datoapanta/Desktop/danilotpnta.github.io/docs/site_libs/bootstrap/bootstrap-dark.min.css\nI change the SVG from this site: https://icons.getbootstrap.com/icons/moon/\n\n\n\nCopy this\n\n// .navbar .quarto-color-scheme-toggle:not(.alternate) .bi::before {\n//   background-image: url('data:image/svg+xml,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"#{colorToRGBA($navbar-light-color)}\" class=\"bi bi-toggle-off\" viewBox=\"0 0 16 16\"&gt;&lt;path d=\"M11 4a4 4 0 0 1 0 8H8a4.992 4.992 0 0 0 2-4 4.992 4.992 0 0 0-2-4h3zm-6 8a4 4 0 1 1 0-8 4 4 0 0 1 0 8zM0 8a5 5 0 0 0 5 5h6a5 5 0 0 0 0-10H5a5 5 0 0 0-5 5z\"/&gt;&lt;/svg&gt;');\n// }\n\n// Toggle MOON\n.navbar .quarto-color-scheme-toggle:not(.alternate) .bi::before {\n  background-image: url('data:image/svg+xml,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"#{colorToRGBA($navbar-light-color)}\" class=\"bi bi-toggle-off\" viewBox=\"0 0 16 16\"&gt;&lt;path d=\"M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278zM4.858 1.311A7.269 7.269 0 0 0 1.025 7.71c0 4.02 3.279 7.276 7.319 7.276a7.316 7.316 0 0 0 5.205-2.162c-.337.042-.68.063-1.029.063-4.61 0-8.343-3.714-8.343-8.29 0-1.167.242-2.278.681-3.286z\"/&gt;&lt;/svg&gt;');\n}\n\n// Toggle MOON filled\n// .navbar .quarto-color-scheme-toggle.alternate .bi::before {\n//   background-image: url('data:image/svg+xml,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"#{colorToRGBA($navbar-light-color)}\" class=\"bi bi-toggle-on\" viewBox=\"0 0 16 16\"&gt;&lt;path d=\"M5 3a5 5 0 0 0 0 10h6a5 5 0 0 0 0-10H5zm6 9a4 4 0 1 1 0-8 4 4 0 0 1 0 8z\"/&gt;&lt;/svg&gt;');\n// }\n\n.navbar .quarto-color-scheme-toggle.alternate .bi::before {\n  background-image: url('data:image/svg+xml,&lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"#{colorToRGBA($navbar-light-color)}\" class=\"bi bi-toggle-on\" viewBox=\"0 0 16 16\"&gt;&lt;path d=\"M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z\"/&gt;&lt;/svg&gt;');\n}"
  },
  {
    "objectID": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#changes-title-of-categories-to-all-categories",
    "href": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#changes-title-of-categories-to-all-categories",
    "title": "Changes to Vanilla Quarto",
    "section": "3 Changes title of “Categories” to “All Categories”",
    "text": "3 Changes title of “Categories” to “All Categories”\n\n/Applications/quarto/bin/quarto.js\n\nCopy this:\n// headingEl.innerText = localizedString(format, kListingPageFieldCategories);\nheadingEl.innerText = \"All Categories\";"
  },
  {
    "objectID": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#remove-the-all-from-categories-sidebar",
    "href": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#remove-the-all-from-categories-sidebar",
    "title": "Changes to Vanilla Quarto",
    "section": "4 Remove the “All” from “Categories” Sidebar",
    "text": "4 Remove the “All” from “Categories” Sidebar\n\n/Applications/quarto/bin/quarto.js\n\nCopy this:\nconst allCategory = localizedString(format, kListingPageCategoryAll);\n// const allEl = categoryElement(doc, allCategory, formatFn(allCategory, totalCategories), \"\");"
  },
  {
    "objectID": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#adds-extra-feature-to-the-headeroffset-function",
    "href": "notes/2023-08-27_changes-to-vanilla-quarto/index.html#adds-extra-feature-to-the-headeroffset-function",
    "title": "Changes to Vanilla Quarto",
    "section": "5 Adds extra feature to the headerOffset function",
    "text": "5 Adds extra feature to the headerOffset function\n\nIf the header is absolute then the header.height will be zero to fix the TOC\n/Applications/quarto/share/projects/website/navigation/quarto-nav.js\n\n\n\nCopy this\n\n// function headerOffset() {\n//   // Set an offset if there is are fixed top navbar\n//   const headerEl = window.document.querySelector(\"header.fixed-top\");\n//   if (headerEl) {\n//     return headerEl.clientHeight;\n//   } else {\n//     return 0;\n//   }\n// }\n\nfunction headerOffset() {\n  // Set an offset if there is are fixed top navbar\n  const headerEl = window.document.querySelector(\"header.fixed-top\");\n  if (headerEl) {\n    // If the page is a blog post then return the height as 0\n    const blogSection = window.document.querySelector(\"header.blog-page\");\n    if (blogSection) {\n      // Add extra padding to display well the navbar\n      document.getElementById(\n        \"quarto-content\"\n      ).style.paddingTop = `${headerEl.clientHeight}px`;\n      // returns 0 to fix the TOC where it displays the section\n      return 0;\n    } else {\n      return headerEl.clientHeight;\n    }\n  } else {\n    return 0;\n  }\n}"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html",
    "title": "Features, tools & improvements website",
    "section": "",
    "text": "Description of this Note\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 27, 2023"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#change-directories",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#change-directories",
    "title": "Features, tools & improvements website",
    "section": "1 Change directories:",
    "text": "1 Change directories:\n_quarto.yml http://localhost:4200/dev/ include_footer.js: http://localhost:4200/links.js"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#include-a-page-on-how-to-use-the-command-line",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#include-a-page-on-how-to-use-the-command-line",
    "title": "Features, tools & improvements website",
    "section": "2 Include a page on how to use the command line:",
    "text": "2 Include a page on how to use the command line:\nhttps://www.taniarascia.com/how-to-use-the-command-line-for-apple-macos-and-linux/"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#read-how-to-use-node.js",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#read-how-to-use-node.js",
    "title": "Features, tools & improvements website",
    "section": "3 Read how to use Node.js:",
    "text": "3 Read how to use Node.js:\nhttps://www.taniarascia.com/how-to-install-and-use-node-js-and-npm-mac-and-windows/"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#firebase-real-time-database-for-my-static-website",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#firebase-real-time-database-for-my-static-website",
    "title": "Features, tools & improvements website",
    "section": "4 FIREBASE REAL TIME DATABASE FOR MY STATIC WEBSITE",
    "text": "4 FIREBASE REAL TIME DATABASE FOR MY STATIC WEBSITE\nhttps://xyzcoder.github.io/firebase/2019/03/17/firebase-real-time-database.html https://stackoverflow.com/questions/46574537/how-to-set-up-cloud-firestore-for-static-hosted-website https://maxbarry.medium.com/how-i-used-google-drive-and-firebase-to-give-my-static-site-a-cms-7226e01a51b5\n[To create a like button]- https://mazipan.space/en/create-simple-like-button-using-firebase-rtdb"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#copy-the-about-page-of",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#copy-the-about-page-of",
    "title": "Features, tools & improvements website",
    "section": "5 Copy the about page of:",
    "text": "5 Copy the about page of:\nhttps://beamilz.com/about.html"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#copy-his-cv",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#copy-his-cv",
    "title": "Features, tools & improvements website",
    "section": "6 Copy his CV:",
    "text": "6 Copy his CV:\nhttps://slama.dev/cv.pdf"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#review-the-subscribe-letter",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#review-the-subscribe-letter",
    "title": "Features, tools & improvements website",
    "section": "7 Review the Subscribe letter:",
    "text": "7 Review the Subscribe letter:\nhttps://taniarascia.substack.com/archive"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#html-js-css-container",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#html-js-css-container",
    "title": "Features, tools & improvements website",
    "section": "8 HTML, JS, CSS container",
    "text": "8 HTML, JS, CSS container\nhttps://jsbin.com/zusihologo/edit?html,css,output"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#select-sizes",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#select-sizes",
    "title": "Features, tools & improvements website",
    "section": "9 Select sizes:",
    "text": "9 Select sizes:\nhttps://jsbin.com/zusihologo/edit?html,css,"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#this-is-how-you-create-small-slides",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#this-is-how-you-create-small-slides",
    "title": "Features, tools & improvements website",
    "section": "10 This is how you create small slides:",
    "text": "10 This is how you create small slides:\nSLIDES FORMAT: RMD!!! https://github.com/djnavarro/slides-arrow-latinr-2022/blob/main/index.qmd\n\nSlides: https://slides.com/danilotoapanta\nCheck Codebox Sandbox and also embeeding html with Stackbitz\n\n\n\n\nLook into Google SEO to make page appear\nI like this blog: https://blog.meain.io/2023/releasing-scopeline-el/\nfix the content bar make it like her: https://ellakaye.co.uk/"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#javacript-to-counter",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#javacript-to-counter",
    "title": "Features, tools & improvements website",
    "section": "11 Javacript to counter:",
    "text": "11 Javacript to counter:\n/Users/datoapanta/Desktop/garrickadenbuie-com/_quarto.yml\nhttps://counter.dev/dashboard.html?demo=1\n– Fix the monospace"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#make-related-posts-like-this",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#make-related-posts-like-this",
    "title": "Features, tools & improvements website",
    "section": "12 Make related posts like this:",
    "text": "12 Make related posts like this:\n\nhttps://mattorb.com/swift-conciseness-and-trade-offs/\nhttps://github.blog/2023-08-04-a-checklist-and-guide-to-get-your-repository-collaboration-ready/#:~:text=about%20other%C2%A0plans%3F-,Related%20posts,-Engineering"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#add-tree-boxes-projects",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#add-tree-boxes-projects",
    "title": "Features, tools & improvements website",
    "section": "13 Add tree boxes projects:",
    "text": "13 Add tree boxes projects:\n\nViews\nTotal Projects\nMost clicked category"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#install-analytics",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#install-analytics",
    "title": "Features, tools & improvements website",
    "section": "14 Install Analytics:",
    "text": "14 Install Analytics:\n\nhttps://stats.arp242.net/?hl-period=year&period-start=2022-08-08&period-end=2023-08-08&filter=&daily=off"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#create-this-website",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#create-this-website",
    "title": "Features, tools & improvements website",
    "section": "15 Create this website",
    "text": "15 Create this website\nhttps://note.nkmk.me/en/python-os-mkdir-makedirs/"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#make-use-of-jupyter-widgets",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#make-use-of-jupyter-widgets",
    "title": "Features, tools & improvements website",
    "section": "16 Make use of jupyter widgets",
    "text": "16 Make use of jupyter widgets"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#copy-this-for-the-button-of-widgets",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#copy-this-for-the-button-of-widgets",
    "title": "Features, tools & improvements website",
    "section": "17 Copy this for the button of widgets:",
    "text": "17 Copy this for the button of widgets:\nhttps://wiki.manjaro.org/index.php?title=Main_Page/nl&action=history"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#encrypt-pages-with-password",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#encrypt-pages-with-password",
    "title": "Features, tools & improvements website",
    "section": "18 Encrypt pages with password",
    "text": "18 Encrypt pages with password\nhttps://robinmoisson.github.io/staticrypt/"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#implement-further-reading-like-this",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#implement-further-reading-like-this",
    "title": "Features, tools & improvements website",
    "section": "19 Implement further reading like this:",
    "text": "19 Implement further reading like this:\nhttps://engineeringfordatascience.com/posts/how_to_use_allure_pytest_bdd_and_allure_pytest_in_the_same_project/#further-reading"
  },
  {
    "objectID": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#make-use-of-ai-voice-to-read-my-poems",
    "href": "notes/2023-08-27_features-tools-&-improvements-website-/index.html#make-use-of-ai-voice-to-read-my-poems",
    "title": "Features, tools & improvements website",
    "section": "20 Make use of AI voice to read my poems",
    "text": "20 Make use of AI voice to read my poems"
  },
  {
    "objectID": "notes/2023-10-06_how-to-embed-ml-application/index.html",
    "href": "notes/2023-10-06_how-to-embed-ml-application/index.html",
    "title": "How to embed ML application?",
    "section": "",
    "text": "Example how to add ml application from gradio.app\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          October 6, 2023"
  },
  {
    "objectID": "notes/2023-10-06_how-to-embed-ml-application/index.html#adding-the-script",
    "href": "notes/2023-10-06_how-to-embed-ml-application/index.html#adding-the-script",
    "title": "How to embed ML application?",
    "section": "1 Adding the script",
    "text": "1 Adding the script\n&lt;script type=\"module\"\nsrc=\"https://gradio.s3-us-west-2.amazonaws.com/3.36.1/gradio.js\"&gt;\n&lt;/script&gt;\n&lt;gradio-app space=\"ForBo7/FloodDetector\"&gt;&lt;/gradio-app&gt;"
  },
  {
    "objectID": "notes/2023-10-06_how-to-embed-ml-application/index.html#styling",
    "href": "notes/2023-10-06_how-to-embed-ml-application/index.html#styling",
    "title": "How to embed ML application?",
    "section": "2 Styling",
    "text": "2 Styling\nTo avoid lines surrounding, add to theme-light.scss\n.info.svelte-1kyws56.svelte-1kyws56 {\n    display: none !important;\n}\n\n.embed-container.svelte-1kyws56.svelte-1kyws56 {\n    border: none !important;\n}"
  },
  {
    "objectID": "notes/2023-08-22_commands-for-quarto/index.html",
    "href": "notes/2023-08-22_commands-for-quarto/index.html",
    "title": "Commands Quarto",
    "section": "",
    "text": "Commands Quarto\n        \n        \n                    \n                \n                    A useful list of Quarto commands\n                \n            \n        \n        \n                    \n                \n                \n                    \n\n                                            \n                            \n                                All\n                            \n                        \n                                            \n                            \n                                Quarto\n                            \n                        \n                                            \n                            \n                                TAGS\n                            \n                        \n                                    \n                \n\n\n                \n                    \n                    \n                    \n                        \n                        \n                        \n                        \n                    \n\n                    \n                                            \n                            \n                               \n                                All\n                            \n                        \n                                            \n                            \n                               \n                                Quarto\n                            \n                        \n                                            \n                            \n                               \n                                TAGS\n                            \n                        \n                    \n                    \n                \n\n                    \n    \n\n\n    \n    \n\n        \n        Author\n        \n                 Danilo Toapanta \n              \n      \n        \n        \n        Published\n        \n          August 22, 2023\n        \n      \n      \n        \n      \n      \n\n    \n    \n\n\n# Creates website in folder 'mysite'\nquarto create-project /Users/datoapanta/Desktop/mysite --type website  \n\n# To render it and preview in local host\nquarto preview mysite\n\n# Creates blog in folder 'blog'\nquarto create-project /Users/datoapanta/Desktop/blog --type website:blog\n\n# To render it and preview in local host\nquarto preview blog"
  },
  {
    "objectID": "now/index.html",
    "href": "now/index.html",
    "title": "Danilo Toapanta",
    "section": "",
    "text": "What I’m working on right now\n\n\nUpdated on August 16th, 2023.\n\n\n    \n\n\nCore Habits\n\nRead ≧ 20 min\nExercise ≧ 1 hour\nMeditate ≧ 10 minutes\nRun weekly ≧ 18km\n\n\n\nInternship\n\nConnect Hugging Face ML application to repo\nYAML automation for downloading JPEG\n\n\n\nWebsite\n\nFinishing up the project page.\nFuture features can be found at DEV\n\n\n\nR Studio\n\nLearning file management\n\n\n\nReading\n\nAtomic Habits by James Clear\n\n—\n\nBooks I’ve enjoy reading\n\n\n\nArchive \n\n\n\n\n\n\n\nWhat I’m working on right now\n\n\nUpdated on July 24th, 2023.\n\n\n    \n\n\nCore habits\n\nRead ≧ 20 min\nExercise ≧ 1 hour\nMeditate ≧ 10 minutes\n\n\n\nWebsite\n\nGet a domain to publish this site\nFuture features can be found at DEV\n\n\n\nReading\n\n12 Rules for Life by Jordan Peterson\n\n\n\n\n\n\n\nWhat I’m working on right now\n\n\nUpdated on June 28th, 2023\n\n\n    \n\n\n\nWebsite\n\nStart coding from scracth this site"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html",
    "title": "MNIST Classification",
    "section": "",
    "text": "| Danilo Toapanta\n2023-09-04"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#how-to-run-locally",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#how-to-run-locally",
    "title": "MNIST Classification",
    "section": "1 How to run locally",
    "text": "1 How to run locally\n$ pip install -r requirements.txt"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#importing-all-libraries",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#importing-all-libraries",
    "title": "MNIST Classification",
    "section": "2 Importing all libraries",
    "text": "2 Importing all libraries\n\n\nCode\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport cv2\nnp.random.seed(42)                          # This allows us to reproduce the results from our script\nfrom keras.models import Sequential             \nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import Adam, SGD\nfrom keras.utils import to_categorical \n\n\n\n\nCode\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nprint('Total no of Images: ',X_train.shape[0]) \nprint('Size of Image:', X_train.shape[1:])\nprint('Total no of labels:', y_train.shape)\n\n\nTotal no of Images:  60000\nSize of Image: (28, 28)\nTotal no of labels: (60000,)\n\n\n\n\nCode\n# Look input data\nnum = 10\nnum_row = 2\nnum_col = 5\nimages = X_train[:num]\nlabels = y_train[:num]\n\n# Ploting images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i//num_col, i%num_col]\n    ax.imshow(images[i], cmap='gray')\n    ax.set_title('Label: {}'.format(labels[i]))\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#prepare-input-data",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#prepare-input-data",
    "title": "MNIST Classification",
    "section": "3 Prepare input data",
    "text": "3 Prepare input data\n\n\nCode\nX_train = X_train.reshape((X_train.shape[0],-1))\nX_test = X_test.reshape((X_test.shape[0], -1))\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nprint(X_train.shape, X_test.shape)\n\n\n(60000, 784) (10000, 784)\n\n\n\n\nCode\n# Normalize data\nX_train = X_train/255\nX_test = X_test/255\n\n# print(X_train[0])\nX_train.shape\n\n\n(60000, 784)\n\n\n\n\nCode\n# Perfom one encoding\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\nprint(y_train.shape)\n\n\n(60000, 10)\n\n\n\n\nCode\nnum_classes = y_test.shape[1]\nnum_pixels = 784"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#defining-the-model",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#defining-the-model",
    "title": "MNIST Classification",
    "section": "4 Defining the model",
    "text": "4 Defining the model\n\n\nCode\n# Define baseline model\n\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(256, input_dim=num_pixels, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    return model\n\n\n\n\nCode\n# Build the model\nmodel = baseline_model()\nmodel.summary()\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_6 (Dense)             (None, 256)               200960    \n                                                                 \n dense_7 (Dense)             (None, 64)                16448     \n                                                                 \n dense_8 (Dense)             (None, 10)                650       \n                                                                 \n=================================================================\nTotal params: 218058 (851.79 KB)\nTrainable params: 218058 (851.79 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nCode\nopt = SGD(lr = 0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer= opt, metrics=['accuracy'])\n\n\nWARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD."
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#train-model",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#train-model",
    "title": "MNIST Classification",
    "section": "5 Train model",
    "text": "5 Train model\n\n\nCode\nmodel.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)\n\n\nEpoch 1/5\n1875/1875 [==============================] - 9s 4ms/step - loss: 0.6029 - accuracy: 0.8422\nEpoch 2/5\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.2849 - accuracy: 0.9181\nEpoch 3/5\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.2314 - accuracy: 0.9346\nEpoch 4/5\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1962 - accuracy: 0.9440\nEpoch 5/5\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1701 - accuracy: 0.9510\n\n\n&lt;keras.src.callbacks.History at 0x13e2b0b50&gt;"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#test-model",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#test-model",
    "title": "MNIST Classification",
    "section": "6 Test model",
    "text": "6 Test model\n\n\nCode\nscores = model.evaluate(X_test, y_test, verbose=1)\nprint(\"Error: %.2f%%\" % (100-scores[1]*100))\n\n\n313/313 [==============================] - 1s 3ms/step - loss: 0.1672 - accuracy: 0.9516\nError: 4.84%"
  },
  {
    "objectID": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#predicting",
    "href": "0257e11a/blog/2023-09-04_mnist-classification/index.out.html#predicting",
    "title": "MNIST Classification",
    "section": "7 Predicting",
    "text": "7 Predicting\n\n\nCode\nimg_width, img_height = 28, 28\ngray_image = X_test[0]\nplt.imshow(gray_image,cmap='Greys')\nplt.show()\n# gray_image.shape\nx = np.expand_dims(gray_image, axis=0)\nx = x.reshape((1, -1))\n\n\n\n\n\n\n\nCode\npreds = model.predict(x)\nprob = np.argmax(preds, axis=1)\n\nprint('Predicted value is ', prob)\nprint('Probability across all numbers :', preds[0])\n\n\n1/1 [==============================] - 0s 30ms/step\nPredicted value is  [7]\nProbability across all numbers : [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]"
  },
  {
    "objectID": "0257e11a/blog/2023-11-07_numpy-reshape-rgb-image-to-1d-array/index.out.html",
    "href": "0257e11a/blog/2023-11-07_numpy-reshape-rgb-image-to-1d-array/index.out.html",
    "title": "Numpy reshape RGB image to 1D array",
    "section": "",
    "text": "| Danilo Toapanta\n2023-11-07\nImagine you want to train a NN with:\n\nX: s x 3 x 32 x 32\n\nWhere:\n\ns: num samples\n3 x 32 x 32: color img\n\nNow to feed in the NN we want the shapes to be:\n\nX: s x (3072)\n\nDo we lose information? The answers is not, we just reshape X.\nThe following takes 2 imgs, with dimensions 2x2:\n\n\nCode\nimport numpy as np\n\n# Create example 3x2x2 images (two images)\noriginal_images = np.array([[[[1, 2],\n                             [3, 4]],\n                            [[5, 6],\n                             [7, 8]],\n                            [[9, 10],\n                             [11, 12]]],\n\n                           [[[13, 14],\n                             [15, 16]],\n                            [[17, 18],\n                             [19, 20]],\n                            [[21, 22],\n                             [23, 24]]]])\n\n# Print the shape of the original images\nprint(\"Original Images Shape:\", original_images.shape)\n\n# Print the original images\nprint(\"Original Images:\\n\", original_images)\n\n# Reshape the images into num_imgs x (3*2*2) matrices\nnum_imgs = original_images.shape[0]\nreshaped_images = original_images.reshape(num_imgs, -1)\n\n# Print the shape of the reshaped images\nprint(\"Reshaped Images Shape:\", reshaped_images.shape)\n\n# Print the reshaped images\nprint(\"Reshaped Images:\\n\", reshaped_images)\n\n\nOriginal Images Shape: (2, 3, 2, 2)\nOriginal Images:\n [[[[ 1  2]\n   [ 3  4]]\n\n  [[ 5  6]\n   [ 7  8]]\n\n  [[ 9 10]\n   [11 12]]]\n\n\n [[[13 14]\n   [15 16]]\n\n  [[17 18]\n   [19 20]]\n\n  [[21 22]\n   [23 24]]]]\nReshaped Images Shape: (2, 12)\nReshaped Images:\n [[ 1  2  3  4  5  6  7  8  9 10 11 12]\n [13 14 15 16 17 18 19 20 21 22 23 24]]"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz",
    "title": "Self-supervised Learning I",
    "section": "18 Quiz:",
    "text": "18 Quiz:\n\n\n\n\nSlide 18\n\n\n\n\nSlef-supervised refers more to you want get something that you can use for another datasets. So topically in representation learning the dinstiction is clear because if you are learning a contrastive model that by itself is not usefull but in that case you can say self-supervised learning is part of supervised learning methodologies.\n\nNormal AE are unsupervised learning methods\nThere another autoencoders that are self-supervised learners."
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-8",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-8",
    "title": "Self-supervised Learning I",
    "section": "64 Title",
    "text": "64 Title\n\n\n\n\nSlide 64"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-1",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-1",
    "title": "Self-supervised Learning I",
    "section": "44 Quiz:",
    "text": "44 Quiz:\n\n\n\n\nSlide 44\n\n\n\n\n\nSo if you stay in the same domain, or a clone one you know that that the \\(lr\\) would be quite similar\nYou also can set the network architecture because if you know is a vision recognition taks then you know a Conv architecture is meaningful.\nIf your dataset is small then you reduce it because you trained before with tons of images so the epochs were also big"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-9",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-9",
    "title": "Self-supervised Learning I",
    "section": "67 Title",
    "text": "67 Title\n\n\n\n\nSlide 67"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-of-odject-parts-for-semantic-segmentation",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#self-supervised-learning-of-odject-parts-for-semantic-segmentation",
    "title": "Self-supervised Learning I",
    "section": "60 Self-Supervised Learning of Odject Parts for semantic Segmentation",
    "text": "60 Self-Supervised Learning of Odject Parts for semantic Segmentation\n\n\n\n\nSlide 60"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-2",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#quiz-2",
    "title": "Self-supervised Learning I",
    "section": "63 Quiz",
    "text": "63 Quiz\n\n\n\n\nSlide 63\n\n\n\n\n\nFalse. ROI-Align cannot take care of non-recatangular selections\nFalse, just like any pooling can provide gradients"
  },
  {
    "objectID": "blog/2023-12-14_self-supervised-learning-i/index.html#title-10",
    "href": "blog/2023-12-14_self-supervised-learning-i/index.html#title-10",
    "title": "Self-supervised Learning I",
    "section": "83 Title",
    "text": "83 Title\n\n\n\n\nSlide 83"
  }
]